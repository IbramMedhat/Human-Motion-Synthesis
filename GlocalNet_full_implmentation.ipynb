{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6oWUCJCnPmT",
    "outputId": "83206498-5fb1-4c3a-f4ec-1bf1a49f2b8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Model as Model_\n",
    "from tensorflow.keras.layers import Input, ReLU, LSTM, Dense, TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow_model_remediation.min_diff.losses.mmd_loss as MMD\n",
    "import tensorflow_model_remediation.min_diff.losses.adjusted_mmd_loss as adjustedMMD\n",
    "\n",
    "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ikRIhsrrnhlH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class Dataset_Preprocessing:\n",
    "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17, datatype = 'float32'):\n",
    "        \n",
    "        #Dataset Directory path\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "        #Which Dimension file to include, possible values: 2 and 3\n",
    "        self.include_dimension = include_dimension\n",
    "        \n",
    "        #Total frames in one Sample\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        #Default Datatype for all the samples\n",
    "        self.datatype = datatype\n",
    "        \n",
    "        #Activity classes to include\n",
    "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
    "        \n",
    "        #Total activity classes\n",
    "        self.total_classes = len(self.classes)\n",
    "        \n",
    "        #Subject Folders names in the Dataset\n",
    "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
    "    \n",
    "    def read_dataset(self):\n",
    "        try:\n",
    "            #Contains all the different activity vectors\n",
    "            activity_vector = {}\n",
    "            \n",
    "            #Contains the overall dataset\n",
    "            sampled_data = None\n",
    "            \n",
    "            #Based on dimensions, which folder to use for extracting the dataset files\n",
    "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
    "            \n",
    "            #Checking if the dataset path is valid\n",
    "            if not os.path.exists(self.dir_path):\n",
    "                print('The Data Directory Does not Exist!')\n",
    "                return None\n",
    "\n",
    "            #Iterating over all the subject folders\n",
    "            for fld in self.internal_folders:\n",
    "                #Iterating for each file in the specified folder\n",
    "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
    "                    #Extracting the activity from the filename\n",
    "                    activity = self.__extract_activity(file)\n",
    "                    \n",
    "                    if activity not in self.classes:\n",
    "                        continue\n",
    "                    \n",
    "                    #Reading the CSV file using Pandas\n",
    "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
    "\n",
    "                    #Formulating the activity vector using one hot encoding\n",
    "                    if activity not in activity_vector:\n",
    "                        total_keys = len(activity_vector.keys())\n",
    "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
    "                        activity_vector[activity][total_keys] = 1\n",
    "                    vector = activity_vector[activity]\n",
    "                    \n",
    "                    #Sampling the dataset\n",
    "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
    "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
    "            \n",
    "            #Changing the Datatype\n",
    "            sampled_data = sampled_data.astype(self.datatype)\n",
    "            \n",
    "            return sampled_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __extract_activity(self, filename):\n",
    "        try:\n",
    "            #Extracting the filename and excluding the extension\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            #Substituting the empty string with characters other than english alphabets\n",
    "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
    "            return activity\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __group_samples(self, dataset, sample_size, activity):\n",
    "        try:\n",
    "            #Checking if the dataset is a Pandas Dataframe\n",
    "            if not isinstance(dataset, pd.DataFrame):\n",
    "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
    "                return None\n",
    "            \n",
    "            #Appending activity class to each row in the dataset\n",
    "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
    "            \n",
    "            #Reshaping the dataset into sample batches\n",
    "            total_samples = dataset.shape[0]//sample_size\n",
    "            total_features = dataset.shape[1]\n",
    "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
    "            \n",
    "            return grouped_rows\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "f8PgK1UVnkrw"
   },
   "outputs": [],
   "source": [
    "#For long term prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
    "sampled_data = Dataset_Preprocessing('./H3.6csv', sample_size=60).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "416CBxXNlTSq"
   },
   "outputs": [],
   "source": [
    "#To make the data divisible for batch size of hunderd\n",
    "total_batches = sampled_data.shape[0]\n",
    "sampled_data = sampled_data[:total_batches-(total_batches%100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8nABIG0rnxl_"
   },
   "outputs": [],
   "source": [
    "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
    "    \"\"\"\n",
    "    Function for splitting the data into features(with sequance size=iput_sequance_size)\n",
    "    and labels which should be the remainder of the sample length \n",
    "    \"\"\"\n",
    "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
    "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
    "    labels = dataset[:,np.s_[input_sequance_size:], :64]\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "h6bSQTwdn2Fo"
   },
   "outputs": [],
   "source": [
    "sampled_dataX, sampled_dataY = split_to_features_labels(sampled_data, input_sequance_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVXQiscVn4O2",
    "outputId": "565d9101-020c-464b-bc6a-1956ffdf014d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 25500\n",
      "Total Frames: 50\n",
      "Total Features: 64\n"
     ]
    }
   ],
   "source": [
    "print('Total Samples: {}'.format(sampled_dataY.shape[0]))\n",
    "print('Total Frames: {}'.format(sampled_dataY.shape[1]))\n",
    "print('Total Features: {}'.format(sampled_dataY.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "QsfCMT-0zRTs"
   },
   "outputs": [],
   "source": [
    "class InterpolationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom interpolation layer extending the keras layer class\n",
    "    it has one attribute num_frames to be interpolated between each two consecutive \n",
    "    timesteps\n",
    "    it has one main function interpolateFrames  \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, num_frames=5):\n",
    "        super(InterpolationLayer, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "       \n",
    "    def interpolateFrames(self, inputs):\n",
    "      \"\"\"\n",
    "      Takes input tensors of shape(batch_size, timesteps, features)\n",
    "      returns interpolated frames with shape(batch_size, timesteps*num_frames, features)\n",
    "      \"\"\"\n",
    "      batch_size = inputs.shape[0]\n",
    "      timesteps = inputs.shape[1]\n",
    "      features = inputs.shape[2]\n",
    "      #interpolated_frames = tf.zeros([batch_size, timesteps, 0, features])\n",
    "      interpolated_frames = tf.zeros([0, features])\n",
    "\n",
    "      for batch in range(batch_size) :\n",
    "        for t in range(timesteps) :\n",
    "          for j in range(self.num_frames) :\n",
    "            X_i0 = inputs[batch, t]\n",
    "            if(t == timesteps-1) :\n",
    "              X_i1 = inputs[batch, t]\n",
    "            else :  \n",
    "              X_i1 = inputs[batch, t+1]\n",
    "            alpha_j = j/self.num_frames\n",
    "            current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
    "            current_frame = tf.reshape(current_frame, [1, features])\n",
    "            interpolated_frames = tf.concat((interpolated_frames, current_frame), axis=0)\n",
    "            \n",
    "      interpolated_frames = tf.reshape(interpolated_frames,\n",
    "                                       [batch_size, (timesteps)*self.num_frames, features])\n",
    "      return interpolated_frames\n",
    "\n",
    "    def call(self, inputs):\n",
    "      return self.interpolateFrames(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "7PIJXvLaaEvY"
   },
   "outputs": [],
   "source": [
    "class GlocalNet(Model_):\n",
    "    \"\"\"\n",
    "    A full GlocalNet implementation include the three main stages\n",
    "    Glogen generating initial sparse frames\n",
    "    Interpolation layer generating dense frames from Glogen output\n",
    "    Locgen generating the final output by smoothing the interpolated frames\n",
    "    \"\"\"\n",
    "    def __init__(self, enocder_hidden_state=200, decoder_hidden_state=200, \n",
    "                 output_diminsion=64, activation='relu', interpolation_frames=5):\n",
    "        super(GlocalNet, self).__init__()\n",
    "        #Glogen layers\n",
    "        self.glogen_encoder = LSTM(enocder_hidden_state, return_state=True, return_sequences=True)\n",
    "        self.glogen_decoder = LSTM(decoder_hidden_state, return_sequences=True, return_state=True)\n",
    "        self.glogen_dense_layer = TimeDistributed(Dense(output_diminsion, activation=activation)) \n",
    "        \n",
    "        #Interpolation layer\n",
    "        self.interpolation_layer = InterpolationLayer(num_frames=interpolation_frames)\n",
    "\n",
    "        #Locgen layers\n",
    "        self.locgen_encoder = LSTM(enocder_hidden_state, return_sequences=True, return_state=True)\n",
    "        self.locgen_decoder = LSTM(decoder_hidden_state, return_sequences=True, return_state=True)\n",
    "        self.locgen_dense_layer = TimeDistributed(Dense(output_diminsion, activation=activation)) \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #Glogen calls      \n",
    "        encoder_outputs, state_h, state_c = self.glogen_encoder(inputs)\n",
    "        encoder_states = [state_h, state_c]\n",
    "        output, _, _ = self.glogen_decoder(encoder_outputs, initial_state=encoder_states)\n",
    "        glogen_output = self.glogen_dense_layer(output)\n",
    "\n",
    "        #Interpolation call\n",
    "        interpolated_frames = self.interpolation_layer(glogen_output)\n",
    "        \n",
    "        #Locgen calls\n",
    "        locgen_encoder_outputs, locgen_state_h, locgen_state_c = self.locgen_encoder(interpolated_frames)\n",
    "        locgen_encoder_states = [locgen_state_h, locgen_state_c]\n",
    "        locgen_output, _, _ = self.locgen_decoder(locgen_encoder_outputs, initial_state=locgen_encoder_states)\n",
    "        final_output = self.locgen_dense_layer(locgen_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "bVUFJsv3fVYh"
   },
   "outputs": [],
   "source": [
    "class JointLoss() :\n",
    "    \"\"\"\n",
    "    Joint loss class with two weight attributes for two different losses\n",
    "    first one is the loss joint and the second is the loss_motion_flow\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda1=0.5, lambda2=0.5) :\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "\n",
    "    def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        \"\"\"\n",
    "        Loss between the joint positions and its corresponding counterparts in the groundtruth\n",
    "        \"\"\"\n",
    "        diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
    "        return tf.reduce_sum(diff_norm_2, axis=1) \n",
    "\n",
    "    def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        \"\"\"\n",
    "        Loss between the motion flow of predicted sequance and the ground truth\n",
    "        where the motion flow is the euclidean distance between each two consecutive frames\n",
    "        \"\"\"\n",
    "        predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
    "        real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
    "        prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
    "        return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
    "\n",
    "\n",
    "    def total_loss(self, target_sequance_batch, predicted_sequance_batch) :\n",
    "        \"\"\"\n",
    "        calculating the total loss through a combination of the joint_loss and motion_flow_loss\n",
    "        \"\"\"\n",
    "        joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
    "        motion_flow_loss = self.loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
    "        return self.lambda1*joints_loss + self.lambda2*motion_flow_loss\n",
    "\n",
    "    def custom_sequence_MMD_loss(self, target_sequance_batch, predicted_sequance_batch):\n",
    "        \"\"\"\n",
    "        Calculating the Sequence MMD Loss between prediction and the ground Truth. Additionally combining the last two dimensions \n",
    "        \"\"\"\n",
    "        mmd_loss = MMD.MMDLoss()\n",
    "        total_batches = predicted_sequance_batch.shape[0]\n",
    "        frames_per_batch = predicted_sequance_batch.shape[1] * predicted_sequance_batch.shape[2]\n",
    "        return mmd_loss(tf.reshape(predicted_sequance_batch, [total_batches, frames_per_batch]),\n",
    "                        tf.reshape(target_sequance_batch, [total_batches, frames_per_batch]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "labA65KEakkZ"
   },
   "outputs": [],
   "source": [
    "def run_experiment(learning_rate=0.002, lambda1=0.5, lambda2=0.5, use_mse=False,\n",
    "                   use_MMD=False, metrics=None, batch_size=100, epochs=50,\n",
    "                   validation_split=0.2) :\n",
    "    \"\"\"\n",
    "    Method takes all hyperparameters as input paramters and returns the model and history as\n",
    "    a result\n",
    "    \"\"\"\n",
    "    glocal_model = GlocalNet()\n",
    "    if use_mse :\n",
    "        loss_function = tf.keras.losses.mean_squared_error\n",
    "    elif use_MMD :\n",
    "        loss_function = JointLoss().custom_sequence_MMD_loss\n",
    "    else :\n",
    "        loss_function = JointLoss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
    "\n",
    "    glocal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                       loss=loss_function, metrics=metrics)\n",
    "    history = glocal_model.fit(sampled_dataX, sampled_dataY,\n",
    "                              batch_size=batch_size,\n",
    "                              epochs=epochs)\n",
    "    return history, glocal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTHffB1KrfBy"
   },
   "source": [
    "## Experiment Running for MSE and joint loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sQ-XPaUFd4D0",
    "outputId": "bf808cf3-cea2-4283-e53f-4d4e2dbba85b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "255/255 [==============================] - 415s 240ms/step - loss: 197673.3438 - mean_absolute_percentage_error: 88.2866\n",
      "Epoch 2/10\n",
      "255/255 [==============================] - 63s 248ms/step - loss: 128336.4766 - mean_absolute_percentage_error: 67.4349\n",
      "Epoch 3/10\n",
      "255/255 [==============================] - 62s 243ms/step - loss: 82020.9609 - mean_absolute_percentage_error: 50.1100\n",
      "Epoch 4/10\n",
      "255/255 [==============================] - 64s 250ms/step - loss: 52302.0547 - mean_absolute_percentage_error: 37.0668\n",
      "Epoch 5/10\n",
      "255/255 [==============================] - 61s 240ms/step - loss: 34254.2656 - mean_absolute_percentage_error: 28.3228\n",
      "Epoch 6/10\n",
      "255/255 [==============================] - 61s 241ms/step - loss: 23976.3828 - mean_absolute_percentage_error: 23.0670\n",
      "Epoch 7/10\n",
      "255/255 [==============================] - 63s 245ms/step - loss: 18552.9414 - mean_absolute_percentage_error: 20.3186\n",
      "Epoch 8/10\n",
      "255/255 [==============================] - 61s 241ms/step - loss: 15939.6357 - mean_absolute_percentage_error: 19.0722\n",
      "Epoch 9/10\n",
      "255/255 [==============================] - 62s 242ms/step - loss: 14812.7559 - mean_absolute_percentage_error: 18.6462\n",
      "Epoch 10/10\n",
      "255/255 [==============================] - 62s 245ms/step - loss: 14385.7510 - mean_absolute_percentage_error: 18.5867\n"
     ]
    }
   ],
   "source": [
    "history_mse, glocal_model_mse = run_experiment(epochs=10, use_mse=True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7FQ-HQ4bPVi",
    "outputId": "89e882cb-f5fd-4d5f-d5af-1497efaba172"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "255/255 [==============================] - 419s 248ms/step - loss: 318235136.0000 - mean_absolute_percentage_error: 88.8408\n",
      "Epoch 2/10\n",
      "255/255 [==============================] - 63s 248ms/step - loss: 209204064.0000 - mean_absolute_percentage_error: 68.7124\n",
      "Epoch 3/10\n",
      "255/255 [==============================] - 64s 252ms/step - loss: 136290848.0000 - mean_absolute_percentage_error: 52.0030\n",
      "Epoch 4/10\n",
      "255/255 [==============================] - 63s 248ms/step - loss: 89421984.0000 - mean_absolute_percentage_error: 39.3298\n",
      "Epoch 5/10\n",
      "255/255 [==============================] - 64s 251ms/step - loss: 60899012.0000 - mean_absolute_percentage_error: 30.7696\n",
      "Epoch 6/10\n",
      "255/255 [==============================] - 65s 254ms/step - loss: 44627976.0000 - mean_absolute_percentage_error: 25.5873\n",
      "Epoch 7/10\n",
      "255/255 [==============================] - 64s 250ms/step - loss: 36028060.0000 - mean_absolute_percentage_error: 22.8682\n",
      "Epoch 8/10\n",
      "255/255 [==============================] - 64s 251ms/step - loss: 31884704.0000 - mean_absolute_percentage_error: 21.6249\n",
      "Epoch 9/10\n",
      "255/255 [==============================] - 63s 248ms/step - loss: 30101348.0000 - mean_absolute_percentage_error: 21.2003\n",
      "Epoch 10/10\n",
      "255/255 [==============================] - 64s 252ms/step - loss: 29427098.0000 - mean_absolute_percentage_error: 21.1448\n"
     ]
    }
   ],
   "source": [
    "history_jointLoss, glocal_model_jointLoss = run_experiment(epochs=10, lambda1=0.5, lambda2=0.5, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-ekHZVlrq1o"
   },
   "source": [
    "## Running experiment with MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "KayDheMuk7oE",
    "outputId": "1ffdc5ce-819b-43e6-e9dd-641ab88419ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "history_mmd, glocal_model_mmd = run_experiment(epochs=10, use_mse = True, metrics=[JointLoss().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wKSm6QMuGuc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KTHffB1KrfBy"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
