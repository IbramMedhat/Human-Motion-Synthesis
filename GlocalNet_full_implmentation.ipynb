{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5bGejq7QySe"
   },
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6oWUCJCnPmT",
    "outputId": "7c5b49ce-0b4d-4f9a-eac8-f05ee626a96d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model as Model_\n",
    "from tensorflow.keras.layers import Input, ReLU, LSTM, GRU, SimpleRNN, Dense, TimeDistributed, Bidirectional, GaussianNoise \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_remediation.min_diff.losses.mmd_loss as MMD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tUdJibeQWdF"
   },
   "source": [
    "## Dataset Reading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing all the movable joints in the human skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VYTyMttRQVHV"
   },
   "outputs": [],
   "source": [
    "#Was done in the preprocessing in [1]\n",
    "# Joints in H3.6M -- data has 32 joints, but only 17 that move; these are the indices.\n",
    "H36M_NAMES = ['']*32\n",
    "H36M_NAMES[0]  = 'Hip'\n",
    "H36M_NAMES[1]  = 'RHip'\n",
    "H36M_NAMES[2]  = 'RKnee'\n",
    "H36M_NAMES[3]  = 'RFoot'\n",
    "H36M_NAMES[6]  = 'LHip'\n",
    "H36M_NAMES[7]  = 'LKnee'\n",
    "H36M_NAMES[8]  = 'LFoot'\n",
    "H36M_NAMES[12] = 'Spine'\n",
    "H36M_NAMES[13] = 'Thorax'\n",
    "H36M_NAMES[14] = 'Neck/Nose'\n",
    "H36M_NAMES[15] = 'Head'\n",
    "H36M_NAMES[17] = 'LShoulder'\n",
    "H36M_NAMES[18] = 'LElbow'\n",
    "H36M_NAMES[19] = 'LWrist'\n",
    "H36M_NAMES[25] = 'RShoulder'\n",
    "H36M_NAMES[26] = 'RElbow'\n",
    "H36M_NAMES[27] = 'RWrist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class to Read and Combine all the Dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ikRIhsrrnhlH"
   },
   "outputs": [],
   "source": [
    "class Dataset_loading:\n",
    "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, \n",
    "                 total_classes = 17, datatype = 'float32', include_movable_joints = False, batch_size = 20, \n",
    "                 include_action_labels = True, return_action_labels = False):\n",
    "        \n",
    "        #Dataset Directory path\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "        #Which Dimension file to include, possible values: 2 and 3\n",
    "        self.include_dimension = include_dimension\n",
    "        \n",
    "        #Total frames in one Sample\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        #Default Datatype for all the samples\n",
    "        self.datatype = datatype\n",
    "        \n",
    "        #Batch Size of the dataset for experimentation \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Boolean value to indicate whether to include action class in each frames\n",
    "        self.include_action_labels = include_action_labels\n",
    "        \n",
    "        #Whether to return action labels with data\n",
    "        self.return_action_labels = return_action_labels\n",
    "        \n",
    "        #Activity classes to include\n",
    "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
    "        \n",
    "        #Total activity classes\n",
    "        self.total_classes = len(self.classes)\n",
    "        \n",
    "        #Subject Folders names in the Dataset\n",
    "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
    "\n",
    "        #Boolean value indicating whether to include all joints or only the movable joints.\n",
    "        self.include_movable_joints = include_movable_joints\n",
    "        \n",
    "        self.movable_joints = [0, 1, 2, 3, 6, 7, 8, 12, 13, 14, 15, 17, 18, 19, 25, 26, 27]\n",
    "    \n",
    "    def read_dataset(self):\n",
    "        try:\n",
    "            #Contains all the different activity vectors\n",
    "            activity_vector = {}\n",
    "            \n",
    "            #Contains the overall dataset\n",
    "            sampled_data = None\n",
    "            sampled_labels = None\n",
    "            \n",
    "            #Based on dimensions, which folder to use for extracting the dataset files\n",
    "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
    "            \n",
    "            #Checking if the dataset path is valid\n",
    "            if not os.path.exists(self.dir_path):\n",
    "                print('The Data Directory Does not Exist!')\n",
    "                return None\n",
    "\n",
    "            #Iterating over all the subject folders\n",
    "            for fld in self.internal_folders:\n",
    "                #Iterating for each file in the specified folder\n",
    "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
    "                    #Extracting the activity from the filename\n",
    "                    activity = self.__extract_activity(file)\n",
    "                    \n",
    "                    if activity not in self.classes:\n",
    "                        continue\n",
    "                    \n",
    "                    #Reading the CSV file using Pandas\n",
    "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
    "\n",
    "                    #Formulating the activity vector using one hot encoding\n",
    "                    if activity not in activity_vector:\n",
    "                        total_keys = len(activity_vector.keys())\n",
    "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
    "                        activity_vector[activity][total_keys] = 1\n",
    "                    vector = activity_vector[activity]\n",
    "                    \n",
    "                    #Sampling the dataset\n",
    "                    grouped_sample, grouped_activity = self.__group_samples(data, self.sample_size, vector)\n",
    "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
    "                    sampled_labels = grouped_activity if sampled_labels is None else np.append(sampled_labels, grouped_activity, axis=0)\n",
    "            \n",
    "            #Changing the Datatype\n",
    "            sampled_data = sampled_data.astype(self.datatype)\n",
    "            \n",
    "            #To make the data divisible for batch size\n",
    "            total_batches = sampled_data.shape[0]\n",
    "            sampled_data = sampled_data[:total_batches - (total_batches % self.batch_size)]\n",
    "            sampled_labels = sampled_labels[:total_batches - (total_batches % self.batch_size)]\n",
    "            \n",
    "            if self.return_action_labels:\n",
    "                return sampled_data, sampled_labels\n",
    "            \n",
    "            return sampled_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __extract_activity(self, filename):\n",
    "        try:\n",
    "            #Extracting the filename and excluding the extension\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            #Substituting the empty string with characters other than english alphabets\n",
    "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
    "            return activity\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __group_samples(self, dataset, sample_size, activity):\n",
    "        try:\n",
    "            #Checking if the dataset is a Pandas Dataframe\n",
    "            if not isinstance(dataset, pd.DataFrame):\n",
    "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
    "                return None\n",
    "            \n",
    "            if self.include_movable_joints:\n",
    "                joints = list(chain.from_iterable((jt*2, (jt*2)+1) for jt in self.movable_joints))\n",
    "                dataset = dataset.iloc[: , joints].copy()\n",
    "\n",
    "            #Appending activity class to each row in the dataset\n",
    "            if self.include_action_labels:\n",
    "                dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
    "            \n",
    "            #Reshaping the dataset into sample batches\n",
    "            total_samples = dataset.shape[0]//sample_size\n",
    "            total_features = dataset.shape[1]\n",
    "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
    "            \n",
    "            grouped_activity = np.tile(activity, (dataset.shape[0]//self.sample_size, 1))\n",
    "            grouped_activity = grouped_activity[:total_samples*self.sample_size].reshape((-1, len(activity)))\n",
    "            \n",
    "            return grouped_rows, grouped_activity\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to split Dataset into Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8nABIG0rnxl_"
   },
   "outputs": [],
   "source": [
    "def split_to_features_labels(dataset, input_sequance_size=10, total_features=64):\n",
    "    \"\"\"\n",
    "    Function for splitting the data into features(with sequance size=iput_sequance_size)\n",
    "    and labels which should be the remainder of the sample length \n",
    "    \"\"\"\n",
    "    assert input_sequance_size < dataset.shape[1], f\"input sequence should be smaller than the total sample size\"\n",
    "    \n",
    "    #Dividing the dataset into features and labels by splitting the Time Frame Dimension\n",
    "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
    "    labels = dataset[:,np.s_[input_sequance_size:], :total_features]\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5HkmNJJSKg3"
   },
   "source": [
    "### A function for downsampling the dataset on number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "wNJmfjiGSDY2"
   },
   "outputs": [],
   "source": [
    "def downsampling(sampled_data, downsample_technique = 'skip'):\n",
    "    \"\"\"\n",
    "    The function used to down-sample the data using two different techniques. In Skip, one frame is skipped consecutively and\n",
    "    in the mean technique, two frames are averaged consecutively.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert downsample_technique in ['skip', 'mean'], f\"Only Skip and Mean techniques are available\"\n",
    "    \n",
    "    #Creating an empty variable to store Downsampled data when the technique is Mean\n",
    "    samples_per_batch = int(sampled_data.shape[1] / 2)\n",
    "    total_features = sampled_data.shape[2]\n",
    "    downsampled_data = np.empty(shape=(0, samples_per_batch, total_features))\n",
    "    \n",
    "    #In Skip technique, we skip 2 frames consecutively.\n",
    "    if downsample_technique == 'skip':\n",
    "        downsampled_data = sampled_data[:,::2,:]\n",
    "    else:\n",
    "        #Iterating over batches\n",
    "        for batch in sampled_data:\n",
    "    \n",
    "            averaged_batch = np.empty(shape=(0, total_features))\n",
    "    \n",
    "            #In each iteration, averaging 2 Frames and appending it to the variable\n",
    "            for i in range(0, batch.shape[0], 2):\n",
    "                averaged_batch = np.append(averaged_batch, np.mean(batch[i:i+2, :], axis = 0).reshape((1, total_features)), axis = 0)\n",
    "            \n",
    "            #Appending the whole batched averaged downsampled data to the new variable created before\n",
    "            downsampled_data = np.append(downsampled_data, averaged_batch.reshape((1, samples_per_batch, total_features)), axis = 0)\n",
    "    \n",
    "    return downsampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLpG7OlQSUjW"
   },
   "source": [
    "### Adding more preprocessing steps (Normalization and gussian noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "VzjeqMm1KNjQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(sampled_dataX, sampled_dataY, normalize=True, add_noise=True\n",
    "                    , stddev=0.05) :\n",
    "    \"\"\"\n",
    "    Function to preprocess data by normalizing input features and adding guassian\n",
    "    noise to increase model robustness\n",
    "    \"\"\"  \n",
    "    if normalize :\n",
    "        sampled_dataX =  tf.keras.utils.normalize(sampled_dataX, axis=2)\n",
    "    \n",
    "    if add_noise :\n",
    "        guassian_noise_layer = tf.keras.layers.GaussianNoise(stddev=stddev)\n",
    "        sampled_dataX = guassian_noise_layer(sampled_dataX)\n",
    "    \n",
    "    return sampled_dataX, sampled_dataY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwCrXubfJka4"
   },
   "source": [
    "## Defining different components of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Interpolation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QsfCMT-0zRTs"
   },
   "outputs": [],
   "source": [
    "class InterpolationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom interpolation layer extending the keras layer class\n",
    "    it has one attribute num_frames to be interpolated between each two consecutive \n",
    "    timesteps\n",
    "    it has one main function interpolateFrames  \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, num_frames=5):\n",
    "        super(InterpolationLayer, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def interpolateFrames(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes input tensors of shape(batch_size, timesteps, features)\n",
    "        returns interpolated frames with shape(batch_size, timesteps*num_frames, features)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        timesteps = inputs.shape[1]\n",
    "        features = inputs.shape[2]\n",
    "        interpolated_frames = tf.zeros([0, features])\n",
    "        \n",
    "        for batch in tf.range(tf.shape(inputs)[0]) :\n",
    "            tf.autograph.experimental.set_loop_options(\n",
    "            shape_invariants=[(interpolated_frames, tf.TensorShape([None, features]))])\n",
    "            for t in range(timesteps) :\n",
    "                for j in range(self.num_frames) :\n",
    "                    X_i0 = inputs[batch, t]\n",
    "                    if(t == timesteps-1) :\n",
    "                        X_i1 = inputs[batch, t]\n",
    "                    else :  \n",
    "                        X_i1 = inputs[batch, t+1]\n",
    "                    alpha_j = j/self.num_frames\n",
    "                    current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
    "                    current_frame = tf.reshape(current_frame, [1, features])\n",
    "                    interpolated_frames = tf.concat((interpolated_frames, current_frame), axis=0)\n",
    "\n",
    "        interpolated_frames = tf.reshape(interpolated_frames,[tf.shape(inputs)[0], (timesteps)*self.num_frames, features])\n",
    "        return interpolated_frames\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.interpolateFrames(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9xiHeHMaQRH"
   },
   "source": [
    "### Trying to create the Keras GlocalNet model through a custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "70zZuaX0Y6Ag"
   },
   "outputs": [],
   "source": [
    "def create_glocalNet_model(enocder_hidden_state=200, decoder_hidden_state=200, \n",
    "                 output_diminsion=64, input_diminsions=74, LSTM_dropout=0.25, dense_activation='relu',\n",
    "                 interpolation_frames=5, exclude_locgen=False) :\n",
    "    #Glogen encoder\n",
    "    encoder_inputs = Input(shape=(10, input_diminsions))\n",
    "    encoder = Bidirectional(LSTM(enocder_hidden_state, return_sequences=True, return_state=True))\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    #Glogen decoder\n",
    "    decoder_lstm = Bidirectional(LSTM(decoder_hidden_state, return_sequences=True, return_state=True))\n",
    "    decoder_outputs, _, _ = decoder_lstm(encoder_outputs,\n",
    "                                        initial_state=encoder_states)\n",
    "    decoder_dense = TimeDistributed(Dense(output_diminsion, activation=dense_activation))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    #Interpolation layer\n",
    "    interpolation_layer = InterpolationLayer(num_frames=interpolation_frames)\n",
    "    interpolation_output = interpolation_layer(decoder_outputs)\n",
    "    #return the model if execlude_locgen is true\n",
    "    if(exclude_locgen) :\n",
    "        return Model_(encoder_inputs, interpolation_output)\n",
    "    \n",
    "    #Locgen encoder\n",
    "    encoder_locgen = Bidirectional(LSTM(enocder_hidden_state, return_sequences=True, return_state=True))\n",
    "    encoder_outputs_locgen, state_h_locgen, state_c_locgen = encoder_locgen(interpolation_output)\n",
    "    encoder_states_locgen = [state_h_locgen, state_c_locgen]\n",
    "    #Locgen decoder\n",
    "    decoder_lstm_locgen = Bidirectional(LSTM(decoder_hidden_state, return_sequences=True, return_state=True))\n",
    "    decoder_outputs_locgen, _, _ = decoder_lstm(encoder_outputs_locgen,\n",
    "                                        initial_state=encoder_states_locgen)\n",
    "    decoder_dense_locgen = TimeDistributed(Dense(output_diminsion, activation=dense_activation))\n",
    "    glocalNet_output = decoder_dense_locgen(decoder_outputs_locgen)\n",
    "    return Model_(encoder_inputs, glocalNet_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ccVrCxin9KA"
   },
   "source": [
    "### Custom GlocalNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7PIJXvLaaEvY"
   },
   "outputs": [],
   "source": [
    "class GlocalNet(Model_):\n",
    "    \"\"\"\n",
    "    A full GlocalNet implementation include the three main stages\n",
    "    Glogen generating initial sparse frames\n",
    "    Interpolation layer generating dense frames from Glogen output\n",
    "    Locgen generating the final output by smoothing the interpolated frames\n",
    "    \"\"\"\n",
    "    def __init__(self, enocder_hidden_state=200, decoder_hidden_state=200, \n",
    "                 output_diminsion=64, LSTM_dropout=0.25, dense_activation='relu',\n",
    "                 interpolation_frames=5, exclude_locgen=False, only_glogen = False,\n",
    "                 self_attention = False, cross_attention = False):\n",
    "        super(GlocalNet, self).__init__()\n",
    "        \n",
    "        assert ~(self_attention and cross_attention), f'only one type of attention can be used' \n",
    "        \n",
    "        self.exclude_locgen = exclude_locgen\n",
    "        self.only_glogen = only_glogen\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        \n",
    "        #Glogen layers\n",
    "        self.glogen_encoder = Bidirectional(LSTM(enocder_hidden_state, return_state=True\n",
    "                                   , return_sequences=True, dropout=LSTM_dropout))\n",
    "        self.glogen_decoder = Bidirectional(LSTM(decoder_hidden_state, return_sequences=True,\n",
    "                                   return_state=True, dropout=LSTM_dropout))\n",
    "        #Locgen layers\n",
    "        self.locgen_encoder = Bidirectional(LSTM(enocder_hidden_state, return_sequences=True,\n",
    "                                   return_state=True, dropout=LSTM_dropout))\n",
    "        self.locgen_decoder = Bidirectional(LSTM(decoder_hidden_state, return_sequences=True,\n",
    "                                   return_state=True, dropout=LSTM_dropout))\n",
    "        #Glogen dense layer\n",
    "        self.glogen_dense_layer = TimeDistributed(Dense(output_diminsion,\n",
    "                                                        activation=dense_activation)) \n",
    "        #Interpolation layer\n",
    "        self.interpolation_layer = InterpolationLayer(num_frames=interpolation_frames)\n",
    "        #Locgen dense layer\n",
    "        self.locgen_dense_layer = TimeDistributed(Dense(output_diminsion,\n",
    "                                                        activation=dense_activation)) \n",
    "        \n",
    "        #Attention layer to be used in both self and cross attention cases\n",
    "        self.attention_layer = tf.keras.layers.Attention()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #Glogen calls      \n",
    "        encoder_outputs, fwd_state_h, fwd_state_c ,back_state_h, back_state_c= self.glogen_encoder(inputs)\n",
    "        encoder_states = [fwd_state_h, fwd_state_c ,back_state_h, back_state_c]\n",
    "        \n",
    "        #Cross attention case\n",
    "        if self.cross_attention :\n",
    "            decoder_output,_,_,_,_ = self.glogen_decoder(encoder_outputs, initial_state=encoder_states)\n",
    "            attention_score = self.attention_layer(inputs = [decoder_output, encoder_outputs])\n",
    "            decoder_output,_,_,_,_ = self.glogen_decoder(attention_score, initial_state=encoder_states)\n",
    "        \n",
    "        #Self attention case\n",
    "        elif self.self_attention:\n",
    "            glogen_decoder_input = self.attention_layer(inputs = [encoder_outputs, encoder_outputs])\n",
    "            decoder_output,_,_,_,_ = self.glogen_decoder(glogen_decoder_input, initial_state=encoder_states)\n",
    "            \n",
    "        #Basic case without attention\n",
    "        else :\n",
    "            decoder_output,_,_,_,_ = self.glogen_decoder(encoder_outputs, initial_state=encoder_states)\n",
    "        \n",
    "        glogen_output = self.glogen_dense_layer(decoder_output)\n",
    "        \n",
    "        if self.only_glogen:\n",
    "            return glogen_output\n",
    "\n",
    "        #Interpolation call\n",
    "        interpolated_frames = self.interpolation_layer(glogen_output)\n",
    "        \n",
    "        if self.exclude_locgen :\n",
    "            return interpolated_frames\n",
    "\n",
    "        #Locgen calls\n",
    "        locgen_encoder_outputs, locgen_fwd_state_h, locgen_fwd_state_c,locgen_back_state_h, locgen_back_state_c = self.locgen_encoder(interpolated_frames)\n",
    "        locgen_encoder_states = [locgen_fwd_state_h, locgen_fwd_state_c,locgen_back_state_h, locgen_back_state_c]\n",
    "        locgen_output, _, _,_,_  = self.locgen_decoder(locgen_encoder_outputs, initial_state=locgen_encoder_states)\n",
    "        final_output = self.locgen_dense_layer(locgen_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionClassifier(Model_):\n",
    "    def __init__(self):\n",
    "        super(ActionClassifier, self).__init__()\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=128, kernel_size=8)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=256, kernel_size=5)\n",
    "        self.conv3 = tf.keras.layers.Conv1D(filters=128, kernel_size=3)\n",
    "        self.denseLayer = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "        self.batchNormalization1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchNormalization2 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchNormalization3 = tf.keras.layers.BatchNormalization()\n",
    "        self.globalAveragePooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.relu3 = tf.keras.layers.ReLU()\n",
    "        \n",
    "    def call(self, input):      \n",
    "        output = self.conv1(input)\n",
    "        output = self.batchNormalization1(output)\n",
    "        output = self.relu1(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.batchNormalization2(output)\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.batchNormalization3(output)\n",
    "        output = self.relu3(output)\n",
    "        \n",
    "        output = self.globalAveragePooling(output)\n",
    "        output = self.denseLayer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining different Types of Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "bVUFJsv3fVYh"
   },
   "outputs": [],
   "source": [
    "class Loss() :\n",
    "    \"\"\"\n",
    "    Joint loss class with two weight attributes for two different losses\n",
    "    first one is the loss joint and the second is the loss_motion_flow\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda1=0.5, lambda2=0.5) :\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "\n",
    "    def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        \"\"\"\n",
    "        Loss between the joint positions and its corresponding counterparts in the groundtruth\n",
    "        \"\"\"\n",
    "        diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
    "        return tf.reduce_sum(diff_norm_2, axis=1) \n",
    "\n",
    "    def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        \"\"\"\n",
    "        Loss between the motion flow of predicted sequance and the ground truth\n",
    "        where the motion flow is the euclidean distance between each two consecutive frames\n",
    "        \"\"\"\n",
    "        predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
    "        real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
    "        prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
    "        return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
    "\n",
    "    def total_loss(self, target_sequance_batch, predicted_sequance_batch) :\n",
    "        \"\"\"\n",
    "        calculating the total loss through a combination of the joint_loss and motion_flow_loss\n",
    "        \"\"\"\n",
    "        joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
    "        motion_flow_loss = self.loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
    "        return self.lambda1*joints_loss + self.lambda2*motion_flow_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining different types of Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \"\"\"\n",
    "    A class containing different types of Evaluation Metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mmd_kernel='gaussian') :\n",
    "        self.mmd_kernel = mmd_kernel\n",
    "    \n",
    "    def custom_sequence_MMD_loss(self, target_sequance_batch, predicted_sequance_batch):\n",
    "        \"\"\"\n",
    "        Calculating the Sequence MMD Loss between prediction and the ground Truth.\n",
    "        Additionally combining the last two dimensions \n",
    "        \"\"\"\n",
    "        mmd_loss = MMD.MMDLoss(kernel=self.mmd_kernel)\n",
    "        \n",
    "        total_batches = predicted_sequance_batch.shape[0]\n",
    "        frames_per_batch = predicted_sequance_batch.shape[1] * predicted_sequance_batch.shape[2]\n",
    "        \n",
    "        return mmd_loss(tf.reshape(predicted_sequance_batch, [total_batches, frames_per_batch]),\n",
    "                        tf.reshape(target_sequance_batch, [total_batches, frames_per_batch]))\n",
    "        \n",
    "    def MPJPE2(self, y_true, y_pred, number_of_joints = 32):\n",
    "        \"\"\"\n",
    "        Calculating the Mean Per Joint Position Error (MPJPE) between prediction and the ground Truth.\n",
    "        \"\"\"\n",
    "        yt= y_true.reshape((-1,number_of_joints,2))\n",
    "        yp= y_pred.reshape((-1,number_of_joints,2))\n",
    "        dist= np.zeros(yt.shape[0])\n",
    "        \n",
    "        for i in range(yt.shape[0]):\n",
    "            dist[i] = np.linalg.norm(yt[i] - yp[i])\n",
    "        \n",
    "        return np.mean(dist)\n",
    "    \n",
    "    def NPSS(self, euler_gt_sequences, euler_pred_sequences):\n",
    "        \"\"\"\n",
    "        A function to compute the Normalized Power Spectrum Similarity (NPSS) metric between predictions and the ground Truth [2] and [3].\n",
    "        \"\"\"        \n",
    "        # computing 1) fourier coeffs 2)power of fft 3) normalizing power of fft dim-wise 4) cumsum over freq. 5) EMD \n",
    "        gt_fourier_coeffs = np.zeros(euler_gt_sequences.shape, dtype = 'complex_')\n",
    "        pred_fourier_coeffs = np.zeros(euler_pred_sequences.shape, dtype = 'complex_')\n",
    "\n",
    "        # power vars\n",
    "        gt_power = np.zeros((gt_fourier_coeffs.shape))\n",
    "        pred_power = np.zeros((gt_fourier_coeffs.shape))\n",
    "\n",
    "        # normalizing power vars\n",
    "        gt_norm_power = np.zeros(gt_fourier_coeffs.shape)\n",
    "        pred_norm_power = np.zeros(gt_fourier_coeffs.shape)\n",
    "\n",
    "        cdf_gt_power = np.zeros(gt_norm_power.shape)\n",
    "        cdf_pred_power = np.zeros(pred_norm_power.shape)\n",
    "\n",
    "        emd = np.zeros(cdf_pred_power.shape[0:3:2])\n",
    "\n",
    "        # used to store powers of feature_dims and sequences used for avg later\n",
    "        seq_feature_power = np.zeros(euler_gt_sequences.shape[0:3:2])\n",
    "        power_weighted_emd = 0\n",
    "\n",
    "        for s in range(euler_gt_sequences.shape[0]):\n",
    "\n",
    "            for d in range(euler_gt_sequences.shape[2]):\n",
    "                gt_fourier_coeffs[s,:,d] = np.fft.fft(euler_gt_sequences[s,:,d]) # slice is 1D array\n",
    "                pred_fourier_coeffs[s,:,d] = np.fft.fft(euler_pred_sequences[s,:,d])\n",
    "\n",
    "                # computing power of fft per sequence per dim\n",
    "                gt_power[s,:,d] = np.square(np.absolute(gt_fourier_coeffs[s,:,d]))\n",
    "                pred_power[s,:,d] = np.square(np.absolute(pred_fourier_coeffs[s,:,d]))\n",
    "\n",
    "                # matching power of gt and pred sequences\n",
    "                gt_total_power = np.sum(gt_power[s,:,d])\n",
    "                pred_total_power = np.sum(pred_power[s,:,d])\n",
    "                #power_diff = gt_total_power - pred_total_power\n",
    "\n",
    "                # adding power diff to zero freq of pred seq\n",
    "                #pred_power[s,0,d] = pred_power[s,0,d] + power_diff\n",
    "\n",
    "                # computing seq_power and feature_dims power \n",
    "                seq_feature_power[s,d] = gt_total_power\n",
    "\n",
    "                # normalizing power per sequence per dim\n",
    "                if gt_total_power != 0:\n",
    "                    gt_norm_power[s,:,d] = gt_power[s,:,d] / gt_total_power \n",
    "\n",
    "                if pred_total_power !=0:\n",
    "                    pred_norm_power[s,:,d] = pred_power[s,:,d] / pred_total_power\n",
    "\n",
    "                # computing cumsum over freq\n",
    "                cdf_gt_power[s,:,d] = np.cumsum(gt_norm_power[s,:,d]) # slice is 1D\n",
    "                cdf_pred_power[s,:,d] = np.cumsum(pred_norm_power[s,:,d])\n",
    "\n",
    "                # computing EMD \n",
    "                emd[s,d] = np.linalg.norm((cdf_pred_power[s,:,d] - cdf_gt_power[s,:,d]), ord=1)\n",
    "\n",
    "        # computing weighted emd (by sequence and feature powers)\n",
    "        power_weighted_emd = np.average(emd, weights=seq_feature_power) \n",
    "\n",
    "        return power_weighted_emd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to start the experiment of training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "labA65KEakkZ"
   },
   "outputs": [],
   "source": [
    "def run_experiment(sampled_dataX, sampled_dataY, learning_rate=0.002, lambda1=0.5,\n",
    "                   lambda2=0.5, use_mse=False, use_MMD=False, metrics=None, output_diminsion=64,\n",
    "                   batch_size=100, epochs=50, validation_split=0.2, activation=\"relu\",\n",
    "                   dropout=0.25, exclude_locgen=False, interpolate_frames = 5, only_glogen = False,\n",
    "                   self_attention = False, cross_attention = False) :\n",
    "    \"\"\"\n",
    "    Method takes all hyperparameters as input paramters and returns the model and history as\n",
    "    a result\n",
    "    \"\"\"\n",
    "    glocal_model = GlocalNet(dense_activation=activation, LSTM_dropout=dropout,\n",
    "                             exclude_locgen=exclude_locgen, only_glogen=only_glogen,\n",
    "                             output_diminsion=output_diminsion, \n",
    "                             interpolation_frames = interpolate_frames,\n",
    "                             self_attention = self_attention, cross_attention = cross_attention)\n",
    "    if use_mse :\n",
    "        loss_function = tf.keras.losses.mean_squared_error\n",
    "    elif use_MMD :\n",
    "        loss_function = Loss().custom_sequence_MMD_loss\n",
    "    else :\n",
    "        loss_function = Loss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
    "\n",
    "    glocal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                         loss=loss_function, metrics=metrics, run_eagerly=False)\n",
    "    \n",
    "    history = glocal_model.fit(sampled_dataX, sampled_dataY, batch_size=batch_size, \n",
    "                               epochs=epochs, validation_split=validation_split)\n",
    "    \n",
    "    return history, glocal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to Resume the experiment of training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "eq3GlZ4NvHHz"
   },
   "outputs": [],
   "source": [
    "def resume_training(glocal_model, sampled_dataX, sampled_dataY, learning_rate=0.002, lambda1=0.5,\n",
    "                   lambda2=0.5, use_mse=False, use_MMD=False, metrics=None,\n",
    "                   batch_size=100, epochs=50, validation_split=0.2) :\n",
    "    \"\"\"\n",
    "    function to resume training of a model\n",
    "    \"\"\"\n",
    "    if use_mse :\n",
    "        loss_function = tf.keras.losses.mean_squared_error\n",
    "    elif use_MMD :\n",
    "        loss_function = Loss().custom_sequence_MMD_loss\n",
    "    else :\n",
    "        loss_function = Loss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
    "\n",
    "    glocal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss=loss_function, metrics=metrics)\n",
    "    \n",
    "    history = glocal_model.fit(sampled_dataX, sampled_dataY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs, validation_split=validation_split)\n",
    "    \n",
    "    return history, glocal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to Train Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_action_classifier(data, label, \n",
    "                            optimizer = tf.keras.optimizers.Adam(0.001), \n",
    "                            loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                            metrics = [tf.keras.metrics.CategoricalAccuracy()], \n",
    "                            epochs = 500, batch_size = 32, validation_split = 0.2):\n",
    "    \"\"\"\n",
    "    The function trains the Action Classifier and evaluate it on the dataset.\n",
    "    \"\"\"\n",
    "    print('Starting to Train Action Classifier...')\n",
    "    \n",
    "    #Initializing the action classifier model\n",
    "    action_classifer = ActionClassifier()\n",
    "    \n",
    "    #Compiling the model\n",
    "    action_classifer.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    #Fitting the model on the dataset\n",
    "    action_classifer.fit(data, label, epochs=epochs, batch_size=batch_size, verbose = 1, \n",
    "                         validation_split=validation_split, \n",
    "                         callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001)])\n",
    "    \n",
    "    print('\\nTraining Complete!\\n')\n",
    "    \n",
    "    #Evaluating the action classifier\n",
    "    score = action_classifer.evaluate(data, label)\n",
    "    print('Action Classifier Evaluation:\\nLoss: {}\\nAccuracy: {}'.format(score[0], score[1]))\n",
    "    \n",
    "    #Returning the trained action classifier\n",
    "    return action_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to visualize certain frames from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(lenght = 10, path_to_save = ''):\n",
    "    \"\"\"\n",
    "    Function to create and save a GIF from different number of frames\n",
    "    \"\"\"\n",
    "    list=[]\n",
    "    for l in range(lenght):\n",
    "        list.append(f'{path_to_save}_frame{l}.png')\n",
    "\n",
    "    with imageio.get_writer(f'{path_to_save}.gif', mode='I', duration=0.1) as writer:\n",
    "        for filename in list:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CIHR8VGhPfHf"
   },
   "outputs": [],
   "source": [
    "def visualize_frames(sample, dynamic_joints_only=False, num_frames_to_visualize=10, \n",
    "                    path_to_save=\"\", save_gif=False,\n",
    "                    joints_to_ignore=[4,5,9,10,11,16,20,21,22,23,24,28,29,30,31]) :\n",
    "    \"\"\"\n",
    "    Visualization function to draw a certain number of frames in a given sample\n",
    "    ignoring the joints mentioned in joints_to_ignore array\n",
    "    \"\"\"\n",
    "    assert num_frames_to_visualize <= sample.shape[0], f\"number of frames should be less than or equal to the total frames in the sample\"\n",
    "    \n",
    "    fig, axs = plt.subplots(ncols=num_frames_to_visualize, figsize=(40, 10))\n",
    "    fig.tight_layout(pad=1.0)\n",
    "    \n",
    "    for t in range(num_frames_to_visualize) :\n",
    "        #Removing unnecessary joints for visualization\n",
    "        if(dynamic_joints_only) :\n",
    "            #Check if no joints needs to be removed\n",
    "            truncated_frame = sample[t]\n",
    "        else :\n",
    "            #Removing the joints based on joints_to_ignore\n",
    "            joints_to_ignore_2d = [element * 2 for element in joints_to_ignore]\n",
    "            for i in range(len(joints_to_ignore_2d)) :\n",
    "                joints_to_ignore_2d.append(joints_to_ignore_2d[i]+1)\n",
    "            truncated_frame = np.delete(sample[t], joints_to_ignore_2d)   \n",
    "\n",
    "        #In case of including only moving joints for Human3.6M(17 joints)      \n",
    "        x_axis_array = truncated_frame[0:34:2]\n",
    "        y_axis_array = truncated_frame[1:35:2]\n",
    "        #Scattering all the 17 joints\n",
    "        axs[t].scatter(x_axis_array, y_axis_array)\n",
    "        #Plotting right leg\n",
    "        axs[t].plot(x_axis_array[:4], y_axis_array[:4], \"tab:blue\")\n",
    "        #plotting left leg\n",
    "        axs[t].plot(x_axis_array[[0, 4, 5, 6]], y_axis_array[[0, 4, 5, 6]])\n",
    "        #plotting from hip to head\n",
    "        axs[t].plot(x_axis_array[[0, 7, 8, 9, 10]], y_axis_array[[0, 7, 8, 9, 10]])\n",
    "        #plotting from neck to left shoulder\n",
    "        axs[t].plot(x_axis_array[[9, 11, 12, 13]], y_axis_array[[9, 11, 12, 13]])\n",
    "        #plotting from neck to right shoulder\n",
    "        axs[t].plot(x_axis_array[[9, 14, 15, 16]], y_axis_array[[9, 14, 15, 16]])\n",
    "        axs[t].invert_yaxis()\n",
    "        axs[t].set_xticks([])\n",
    "        axs[t].set_yticks([])\n",
    "        \n",
    "        if(len(path_to_save) > 0) :\n",
    "            extent = axs[t].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "            fig.savefig(f'{path_to_save}_frame{t}.png', bbox_inches=extent)\n",
    "    \n",
    "    if save_gif==True:\n",
    "        create_gif(lenght=num_frames_to_visualize,path_to_save=path_to_save)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Baseline Model for short term predictions (10 frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(Model_):\n",
    "    \"\"\"\n",
    "    simple many to many RNN model\n",
    "    \"\"\"\n",
    "    def __init__(self, output_diminsions=64, RNN_type=\"classical\", activation=\"relu\"):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        possible_RNN_types = [\"classical\", \"LSTM\", \"GRU\"]\n",
    "        assert RNN_type in possible_RNN_types, f\"RNN_type should be one of the valid values ['classical', 'LSTM', 'GRU']\"\n",
    "        if(RNN_type == \"classical\") :\n",
    "            self.ruccernt_layer = SimpleRNN(output_diminsions, return_sequences=True)\n",
    "        elif(RNN_type == \"LSTM\") : \n",
    "            self.ruccernt_layer = LSTM(output_diminsions, return_sequences=True)\n",
    "        else :\n",
    "            self.ruccernt_layer = GRU(output_diminsions, return_sequences=True)\n",
    "        self.dense_layer = TimeDistributed(Dense(output_diminsions,\n",
    "                                                        activation=activation)) \n",
    "\n",
    "    def call(self, input) :\n",
    "        output = self.ruccernt_layer(input)\n",
    "        output = self.dense_layer(output)\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to save Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveGlocalNetWeights(model, file_path) :\n",
    "    \"\"\"\n",
    "    A function to save all layers weights except for interpolation layer\n",
    "    \"\"\"\n",
    "    model_layers = np.array([], dtype=object)\n",
    "    for layer in model.layers :\n",
    "        model_layers = np.append(model_layers, layer.get_weights())\n",
    "    np.save(file_path, model_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGlocalNetFromFile(model : GlocalNet, model_weights_file, sample_input, output_diminsions=64,\n",
    "interpolation_frames=2, dense_activation=\"relu\") :\n",
    "    \"\"\"\n",
    "    function use presaved weights from file_path(model_weights_file) and uses sample_input to \n",
    "    build the model\n",
    "    \"\"\"\n",
    "    #Create an empty glocalNet model\n",
    "    model = GlocalNet(exclude_locgen=model.exclude_locgen, output_diminsion=output_diminsions,\n",
    "     interpolation_frames=interpolation_frames, dense_activation=dense_activation)\n",
    "    model_weights = np.load(model_weights_file, allow_pickle=True)\n",
    "    model(sample_input)\n",
    "    if(model.exclude_locgen) :\n",
    "        #Setting Glogen layers\n",
    "        layer0_weights = model_weights[:3]\n",
    "        layer1_weights = model_weights[3:6]\n",
    "        layer4_weights = model_weights[6:]\n",
    "        #Setting the Glogen layers\n",
    "        model.layers[0].set_weights(layer0_weights)\n",
    "        model.layers[1].set_weights(layer1_weights)\n",
    "        model.layers[4].set_weights(layer4_weights)\n",
    "    else :    \n",
    "        #Setting Glogen layers\n",
    "        layer0_weights = model_weights[:3]\n",
    "        layer1_weights = model_weights[3:6]\n",
    "        layer4_weights = model_weights[12:14]\n",
    "        #Setting Locgen layers\n",
    "        layer2_weights = model_weights[6:9]\n",
    "        layer3_weights = model_weights[9:12]\n",
    "        layer6_weights = model_weights[14:]\n",
    "            #Building the model to be able to set the layers' weights\n",
    "        model(sample_input)\n",
    "        #Setting the Glogen layers\n",
    "        model.layers[0].set_weights(layer0_weights)\n",
    "        model.layers[1].set_weights(layer1_weights)\n",
    "        model.layers[4].set_weights(layer4_weights)\n",
    "        #Setting locgen layers\n",
    "        model.layers[2].set_weights(layer2_weights)\n",
    "        model.layers[3].set_weights(layer3_weights)\n",
    "        model.layers[6].set_weights(layer6_weights)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the whole dataset for shortterm predictions\n",
    "sampled_data_short_term = Dataset_loading('./H3.6csv', sample_size=20, include_movable_joints=False).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading only the movable dataset points for shortterm predictions\n",
    "sampled_data_short_term_movable = Dataset_loading('./H3.6csv', sample_size=20, include_movable_joints=True).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "f8PgK1UVnkrw"
   },
   "outputs": [],
   "source": [
    "#For long term prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
    "sampled_data_all = Dataset_loading('./H3.6csv', sample_size=60, include_movable_joints=False).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For long term movable joints prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
    "sampled_data_movable = Dataset_loading('./H3.6csv', sample_size=60, include_movable_joints=True).read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset for Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_classifier, sampled_labels_classifier = Dataset_loading('./H3.6csv', sample_size=60, include_movable_joints=False, include_action_labels=False, return_action_labels=True).read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "h6bSQTwdn2Fo"
   },
   "outputs": [],
   "source": [
    "sampled_dataX_all, sampled_dataY_all = split_to_features_labels(sampled_data_all, input_sequance_size=10, total_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_movable, sampled_dataY_movable = split_to_features_labels(sampled_data_movable, input_sequance_size=10, total_features=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_short_term, sampled_dataY_short_term = split_to_features_labels(sampled_data_short_term, input_sequance_size=10, total_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_short_term_movable, sampled_dataY_short_term_movable = split_to_features_labels(sampled_data_short_term_movable, input_sequance_size=10, total_features=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with all features(short term)\n",
      "Total Samples: 77140\n",
      "Total Frames: 10\n",
      "Total Features: 64\n"
     ]
    }
   ],
   "source": [
    "print('Dataset with all features(short term)')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_short_term.shape[0],\n",
    "                                                                       sampled_dataY_short_term.shape[1],\n",
    "                                                                       sampled_dataY_short_term.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with Movable features(short term)\n",
      "Total Samples: 77140\n",
      "Total Frames: 10\n",
      "Total Features: 34\n"
     ]
    }
   ],
   "source": [
    "print('Dataset with Movable features(short term)')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_short_term_movable.shape[0],\n",
    "                                                                       sampled_dataY_short_term_movable.shape[1],\n",
    "                                                                       sampled_dataY_short_term_movable.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVXQiscVn4O2",
    "outputId": "dffaa998-a452-4358-d205-9754186aba0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with all features\n",
      "Total Samples: 25520\n",
      "Total Frames: 50\n",
      "Total Features: 64\n"
     ]
    }
   ],
   "source": [
    "print('Dataset with all features')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_all.shape[0],\n",
    "                                                                       sampled_dataY_all.shape[1],\n",
    "                                                                       sampled_dataY_all.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with only movable features\n",
      "Total Samples: 25520\n",
      "Total Frames: 50\n",
      "Total Features: 34\n"
     ]
    }
   ],
   "source": [
    "print('Dataset with only movable features')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_movable.shape[0],\n",
    "                                                                       sampled_dataY_movable.shape[1],\n",
    "                                                                       sampled_dataY_movable.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snGEPtNOKLvg"
   },
   "source": [
    "### Adding Noise and Downsampling to improve model performance and robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed and Downsampled Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Z9Bkt02CSh5P"
   },
   "outputs": [],
   "source": [
    "downsampled_data_all = downsampling(sampled_data_all, 'skip')\n",
    "downsampled_data_movable = downsampling(sampled_data_movable, 'skip')\n",
    "downsampled_data_shortterm_movable = downsampling(sampled_data_short_term_movable, 'skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "bsR__kGeSmNG"
   },
   "outputs": [],
   "source": [
    "downsampled_dataX_all, downsampled_dataY_all = split_to_features_labels(downsampled_data_all, input_sequance_size=10, total_features=64)\n",
    "preprocessed_downsampled_dataX_all, preprocessed_downsampled_dataY_all = preprocess_data(downsampled_dataX_all, downsampled_dataY_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataX_movable, downsampled_dataY_movable = split_to_features_labels(downsampled_data_movable, input_sequance_size=10, total_features=34)\n",
    "preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable = preprocess_data(downsampled_dataX_movable, downsampled_dataY_movable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataX_shortterm_movable, downsampled_dataX_shortterm_movable = split_to_features_labels(downsampled_data_shortterm_movable, input_sequance_size=5, total_features=34)\n",
    "preprocessed_downsampled_dataX_shortterm_movable, preprocessed_downsampled_dataY_shortterm_movable = preprocess_data(downsampled_dataX_shortterm_movable, downsampled_dataX_shortterm_movable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to Train Action Classifier...\n",
      "Epoch 1/500\n",
      "638/638 [==============================] - 22s 12ms/step - loss: 1.4815 - categorical_accuracy: 0.4536 - val_loss: 3.0594 - val_categorical_accuracy: 0.2220 - lr: 0.0010\n",
      "Epoch 2/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 1.2516 - categorical_accuracy: 0.5394 - val_loss: 1.8150 - val_categorical_accuracy: 0.3474 - lr: 0.0010\n",
      "Epoch 3/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 1.1409 - categorical_accuracy: 0.5773 - val_loss: 2.2221 - val_categorical_accuracy: 0.3335 - lr: 0.0010\n",
      "Epoch 4/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 1.0644 - categorical_accuracy: 0.6150 - val_loss: 2.7034 - val_categorical_accuracy: 0.2735 - lr: 0.0010\n",
      "Epoch 5/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 1.0124 - categorical_accuracy: 0.6321 - val_loss: 2.6622 - val_categorical_accuracy: 0.2855 - lr: 0.0010\n",
      "Epoch 6/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.9539 - categorical_accuracy: 0.6521 - val_loss: 2.3795 - val_categorical_accuracy: 0.3268 - lr: 0.0010\n",
      "Epoch 7/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.8986 - categorical_accuracy: 0.6730 - val_loss: 2.2929 - val_categorical_accuracy: 0.3045 - lr: 0.0010\n",
      "Epoch 8/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.8656 - categorical_accuracy: 0.6838 - val_loss: 2.5626 - val_categorical_accuracy: 0.3444 - lr: 0.0010\n",
      "Epoch 9/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.8122 - categorical_accuracy: 0.7052 - val_loss: 2.7657 - val_categorical_accuracy: 0.2706 - lr: 0.0010\n",
      "Epoch 10/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.7877 - categorical_accuracy: 0.7136 - val_loss: 4.2026 - val_categorical_accuracy: 0.3629 - lr: 0.0010\n",
      "Epoch 11/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.7436 - categorical_accuracy: 0.7330 - val_loss: 3.8375 - val_categorical_accuracy: 0.1889 - lr: 0.0010\n",
      "Epoch 12/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.7285 - categorical_accuracy: 0.7376 - val_loss: 2.8252 - val_categorical_accuracy: 0.3454 - lr: 0.0010\n",
      "Epoch 13/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.6936 - categorical_accuracy: 0.7536 - val_loss: 1.7928 - val_categorical_accuracy: 0.4461 - lr: 0.0010\n",
      "Epoch 14/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.6815 - categorical_accuracy: 0.7542 - val_loss: 2.7038 - val_categorical_accuracy: 0.3335 - lr: 0.0010\n",
      "Epoch 15/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.6477 - categorical_accuracy: 0.7693 - val_loss: 2.1541 - val_categorical_accuracy: 0.4030 - lr: 0.0010\n",
      "Epoch 16/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.6394 - categorical_accuracy: 0.7696 - val_loss: 2.6373 - val_categorical_accuracy: 0.3421 - lr: 0.0010\n",
      "Epoch 17/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.6151 - categorical_accuracy: 0.7782 - val_loss: 3.5622 - val_categorical_accuracy: 0.3288 - lr: 0.0010\n",
      "Epoch 18/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.6009 - categorical_accuracy: 0.7830 - val_loss: 4.4080 - val_categorical_accuracy: 0.2553 - lr: 0.0010\n",
      "Epoch 19/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.5835 - categorical_accuracy: 0.7899 - val_loss: 2.0361 - val_categorical_accuracy: 0.4246 - lr: 0.0010\n",
      "Epoch 20/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.5630 - categorical_accuracy: 0.7966 - val_loss: 2.1353 - val_categorical_accuracy: 0.4075 - lr: 0.0010\n",
      "Epoch 21/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.5553 - categorical_accuracy: 0.7991 - val_loss: 3.3503 - val_categorical_accuracy: 0.3499 - lr: 0.0010\n",
      "Epoch 22/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.5405 - categorical_accuracy: 0.8076 - val_loss: 3.8168 - val_categorical_accuracy: 0.3246 - lr: 0.0010\n",
      "Epoch 23/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.5217 - categorical_accuracy: 0.8113 - val_loss: 4.3323 - val_categorical_accuracy: 0.2696 - lr: 0.0010\n",
      "Epoch 24/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.5163 - categorical_accuracy: 0.8146 - val_loss: 3.5620 - val_categorical_accuracy: 0.3380 - lr: 0.0010\n",
      "Epoch 25/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.5034 - categorical_accuracy: 0.8176 - val_loss: 2.6169 - val_categorical_accuracy: 0.3870 - lr: 0.0010\n",
      "Epoch 26/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.4881 - categorical_accuracy: 0.8248 - val_loss: 3.0121 - val_categorical_accuracy: 0.3354 - lr: 0.0010\n",
      "Epoch 27/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.4817 - categorical_accuracy: 0.8267 - val_loss: 2.3568 - val_categorical_accuracy: 0.3783 - lr: 0.0010\n",
      "Epoch 28/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.4689 - categorical_accuracy: 0.8309 - val_loss: 5.1952 - val_categorical_accuracy: 0.3078 - lr: 0.0010\n",
      "Epoch 29/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.4640 - categorical_accuracy: 0.8328 - val_loss: 2.3580 - val_categorical_accuracy: 0.4069 - lr: 0.0010\n",
      "Epoch 30/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.4454 - categorical_accuracy: 0.8396 - val_loss: 2.6782 - val_categorical_accuracy: 0.3885 - lr: 0.0010\n",
      "Epoch 31/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.4480 - categorical_accuracy: 0.8383 - val_loss: 3.3030 - val_categorical_accuracy: 0.3419 - lr: 0.0010\n",
      "Epoch 32/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.4323 - categorical_accuracy: 0.8449 - val_loss: 2.6712 - val_categorical_accuracy: 0.3736 - lr: 0.0010\n",
      "Epoch 33/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.4263 - categorical_accuracy: 0.8498 - val_loss: 3.9156 - val_categorical_accuracy: 0.3507 - lr: 0.0010\n",
      "Epoch 34/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.3507 - categorical_accuracy: 0.8772 - val_loss: 2.3334 - val_categorical_accuracy: 0.4920 - lr: 5.0000e-04\n",
      "Epoch 35/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.3373 - categorical_accuracy: 0.8793 - val_loss: 2.2269 - val_categorical_accuracy: 0.4516 - lr: 5.0000e-04\n",
      "Epoch 36/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.3210 - categorical_accuracy: 0.8855 - val_loss: 1.7218 - val_categorical_accuracy: 0.5296 - lr: 5.0000e-04\n",
      "Epoch 37/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.3280 - categorical_accuracy: 0.8826 - val_loss: 2.4341 - val_categorical_accuracy: 0.4812 - lr: 5.0000e-04\n",
      "Epoch 38/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.3227 - categorical_accuracy: 0.8821 - val_loss: 2.2618 - val_categorical_accuracy: 0.5010 - lr: 5.0000e-04\n",
      "Epoch 39/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.3156 - categorical_accuracy: 0.8834 - val_loss: 2.5165 - val_categorical_accuracy: 0.4303 - lr: 5.0000e-04\n",
      "Epoch 40/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.3106 - categorical_accuracy: 0.8893 - val_loss: 2.9393 - val_categorical_accuracy: 0.4626 - lr: 5.0000e-04\n",
      "Epoch 41/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.3155 - categorical_accuracy: 0.8870 - val_loss: 2.5499 - val_categorical_accuracy: 0.4406 - lr: 5.0000e-04\n",
      "Epoch 42/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2956 - categorical_accuracy: 0.8948 - val_loss: 2.4340 - val_categorical_accuracy: 0.4594 - lr: 5.0000e-04\n",
      "Epoch 43/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.2901 - categorical_accuracy: 0.8971 - val_loss: 2.0639 - val_categorical_accuracy: 0.4916 - lr: 5.0000e-04\n",
      "Epoch 44/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2966 - categorical_accuracy: 0.8958 - val_loss: 2.3052 - val_categorical_accuracy: 0.4714 - lr: 5.0000e-04\n",
      "Epoch 45/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2857 - categorical_accuracy: 0.8993 - val_loss: 2.5155 - val_categorical_accuracy: 0.4575 - lr: 5.0000e-04\n",
      "Epoch 46/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2846 - categorical_accuracy: 0.8987 - val_loss: 2.8908 - val_categorical_accuracy: 0.4610 - lr: 5.0000e-04\n",
      "Epoch 47/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2814 - categorical_accuracy: 0.9001 - val_loss: 2.5529 - val_categorical_accuracy: 0.4152 - lr: 5.0000e-04\n",
      "Epoch 48/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2780 - categorical_accuracy: 0.9021 - val_loss: 2.2848 - val_categorical_accuracy: 0.4918 - lr: 5.0000e-04\n",
      "Epoch 49/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2673 - categorical_accuracy: 0.9046 - val_loss: 2.7246 - val_categorical_accuracy: 0.4675 - lr: 5.0000e-04\n",
      "Epoch 50/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2710 - categorical_accuracy: 0.9014 - val_loss: 3.3918 - val_categorical_accuracy: 0.4273 - lr: 5.0000e-04\n",
      "Epoch 51/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2647 - categorical_accuracy: 0.9052 - val_loss: 4.1850 - val_categorical_accuracy: 0.4269 - lr: 5.0000e-04\n",
      "Epoch 52/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2665 - categorical_accuracy: 0.9042 - val_loss: 3.1603 - val_categorical_accuracy: 0.4130 - lr: 5.0000e-04\n",
      "Epoch 53/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.2710 - categorical_accuracy: 0.9031 - val_loss: 2.5509 - val_categorical_accuracy: 0.4610 - lr: 5.0000e-04\n",
      "Epoch 54/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.2633 - categorical_accuracy: 0.9068 - val_loss: 2.5562 - val_categorical_accuracy: 0.4955 - lr: 5.0000e-04\n",
      "Epoch 55/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.2678 - categorical_accuracy: 0.9059 - val_loss: 2.7799 - val_categorical_accuracy: 0.4440 - lr: 5.0000e-04\n",
      "Epoch 56/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.2482 - categorical_accuracy: 0.9135 - val_loss: 2.4368 - val_categorical_accuracy: 0.5035 - lr: 5.0000e-04\n",
      "Epoch 57/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.2143 - categorical_accuracy: 0.9239 - val_loss: 1.9238 - val_categorical_accuracy: 0.5449 - lr: 2.5000e-04\n",
      "Epoch 58/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.2066 - categorical_accuracy: 0.9275 - val_loss: 2.2205 - val_categorical_accuracy: 0.5315 - lr: 2.5000e-04\n",
      "Epoch 59/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.2017 - categorical_accuracy: 0.9292 - val_loss: 2.1551 - val_categorical_accuracy: 0.5433 - lr: 2.5000e-04\n",
      "Epoch 60/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.2014 - categorical_accuracy: 0.9293 - val_loss: 2.5850 - val_categorical_accuracy: 0.5053 - lr: 2.5000e-04\n",
      "Epoch 61/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.2014 - categorical_accuracy: 0.9297 - val_loss: 2.5294 - val_categorical_accuracy: 0.4900 - lr: 2.5000e-04\n",
      "Epoch 62/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1985 - categorical_accuracy: 0.9304 - val_loss: 2.5332 - val_categorical_accuracy: 0.5274 - lr: 2.5000e-04\n",
      "Epoch 63/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1959 - categorical_accuracy: 0.9318 - val_loss: 2.2185 - val_categorical_accuracy: 0.5198 - lr: 2.5000e-04\n",
      "Epoch 64/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1954 - categorical_accuracy: 0.9312 - val_loss: 2.4047 - val_categorical_accuracy: 0.5225 - lr: 2.5000e-04\n",
      "Epoch 65/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1963 - categorical_accuracy: 0.9310 - val_loss: 2.2214 - val_categorical_accuracy: 0.5147 - lr: 2.5000e-04\n",
      "Epoch 66/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1854 - categorical_accuracy: 0.9375 - val_loss: 2.2825 - val_categorical_accuracy: 0.5239 - lr: 2.5000e-04\n",
      "Epoch 67/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1970 - categorical_accuracy: 0.9313 - val_loss: 2.3677 - val_categorical_accuracy: 0.5453 - lr: 2.5000e-04\n",
      "Epoch 68/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1920 - categorical_accuracy: 0.9303 - val_loss: 2.2874 - val_categorical_accuracy: 0.5176 - lr: 2.5000e-04\n",
      "Epoch 69/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1925 - categorical_accuracy: 0.9327 - val_loss: 2.4973 - val_categorical_accuracy: 0.5398 - lr: 2.5000e-04\n",
      "Epoch 70/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1837 - categorical_accuracy: 0.9344 - val_loss: 3.0996 - val_categorical_accuracy: 0.4671 - lr: 2.5000e-04\n",
      "Epoch 71/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1843 - categorical_accuracy: 0.9355 - val_loss: 2.8075 - val_categorical_accuracy: 0.5080 - lr: 2.5000e-04\n",
      "Epoch 72/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1859 - categorical_accuracy: 0.9345 - val_loss: 2.6235 - val_categorical_accuracy: 0.5072 - lr: 2.5000e-04\n",
      "Epoch 73/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1861 - categorical_accuracy: 0.9359 - val_loss: 2.3042 - val_categorical_accuracy: 0.5400 - lr: 2.5000e-04\n",
      "Epoch 74/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1844 - categorical_accuracy: 0.9343 - val_loss: 2.1839 - val_categorical_accuracy: 0.5298 - lr: 2.5000e-04\n",
      "Epoch 75/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1813 - categorical_accuracy: 0.9373 - val_loss: 2.3609 - val_categorical_accuracy: 0.5321 - lr: 2.5000e-04\n",
      "Epoch 76/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1739 - categorical_accuracy: 0.9388 - val_loss: 2.3816 - val_categorical_accuracy: 0.5384 - lr: 2.5000e-04\n",
      "Epoch 77/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1626 - categorical_accuracy: 0.9443 - val_loss: 2.0219 - val_categorical_accuracy: 0.5541 - lr: 1.2500e-04\n",
      "Epoch 78/500\n",
      "638/638 [==============================] - 10s 15ms/step - loss: 0.1622 - categorical_accuracy: 0.9425 - val_loss: 2.1577 - val_categorical_accuracy: 0.5451 - lr: 1.2500e-04\n",
      "Epoch 79/500\n",
      "638/638 [==============================] - 14s 23ms/step - loss: 0.1562 - categorical_accuracy: 0.9470 - val_loss: 2.1322 - val_categorical_accuracy: 0.5533 - lr: 1.2500e-04\n",
      "Epoch 80/500\n",
      "638/638 [==============================] - 13s 20ms/step - loss: 0.1541 - categorical_accuracy: 0.9471 - val_loss: 2.4323 - val_categorical_accuracy: 0.5359 - lr: 1.2500e-04\n",
      "Epoch 81/500\n",
      "638/638 [==============================] - 13s 20ms/step - loss: 0.1514 - categorical_accuracy: 0.9480 - val_loss: 2.2526 - val_categorical_accuracy: 0.5468 - lr: 1.2500e-04\n",
      "Epoch 82/500\n",
      "638/638 [==============================] - 13s 20ms/step - loss: 0.1546 - categorical_accuracy: 0.9460 - val_loss: 2.3794 - val_categorical_accuracy: 0.5425 - lr: 1.2500e-04\n",
      "Epoch 83/500\n",
      "638/638 [==============================] - 12s 19ms/step - loss: 0.1544 - categorical_accuracy: 0.9454 - val_loss: 2.2843 - val_categorical_accuracy: 0.5404 - lr: 1.2500e-04\n",
      "Epoch 84/500\n",
      "638/638 [==============================] - 10s 15ms/step - loss: 0.1524 - categorical_accuracy: 0.9478 - val_loss: 2.2546 - val_categorical_accuracy: 0.5437 - lr: 1.2500e-04\n",
      "Epoch 85/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1440 - categorical_accuracy: 0.9521 - val_loss: 2.6267 - val_categorical_accuracy: 0.5513 - lr: 1.2500e-04\n",
      "Epoch 86/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1473 - categorical_accuracy: 0.9493 - val_loss: 2.4091 - val_categorical_accuracy: 0.5286 - lr: 1.2500e-04\n",
      "Epoch 87/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1449 - categorical_accuracy: 0.9495 - val_loss: 2.3964 - val_categorical_accuracy: 0.5214 - lr: 1.2500e-04\n",
      "Epoch 88/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1466 - categorical_accuracy: 0.9500 - val_loss: 2.2740 - val_categorical_accuracy: 0.5180 - lr: 1.2500e-04\n",
      "Epoch 89/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1446 - categorical_accuracy: 0.9489 - val_loss: 2.4645 - val_categorical_accuracy: 0.5157 - lr: 1.2500e-04\n",
      "Epoch 90/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1447 - categorical_accuracy: 0.9492 - val_loss: 2.6183 - val_categorical_accuracy: 0.5370 - lr: 1.2500e-04\n",
      "Epoch 91/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1449 - categorical_accuracy: 0.9508 - val_loss: 2.3513 - val_categorical_accuracy: 0.5431 - lr: 1.2500e-04\n",
      "Epoch 92/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1443 - categorical_accuracy: 0.9491 - val_loss: 2.4203 - val_categorical_accuracy: 0.5639 - lr: 1.2500e-04\n",
      "Epoch 93/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1450 - categorical_accuracy: 0.9503 - val_loss: 2.6345 - val_categorical_accuracy: 0.5235 - lr: 1.2500e-04\n",
      "Epoch 94/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1442 - categorical_accuracy: 0.9489 - val_loss: 2.7618 - val_categorical_accuracy: 0.5057 - lr: 1.2500e-04\n",
      "Epoch 95/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1460 - categorical_accuracy: 0.9497 - val_loss: 2.5761 - val_categorical_accuracy: 0.5118 - lr: 1.2500e-04\n",
      "Epoch 96/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1451 - categorical_accuracy: 0.9506 - val_loss: 2.1830 - val_categorical_accuracy: 0.5523 - lr: 1.2500e-04\n",
      "Epoch 97/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1390 - categorical_accuracy: 0.9531 - val_loss: 2.1703 - val_categorical_accuracy: 0.5596 - lr: 1.0000e-04\n",
      "Epoch 98/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1389 - categorical_accuracy: 0.9515 - val_loss: 2.0715 - val_categorical_accuracy: 0.5705 - lr: 1.0000e-04\n",
      "Epoch 99/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1412 - categorical_accuracy: 0.9519 - val_loss: 2.3730 - val_categorical_accuracy: 0.5296 - lr: 1.0000e-04\n",
      "Epoch 100/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1385 - categorical_accuracy: 0.9516 - val_loss: 2.7831 - val_categorical_accuracy: 0.5155 - lr: 1.0000e-04\n",
      "Epoch 101/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1388 - categorical_accuracy: 0.9533 - val_loss: 2.3703 - val_categorical_accuracy: 0.5425 - lr: 1.0000e-04\n",
      "Epoch 102/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1358 - categorical_accuracy: 0.9541 - val_loss: 2.6828 - val_categorical_accuracy: 0.5239 - lr: 1.0000e-04\n",
      "Epoch 103/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1356 - categorical_accuracy: 0.9522 - val_loss: 2.4058 - val_categorical_accuracy: 0.5658 - lr: 1.0000e-04\n",
      "Epoch 104/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1307 - categorical_accuracy: 0.9555 - val_loss: 2.1775 - val_categorical_accuracy: 0.5533 - lr: 1.0000e-04\n",
      "Epoch 105/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1339 - categorical_accuracy: 0.9539 - val_loss: 2.3414 - val_categorical_accuracy: 0.5406 - lr: 1.0000e-04\n",
      "Epoch 106/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1303 - categorical_accuracy: 0.9562 - val_loss: 2.3476 - val_categorical_accuracy: 0.5484 - lr: 1.0000e-04\n",
      "Epoch 107/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1336 - categorical_accuracy: 0.9539 - val_loss: 2.2467 - val_categorical_accuracy: 0.5458 - lr: 1.0000e-04\n",
      "Epoch 108/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.1328 - categorical_accuracy: 0.9536 - val_loss: 2.3000 - val_categorical_accuracy: 0.5378 - lr: 1.0000e-04\n",
      "Epoch 109/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1360 - categorical_accuracy: 0.9518 - val_loss: 2.3313 - val_categorical_accuracy: 0.5537 - lr: 1.0000e-04\n",
      "Epoch 110/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1285 - categorical_accuracy: 0.9560 - val_loss: 2.5549 - val_categorical_accuracy: 0.5347 - lr: 1.0000e-04\n",
      "Epoch 111/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1336 - categorical_accuracy: 0.9521 - val_loss: 2.1809 - val_categorical_accuracy: 0.5674 - lr: 1.0000e-04\n",
      "Epoch 112/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1366 - categorical_accuracy: 0.9543 - val_loss: 2.3378 - val_categorical_accuracy: 0.5523 - lr: 1.0000e-04\n",
      "Epoch 113/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1311 - categorical_accuracy: 0.9552 - val_loss: 2.6449 - val_categorical_accuracy: 0.5174 - lr: 1.0000e-04\n",
      "Epoch 114/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1267 - categorical_accuracy: 0.9558 - val_loss: 2.3475 - val_categorical_accuracy: 0.5435 - lr: 1.0000e-04\n",
      "Epoch 115/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1309 - categorical_accuracy: 0.9546 - val_loss: 2.4252 - val_categorical_accuracy: 0.5631 - lr: 1.0000e-04\n",
      "Epoch 116/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1287 - categorical_accuracy: 0.9558 - val_loss: 2.1884 - val_categorical_accuracy: 0.5600 - lr: 1.0000e-04\n",
      "Epoch 117/500\n",
      "638/638 [==============================] - 9s 13ms/step - loss: 0.1379 - categorical_accuracy: 0.9534 - val_loss: 2.3245 - val_categorical_accuracy: 0.5302 - lr: 1.0000e-04\n",
      "Epoch 118/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1257 - categorical_accuracy: 0.9562 - val_loss: 2.5343 - val_categorical_accuracy: 0.5172 - lr: 1.0000e-04\n",
      "Epoch 119/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1260 - categorical_accuracy: 0.9583 - val_loss: 2.5454 - val_categorical_accuracy: 0.5386 - lr: 1.0000e-04\n",
      "Epoch 120/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1254 - categorical_accuracy: 0.9558 - val_loss: 2.2778 - val_categorical_accuracy: 0.5533 - lr: 1.0000e-04\n",
      "Epoch 121/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1310 - categorical_accuracy: 0.9543 - val_loss: 2.8750 - val_categorical_accuracy: 0.5276 - lr: 1.0000e-04\n",
      "Epoch 122/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1338 - categorical_accuracy: 0.9535 - val_loss: 2.3206 - val_categorical_accuracy: 0.5541 - lr: 1.0000e-04\n",
      "Epoch 123/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1239 - categorical_accuracy: 0.9563 - val_loss: 2.2942 - val_categorical_accuracy: 0.5409 - lr: 1.0000e-04\n",
      "Epoch 124/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1277 - categorical_accuracy: 0.9566 - val_loss: 2.3227 - val_categorical_accuracy: 0.5584 - lr: 1.0000e-04\n",
      "Epoch 125/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1258 - categorical_accuracy: 0.9562 - val_loss: 2.3692 - val_categorical_accuracy: 0.5505 - lr: 1.0000e-04\n",
      "Epoch 126/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1243 - categorical_accuracy: 0.9562 - val_loss: 2.6244 - val_categorical_accuracy: 0.5404 - lr: 1.0000e-04\n",
      "Epoch 127/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1251 - categorical_accuracy: 0.9570 - val_loss: 2.3368 - val_categorical_accuracy: 0.5507 - lr: 1.0000e-04\n",
      "Epoch 128/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1281 - categorical_accuracy: 0.9564 - val_loss: 2.8599 - val_categorical_accuracy: 0.5182 - lr: 1.0000e-04\n",
      "Epoch 129/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1211 - categorical_accuracy: 0.9574 - val_loss: 2.5882 - val_categorical_accuracy: 0.5349 - lr: 1.0000e-04\n",
      "Epoch 130/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1251 - categorical_accuracy: 0.9562 - val_loss: 2.4042 - val_categorical_accuracy: 0.5635 - lr: 1.0000e-04\n",
      "Epoch 131/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1264 - categorical_accuracy: 0.9575 - val_loss: 2.4837 - val_categorical_accuracy: 0.5408 - lr: 1.0000e-04\n",
      "Epoch 132/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1260 - categorical_accuracy: 0.9567 - val_loss: 2.2457 - val_categorical_accuracy: 0.5564 - lr: 1.0000e-04\n",
      "Epoch 133/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1216 - categorical_accuracy: 0.9591 - val_loss: 2.2631 - val_categorical_accuracy: 0.5517 - lr: 1.0000e-04\n",
      "Epoch 134/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1231 - categorical_accuracy: 0.9567 - val_loss: 2.7301 - val_categorical_accuracy: 0.5502 - lr: 1.0000e-04\n",
      "Epoch 135/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1266 - categorical_accuracy: 0.9544 - val_loss: 2.5299 - val_categorical_accuracy: 0.5417 - lr: 1.0000e-04\n",
      "Epoch 136/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1246 - categorical_accuracy: 0.9577 - val_loss: 2.4487 - val_categorical_accuracy: 0.5306 - lr: 1.0000e-04\n",
      "Epoch 137/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1196 - categorical_accuracy: 0.9597 - val_loss: 2.3424 - val_categorical_accuracy: 0.5545 - lr: 1.0000e-04\n",
      "Epoch 138/500\n",
      "638/638 [==============================] - 9s 13ms/step - loss: 0.1274 - categorical_accuracy: 0.9567 - val_loss: 2.7048 - val_categorical_accuracy: 0.5470 - lr: 1.0000e-04\n",
      "Epoch 139/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1191 - categorical_accuracy: 0.9594 - val_loss: 2.4191 - val_categorical_accuracy: 0.5488 - lr: 1.0000e-04\n",
      "Epoch 140/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1210 - categorical_accuracy: 0.9576 - val_loss: 2.7561 - val_categorical_accuracy: 0.5216 - lr: 1.0000e-04\n",
      "Epoch 141/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.1192 - categorical_accuracy: 0.9588 - val_loss: 2.7338 - val_categorical_accuracy: 0.5327 - lr: 1.0000e-04\n",
      "Epoch 142/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1244 - categorical_accuracy: 0.9571 - val_loss: 3.0433 - val_categorical_accuracy: 0.5065 - lr: 1.0000e-04\n",
      "Epoch 143/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1201 - categorical_accuracy: 0.9581 - val_loss: 2.4720 - val_categorical_accuracy: 0.5590 - lr: 1.0000e-04\n",
      "Epoch 144/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1199 - categorical_accuracy: 0.9587 - val_loss: 2.4227 - val_categorical_accuracy: 0.5562 - lr: 1.0000e-04\n",
      "Epoch 145/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1183 - categorical_accuracy: 0.9590 - val_loss: 2.3927 - val_categorical_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 146/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1219 - categorical_accuracy: 0.9576 - val_loss: 2.8082 - val_categorical_accuracy: 0.5243 - lr: 1.0000e-04\n",
      "Epoch 147/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1197 - categorical_accuracy: 0.9594 - val_loss: 2.7632 - val_categorical_accuracy: 0.5229 - lr: 1.0000e-04\n",
      "Epoch 148/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1207 - categorical_accuracy: 0.9582 - val_loss: 2.4951 - val_categorical_accuracy: 0.5292 - lr: 1.0000e-04\n",
      "Epoch 149/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.1135 - categorical_accuracy: 0.9607 - val_loss: 2.3730 - val_categorical_accuracy: 0.5676 - lr: 1.0000e-04\n",
      "Epoch 150/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1151 - categorical_accuracy: 0.9609 - val_loss: 2.5960 - val_categorical_accuracy: 0.5476 - lr: 1.0000e-04\n",
      "Epoch 151/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1186 - categorical_accuracy: 0.9587 - val_loss: 2.7823 - val_categorical_accuracy: 0.5233 - lr: 1.0000e-04\n",
      "Epoch 152/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.1255 - categorical_accuracy: 0.9566 - val_loss: 2.6138 - val_categorical_accuracy: 0.5453 - lr: 1.0000e-04\n",
      "Epoch 153/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.1208 - categorical_accuracy: 0.9577 - val_loss: 2.4789 - val_categorical_accuracy: 0.5451 - lr: 1.0000e-04\n",
      "Epoch 154/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1179 - categorical_accuracy: 0.9588 - val_loss: 2.4477 - val_categorical_accuracy: 0.5462 - lr: 1.0000e-04\n",
      "Epoch 155/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1174 - categorical_accuracy: 0.9590 - val_loss: 2.7112 - val_categorical_accuracy: 0.5255 - lr: 1.0000e-04\n",
      "Epoch 156/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1162 - categorical_accuracy: 0.9599 - val_loss: 2.4481 - val_categorical_accuracy: 0.5539 - lr: 1.0000e-04\n",
      "Epoch 157/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.1126 - categorical_accuracy: 0.9603 - val_loss: 2.4635 - val_categorical_accuracy: 0.5574 - lr: 1.0000e-04\n",
      "Epoch 158/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1139 - categorical_accuracy: 0.9612 - val_loss: 2.3965 - val_categorical_accuracy: 0.5627 - lr: 1.0000e-04\n",
      "Epoch 159/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1144 - categorical_accuracy: 0.9614 - val_loss: 2.8392 - val_categorical_accuracy: 0.5413 - lr: 1.0000e-04\n",
      "Epoch 160/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1181 - categorical_accuracy: 0.9574 - val_loss: 2.5888 - val_categorical_accuracy: 0.5398 - lr: 1.0000e-04\n",
      "Epoch 161/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1133 - categorical_accuracy: 0.9617 - val_loss: 2.5109 - val_categorical_accuracy: 0.5445 - lr: 1.0000e-04\n",
      "Epoch 162/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1163 - categorical_accuracy: 0.9598 - val_loss: 2.8097 - val_categorical_accuracy: 0.5341 - lr: 1.0000e-04\n",
      "Epoch 163/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1090 - categorical_accuracy: 0.9635 - val_loss: 2.3805 - val_categorical_accuracy: 0.5366 - lr: 1.0000e-04\n",
      "Epoch 164/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1145 - categorical_accuracy: 0.9607 - val_loss: 2.9756 - val_categorical_accuracy: 0.5251 - lr: 1.0000e-04\n",
      "Epoch 165/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1155 - categorical_accuracy: 0.9613 - val_loss: 2.4581 - val_categorical_accuracy: 0.5413 - lr: 1.0000e-04\n",
      "Epoch 166/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1133 - categorical_accuracy: 0.9605 - val_loss: 2.8454 - val_categorical_accuracy: 0.5155 - lr: 1.0000e-04\n",
      "Epoch 167/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1108 - categorical_accuracy: 0.9616 - val_loss: 2.6789 - val_categorical_accuracy: 0.5482 - lr: 1.0000e-04\n",
      "Epoch 168/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1098 - categorical_accuracy: 0.9618 - val_loss: 2.5482 - val_categorical_accuracy: 0.5298 - lr: 1.0000e-04\n",
      "Epoch 169/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1134 - categorical_accuracy: 0.9592 - val_loss: 2.5919 - val_categorical_accuracy: 0.5359 - lr: 1.0000e-04\n",
      "Epoch 170/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1056 - categorical_accuracy: 0.9625 - val_loss: 2.8090 - val_categorical_accuracy: 0.5288 - lr: 1.0000e-04\n",
      "Epoch 171/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1160 - categorical_accuracy: 0.9597 - val_loss: 2.5637 - val_categorical_accuracy: 0.5268 - lr: 1.0000e-04\n",
      "Epoch 172/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1138 - categorical_accuracy: 0.9610 - val_loss: 2.9263 - val_categorical_accuracy: 0.5141 - lr: 1.0000e-04\n",
      "Epoch 173/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1076 - categorical_accuracy: 0.9635 - val_loss: 2.4294 - val_categorical_accuracy: 0.5662 - lr: 1.0000e-04\n",
      "Epoch 174/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1069 - categorical_accuracy: 0.9639 - val_loss: 2.4741 - val_categorical_accuracy: 0.5535 - lr: 1.0000e-04\n",
      "Epoch 175/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1087 - categorical_accuracy: 0.9618 - val_loss: 2.5192 - val_categorical_accuracy: 0.5374 - lr: 1.0000e-04\n",
      "Epoch 176/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1110 - categorical_accuracy: 0.9606 - val_loss: 3.0717 - val_categorical_accuracy: 0.5464 - lr: 1.0000e-04\n",
      "Epoch 177/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1149 - categorical_accuracy: 0.9592 - val_loss: 2.5050 - val_categorical_accuracy: 0.5505 - lr: 1.0000e-04\n",
      "Epoch 178/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1112 - categorical_accuracy: 0.9621 - val_loss: 2.9589 - val_categorical_accuracy: 0.5257 - lr: 1.0000e-04\n",
      "Epoch 179/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1065 - categorical_accuracy: 0.9633 - val_loss: 2.6530 - val_categorical_accuracy: 0.5460 - lr: 1.0000e-04\n",
      "Epoch 180/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1087 - categorical_accuracy: 0.9624 - val_loss: 2.7142 - val_categorical_accuracy: 0.5419 - lr: 1.0000e-04\n",
      "Epoch 181/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.1058 - categorical_accuracy: 0.9635 - val_loss: 2.6555 - val_categorical_accuracy: 0.5504 - lr: 1.0000e-04\n",
      "Epoch 182/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.1110 - categorical_accuracy: 0.9615 - val_loss: 2.6267 - val_categorical_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 183/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1071 - categorical_accuracy: 0.9626 - val_loss: 2.9090 - val_categorical_accuracy: 0.5312 - lr: 1.0000e-04\n",
      "Epoch 184/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1077 - categorical_accuracy: 0.9632 - val_loss: 2.9802 - val_categorical_accuracy: 0.5396 - lr: 1.0000e-04\n",
      "Epoch 185/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1057 - categorical_accuracy: 0.9639 - val_loss: 2.5412 - val_categorical_accuracy: 0.5515 - lr: 1.0000e-04\n",
      "Epoch 186/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1030 - categorical_accuracy: 0.9655 - val_loss: 2.4783 - val_categorical_accuracy: 0.5531 - lr: 1.0000e-04\n",
      "Epoch 187/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1024 - categorical_accuracy: 0.9640 - val_loss: 2.5982 - val_categorical_accuracy: 0.5603 - lr: 1.0000e-04\n",
      "Epoch 188/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1070 - categorical_accuracy: 0.9632 - val_loss: 2.7561 - val_categorical_accuracy: 0.5347 - lr: 1.0000e-04\n",
      "Epoch 189/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1100 - categorical_accuracy: 0.9613 - val_loss: 2.6864 - val_categorical_accuracy: 0.5392 - lr: 1.0000e-04\n",
      "Epoch 190/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1080 - categorical_accuracy: 0.9619 - val_loss: 2.4780 - val_categorical_accuracy: 0.5613 - lr: 1.0000e-04\n",
      "Epoch 191/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1038 - categorical_accuracy: 0.9634 - val_loss: 2.5092 - val_categorical_accuracy: 0.5576 - lr: 1.0000e-04\n",
      "Epoch 192/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1085 - categorical_accuracy: 0.9624 - val_loss: 2.7268 - val_categorical_accuracy: 0.5378 - lr: 1.0000e-04\n",
      "Epoch 193/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1059 - categorical_accuracy: 0.9644 - val_loss: 2.6600 - val_categorical_accuracy: 0.5366 - lr: 1.0000e-04\n",
      "Epoch 194/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1057 - categorical_accuracy: 0.9636 - val_loss: 2.8052 - val_categorical_accuracy: 0.5507 - lr: 1.0000e-04\n",
      "Epoch 195/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1048 - categorical_accuracy: 0.9640 - val_loss: 2.9525 - val_categorical_accuracy: 0.5364 - lr: 1.0000e-04\n",
      "Epoch 196/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1023 - categorical_accuracy: 0.9649 - val_loss: 2.5277 - val_categorical_accuracy: 0.5605 - lr: 1.0000e-04\n",
      "Epoch 197/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1048 - categorical_accuracy: 0.9645 - val_loss: 2.7314 - val_categorical_accuracy: 0.5182 - lr: 1.0000e-04\n",
      "Epoch 198/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1045 - categorical_accuracy: 0.9636 - val_loss: 2.2428 - val_categorical_accuracy: 0.5623 - lr: 1.0000e-04\n",
      "Epoch 199/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1038 - categorical_accuracy: 0.9628 - val_loss: 2.7209 - val_categorical_accuracy: 0.5372 - lr: 1.0000e-04\n",
      "Epoch 200/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1069 - categorical_accuracy: 0.9629 - val_loss: 2.3780 - val_categorical_accuracy: 0.5539 - lr: 1.0000e-04\n",
      "Epoch 201/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.1012 - categorical_accuracy: 0.9663 - val_loss: 2.4080 - val_categorical_accuracy: 0.5525 - lr: 1.0000e-04\n",
      "Epoch 202/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1051 - categorical_accuracy: 0.9639 - val_loss: 2.8447 - val_categorical_accuracy: 0.5439 - lr: 1.0000e-04\n",
      "Epoch 203/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.1011 - categorical_accuracy: 0.9640 - val_loss: 2.6341 - val_categorical_accuracy: 0.5286 - lr: 1.0000e-04\n",
      "Epoch 204/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.1073 - categorical_accuracy: 0.9630 - val_loss: 2.6562 - val_categorical_accuracy: 0.5413 - lr: 1.0000e-04\n",
      "Epoch 205/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.1021 - categorical_accuracy: 0.9657 - val_loss: 2.7958 - val_categorical_accuracy: 0.5511 - lr: 1.0000e-04\n",
      "Epoch 206/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.1024 - categorical_accuracy: 0.9646 - val_loss: 2.8076 - val_categorical_accuracy: 0.5402 - lr: 1.0000e-04\n",
      "Epoch 207/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1052 - categorical_accuracy: 0.9636 - val_loss: 2.5994 - val_categorical_accuracy: 0.5476 - lr: 1.0000e-04\n",
      "Epoch 208/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.1086 - categorical_accuracy: 0.9619 - val_loss: 2.6175 - val_categorical_accuracy: 0.5392 - lr: 1.0000e-04\n",
      "Epoch 209/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1049 - categorical_accuracy: 0.9651 - val_loss: 2.4681 - val_categorical_accuracy: 0.5637 - lr: 1.0000e-04\n",
      "Epoch 210/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.1000 - categorical_accuracy: 0.9643 - val_loss: 2.5673 - val_categorical_accuracy: 0.5492 - lr: 1.0000e-04\n",
      "Epoch 211/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1009 - categorical_accuracy: 0.9650 - val_loss: 2.4068 - val_categorical_accuracy: 0.5729 - lr: 1.0000e-04\n",
      "Epoch 212/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1017 - categorical_accuracy: 0.9634 - val_loss: 2.6431 - val_categorical_accuracy: 0.5529 - lr: 1.0000e-04\n",
      "Epoch 213/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0986 - categorical_accuracy: 0.9648 - val_loss: 2.4379 - val_categorical_accuracy: 0.5472 - lr: 1.0000e-04\n",
      "Epoch 214/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0976 - categorical_accuracy: 0.9670 - val_loss: 2.4293 - val_categorical_accuracy: 0.5505 - lr: 1.0000e-04\n",
      "Epoch 215/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0994 - categorical_accuracy: 0.9660 - val_loss: 2.5382 - val_categorical_accuracy: 0.5598 - lr: 1.0000e-04\n",
      "Epoch 216/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0997 - categorical_accuracy: 0.9659 - val_loss: 2.8430 - val_categorical_accuracy: 0.5341 - lr: 1.0000e-04\n",
      "Epoch 217/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.1041 - categorical_accuracy: 0.9646 - val_loss: 2.6618 - val_categorical_accuracy: 0.5480 - lr: 1.0000e-04\n",
      "Epoch 218/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1002 - categorical_accuracy: 0.9653 - val_loss: 2.6753 - val_categorical_accuracy: 0.5531 - lr: 1.0000e-04\n",
      "Epoch 219/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1016 - categorical_accuracy: 0.9647 - val_loss: 2.4941 - val_categorical_accuracy: 0.5635 - lr: 1.0000e-04\n",
      "Epoch 220/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0978 - categorical_accuracy: 0.9663 - val_loss: 2.8453 - val_categorical_accuracy: 0.5507 - lr: 1.0000e-04\n",
      "Epoch 221/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0994 - categorical_accuracy: 0.9652 - val_loss: 2.5923 - val_categorical_accuracy: 0.5388 - lr: 1.0000e-04\n",
      "Epoch 222/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1006 - categorical_accuracy: 0.9649 - val_loss: 2.8758 - val_categorical_accuracy: 0.5457 - lr: 1.0000e-04\n",
      "Epoch 223/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1014 - categorical_accuracy: 0.9659 - val_loss: 2.6158 - val_categorical_accuracy: 0.5558 - lr: 1.0000e-04\n",
      "Epoch 224/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0962 - categorical_accuracy: 0.9669 - val_loss: 2.6120 - val_categorical_accuracy: 0.5643 - lr: 1.0000e-04\n",
      "Epoch 225/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0978 - categorical_accuracy: 0.9660 - val_loss: 2.5711 - val_categorical_accuracy: 0.5656 - lr: 1.0000e-04\n",
      "Epoch 226/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0965 - categorical_accuracy: 0.9676 - val_loss: 3.1038 - val_categorical_accuracy: 0.5239 - lr: 1.0000e-04\n",
      "Epoch 227/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1022 - categorical_accuracy: 0.9652 - val_loss: 3.4154 - val_categorical_accuracy: 0.5010 - lr: 1.0000e-04\n",
      "Epoch 228/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0949 - categorical_accuracy: 0.9659 - val_loss: 2.9545 - val_categorical_accuracy: 0.5359 - lr: 1.0000e-04\n",
      "Epoch 229/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0948 - categorical_accuracy: 0.9690 - val_loss: 3.0996 - val_categorical_accuracy: 0.5147 - lr: 1.0000e-04\n",
      "Epoch 230/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.1017 - categorical_accuracy: 0.9652 - val_loss: 2.9494 - val_categorical_accuracy: 0.5251 - lr: 1.0000e-04\n",
      "Epoch 231/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0992 - categorical_accuracy: 0.9651 - val_loss: 2.5277 - val_categorical_accuracy: 0.5605 - lr: 1.0000e-04\n",
      "Epoch 232/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0968 - categorical_accuracy: 0.9669 - val_loss: 2.6580 - val_categorical_accuracy: 0.5588 - lr: 1.0000e-04\n",
      "Epoch 233/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0988 - categorical_accuracy: 0.9661 - val_loss: 2.5714 - val_categorical_accuracy: 0.5621 - lr: 1.0000e-04\n",
      "Epoch 234/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0965 - categorical_accuracy: 0.9673 - val_loss: 2.7297 - val_categorical_accuracy: 0.5439 - lr: 1.0000e-04\n",
      "Epoch 235/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0909 - categorical_accuracy: 0.9686 - val_loss: 2.8108 - val_categorical_accuracy: 0.5272 - lr: 1.0000e-04\n",
      "Epoch 236/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0933 - categorical_accuracy: 0.9700 - val_loss: 2.6444 - val_categorical_accuracy: 0.5521 - lr: 1.0000e-04\n",
      "Epoch 237/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0961 - categorical_accuracy: 0.9657 - val_loss: 2.6993 - val_categorical_accuracy: 0.5652 - lr: 1.0000e-04\n",
      "Epoch 238/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0972 - categorical_accuracy: 0.9660 - val_loss: 2.9811 - val_categorical_accuracy: 0.5259 - lr: 1.0000e-04\n",
      "Epoch 239/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0958 - categorical_accuracy: 0.9673 - val_loss: 2.2998 - val_categorical_accuracy: 0.5517 - lr: 1.0000e-04\n",
      "Epoch 240/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0952 - categorical_accuracy: 0.9683 - val_loss: 2.7002 - val_categorical_accuracy: 0.5613 - lr: 1.0000e-04\n",
      "Epoch 241/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0959 - categorical_accuracy: 0.9677 - val_loss: 2.9265 - val_categorical_accuracy: 0.5176 - lr: 1.0000e-04\n",
      "Epoch 242/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0962 - categorical_accuracy: 0.9673 - val_loss: 2.6177 - val_categorical_accuracy: 0.5504 - lr: 1.0000e-04\n",
      "Epoch 243/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0937 - categorical_accuracy: 0.9671 - val_loss: 2.9352 - val_categorical_accuracy: 0.5306 - lr: 1.0000e-04\n",
      "Epoch 244/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0999 - categorical_accuracy: 0.9647 - val_loss: 2.7563 - val_categorical_accuracy: 0.5368 - lr: 1.0000e-04\n",
      "Epoch 245/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0888 - categorical_accuracy: 0.9689 - val_loss: 2.5399 - val_categorical_accuracy: 0.5621 - lr: 1.0000e-04\n",
      "Epoch 246/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0908 - categorical_accuracy: 0.9687 - val_loss: 2.7619 - val_categorical_accuracy: 0.5294 - lr: 1.0000e-04\n",
      "Epoch 247/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0973 - categorical_accuracy: 0.9663 - val_loss: 2.5728 - val_categorical_accuracy: 0.5584 - lr: 1.0000e-04\n",
      "Epoch 248/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0931 - categorical_accuracy: 0.9670 - val_loss: 3.0146 - val_categorical_accuracy: 0.5437 - lr: 1.0000e-04\n",
      "Epoch 249/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0942 - categorical_accuracy: 0.9677 - val_loss: 2.8205 - val_categorical_accuracy: 0.5237 - lr: 1.0000e-04\n",
      "Epoch 250/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0948 - categorical_accuracy: 0.9672 - val_loss: 3.1254 - val_categorical_accuracy: 0.5155 - lr: 1.0000e-04\n",
      "Epoch 251/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0912 - categorical_accuracy: 0.9687 - val_loss: 2.8543 - val_categorical_accuracy: 0.5282 - lr: 1.0000e-04\n",
      "Epoch 252/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0935 - categorical_accuracy: 0.9687 - val_loss: 2.8197 - val_categorical_accuracy: 0.5484 - lr: 1.0000e-04\n",
      "Epoch 253/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0907 - categorical_accuracy: 0.9680 - val_loss: 2.7020 - val_categorical_accuracy: 0.5537 - lr: 1.0000e-04\n",
      "Epoch 254/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0969 - categorical_accuracy: 0.9661 - val_loss: 2.6641 - val_categorical_accuracy: 0.5617 - lr: 1.0000e-04\n",
      "Epoch 255/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0924 - categorical_accuracy: 0.9659 - val_loss: 2.5490 - val_categorical_accuracy: 0.5492 - lr: 1.0000e-04\n",
      "Epoch 256/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0967 - categorical_accuracy: 0.9655 - val_loss: 2.8579 - val_categorical_accuracy: 0.5139 - lr: 1.0000e-04\n",
      "Epoch 257/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0889 - categorical_accuracy: 0.9685 - val_loss: 2.9225 - val_categorical_accuracy: 0.5325 - lr: 1.0000e-04\n",
      "Epoch 258/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0940 - categorical_accuracy: 0.9660 - val_loss: 2.6134 - val_categorical_accuracy: 0.5537 - lr: 1.0000e-04\n",
      "Epoch 259/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0895 - categorical_accuracy: 0.9684 - val_loss: 2.7280 - val_categorical_accuracy: 0.5603 - lr: 1.0000e-04\n",
      "Epoch 260/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0923 - categorical_accuracy: 0.9686 - val_loss: 2.7777 - val_categorical_accuracy: 0.5435 - lr: 1.0000e-04\n",
      "Epoch 261/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0909 - categorical_accuracy: 0.9692 - val_loss: 2.8068 - val_categorical_accuracy: 0.5553 - lr: 1.0000e-04\n",
      "Epoch 262/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0910 - categorical_accuracy: 0.9684 - val_loss: 2.6962 - val_categorical_accuracy: 0.5574 - lr: 1.0000e-04\n",
      "Epoch 263/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.0949 - categorical_accuracy: 0.9669 - val_loss: 3.0786 - val_categorical_accuracy: 0.5392 - lr: 1.0000e-04\n",
      "Epoch 264/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0900 - categorical_accuracy: 0.9694 - val_loss: 3.0552 - val_categorical_accuracy: 0.5380 - lr: 1.0000e-04\n",
      "Epoch 265/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0958 - categorical_accuracy: 0.9662 - val_loss: 2.7891 - val_categorical_accuracy: 0.5468 - lr: 1.0000e-04\n",
      "Epoch 266/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0901 - categorical_accuracy: 0.9685 - val_loss: 3.0808 - val_categorical_accuracy: 0.5359 - lr: 1.0000e-04\n",
      "Epoch 267/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0913 - categorical_accuracy: 0.9682 - val_loss: 2.7124 - val_categorical_accuracy: 0.5502 - lr: 1.0000e-04\n",
      "Epoch 268/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0868 - categorical_accuracy: 0.9701 - val_loss: 2.9022 - val_categorical_accuracy: 0.5249 - lr: 1.0000e-04\n",
      "Epoch 269/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0865 - categorical_accuracy: 0.9702 - val_loss: 2.7333 - val_categorical_accuracy: 0.5531 - lr: 1.0000e-04\n",
      "Epoch 270/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0884 - categorical_accuracy: 0.9696 - val_loss: 2.8896 - val_categorical_accuracy: 0.5529 - lr: 1.0000e-04\n",
      "Epoch 271/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0851 - categorical_accuracy: 0.9704 - val_loss: 2.7276 - val_categorical_accuracy: 0.5290 - lr: 1.0000e-04\n",
      "Epoch 272/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0905 - categorical_accuracy: 0.9683 - val_loss: 2.5970 - val_categorical_accuracy: 0.5578 - lr: 1.0000e-04\n",
      "Epoch 273/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0895 - categorical_accuracy: 0.9698 - val_loss: 2.9834 - val_categorical_accuracy: 0.5255 - lr: 1.0000e-04\n",
      "Epoch 274/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0863 - categorical_accuracy: 0.9709 - val_loss: 2.8917 - val_categorical_accuracy: 0.5535 - lr: 1.0000e-04\n",
      "Epoch 275/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0907 - categorical_accuracy: 0.9688 - val_loss: 2.6747 - val_categorical_accuracy: 0.5601 - lr: 1.0000e-04\n",
      "Epoch 276/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0864 - categorical_accuracy: 0.9702 - val_loss: 2.7018 - val_categorical_accuracy: 0.5425 - lr: 1.0000e-04\n",
      "Epoch 277/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0879 - categorical_accuracy: 0.9698 - val_loss: 2.8968 - val_categorical_accuracy: 0.5476 - lr: 1.0000e-04\n",
      "Epoch 278/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0910 - categorical_accuracy: 0.9680 - val_loss: 2.5944 - val_categorical_accuracy: 0.5460 - lr: 1.0000e-04\n",
      "Epoch 279/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0841 - categorical_accuracy: 0.9712 - val_loss: 2.7078 - val_categorical_accuracy: 0.5537 - lr: 1.0000e-04\n",
      "Epoch 280/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0886 - categorical_accuracy: 0.9694 - val_loss: 2.6795 - val_categorical_accuracy: 0.5556 - lr: 1.0000e-04\n",
      "Epoch 281/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0852 - categorical_accuracy: 0.9706 - val_loss: 2.5750 - val_categorical_accuracy: 0.5562 - lr: 1.0000e-04\n",
      "Epoch 282/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0851 - categorical_accuracy: 0.9699 - val_loss: 2.8099 - val_categorical_accuracy: 0.5439 - lr: 1.0000e-04\n",
      "Epoch 283/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0918 - categorical_accuracy: 0.9685 - val_loss: 2.8745 - val_categorical_accuracy: 0.5470 - lr: 1.0000e-04\n",
      "Epoch 284/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0895 - categorical_accuracy: 0.9688 - val_loss: 2.6550 - val_categorical_accuracy: 0.5600 - lr: 1.0000e-04\n",
      "Epoch 285/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0889 - categorical_accuracy: 0.9704 - val_loss: 2.8991 - val_categorical_accuracy: 0.5423 - lr: 1.0000e-04\n",
      "Epoch 286/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0851 - categorical_accuracy: 0.9717 - val_loss: 2.7617 - val_categorical_accuracy: 0.5484 - lr: 1.0000e-04\n",
      "Epoch 287/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0841 - categorical_accuracy: 0.9724 - val_loss: 3.1235 - val_categorical_accuracy: 0.5535 - lr: 1.0000e-04\n",
      "Epoch 288/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0831 - categorical_accuracy: 0.9714 - val_loss: 2.6869 - val_categorical_accuracy: 0.5564 - lr: 1.0000e-04\n",
      "Epoch 289/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0879 - categorical_accuracy: 0.9693 - val_loss: 3.1165 - val_categorical_accuracy: 0.5190 - lr: 1.0000e-04\n",
      "Epoch 290/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0826 - categorical_accuracy: 0.9714 - val_loss: 2.7298 - val_categorical_accuracy: 0.5719 - lr: 1.0000e-04\n",
      "Epoch 291/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0862 - categorical_accuracy: 0.9701 - val_loss: 2.5403 - val_categorical_accuracy: 0.5684 - lr: 1.0000e-04\n",
      "Epoch 292/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0808 - categorical_accuracy: 0.9719 - val_loss: 2.9677 - val_categorical_accuracy: 0.5347 - lr: 1.0000e-04\n",
      "Epoch 293/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0877 - categorical_accuracy: 0.9712 - val_loss: 2.9718 - val_categorical_accuracy: 0.5425 - lr: 1.0000e-04\n",
      "Epoch 294/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0906 - categorical_accuracy: 0.9686 - val_loss: 2.9141 - val_categorical_accuracy: 0.5353 - lr: 1.0000e-04\n",
      "Epoch 295/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0818 - categorical_accuracy: 0.9722 - val_loss: 2.8895 - val_categorical_accuracy: 0.5494 - lr: 1.0000e-04\n",
      "Epoch 296/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0826 - categorical_accuracy: 0.9713 - val_loss: 2.6023 - val_categorical_accuracy: 0.5553 - lr: 1.0000e-04\n",
      "Epoch 297/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0855 - categorical_accuracy: 0.9701 - val_loss: 2.9817 - val_categorical_accuracy: 0.5494 - lr: 1.0000e-04\n",
      "Epoch 298/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0914 - categorical_accuracy: 0.9687 - val_loss: 2.6951 - val_categorical_accuracy: 0.5496 - lr: 1.0000e-04\n",
      "Epoch 299/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0843 - categorical_accuracy: 0.9699 - val_loss: 2.8487 - val_categorical_accuracy: 0.5353 - lr: 1.0000e-04\n",
      "Epoch 300/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0793 - categorical_accuracy: 0.9732 - val_loss: 2.6928 - val_categorical_accuracy: 0.5435 - lr: 1.0000e-04\n",
      "Epoch 301/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0887 - categorical_accuracy: 0.9692 - val_loss: 3.5096 - val_categorical_accuracy: 0.5078 - lr: 1.0000e-04\n",
      "Epoch 302/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0876 - categorical_accuracy: 0.9696 - val_loss: 3.1062 - val_categorical_accuracy: 0.5212 - lr: 1.0000e-04\n",
      "Epoch 303/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0844 - categorical_accuracy: 0.9716 - val_loss: 2.5752 - val_categorical_accuracy: 0.5547 - lr: 1.0000e-04\n",
      "Epoch 304/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0884 - categorical_accuracy: 0.9697 - val_loss: 2.5635 - val_categorical_accuracy: 0.5598 - lr: 1.0000e-04\n",
      "Epoch 305/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0806 - categorical_accuracy: 0.9718 - val_loss: 2.8177 - val_categorical_accuracy: 0.5498 - lr: 1.0000e-04\n",
      "Epoch 306/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0807 - categorical_accuracy: 0.9730 - val_loss: 2.8455 - val_categorical_accuracy: 0.5368 - lr: 1.0000e-04\n",
      "Epoch 307/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0873 - categorical_accuracy: 0.9698 - val_loss: 3.3647 - val_categorical_accuracy: 0.5408 - lr: 1.0000e-04\n",
      "Epoch 308/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0825 - categorical_accuracy: 0.9718 - val_loss: 2.6762 - val_categorical_accuracy: 0.5631 - lr: 1.0000e-04\n",
      "Epoch 309/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0797 - categorical_accuracy: 0.9734 - val_loss: 2.8886 - val_categorical_accuracy: 0.5615 - lr: 1.0000e-04\n",
      "Epoch 310/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0845 - categorical_accuracy: 0.9709 - val_loss: 2.7649 - val_categorical_accuracy: 0.5637 - lr: 1.0000e-04\n",
      "Epoch 311/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0823 - categorical_accuracy: 0.9725 - val_loss: 2.7518 - val_categorical_accuracy: 0.5478 - lr: 1.0000e-04\n",
      "Epoch 312/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0823 - categorical_accuracy: 0.9722 - val_loss: 2.7888 - val_categorical_accuracy: 0.5547 - lr: 1.0000e-04\n",
      "Epoch 313/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0782 - categorical_accuracy: 0.9732 - val_loss: 2.9446 - val_categorical_accuracy: 0.5492 - lr: 1.0000e-04\n",
      "Epoch 314/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0845 - categorical_accuracy: 0.9708 - val_loss: 2.9593 - val_categorical_accuracy: 0.5505 - lr: 1.0000e-04\n",
      "Epoch 315/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0841 - categorical_accuracy: 0.9721 - val_loss: 2.8613 - val_categorical_accuracy: 0.5492 - lr: 1.0000e-04\n",
      "Epoch 316/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0816 - categorical_accuracy: 0.9714 - val_loss: 2.6454 - val_categorical_accuracy: 0.5529 - lr: 1.0000e-04\n",
      "Epoch 317/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0851 - categorical_accuracy: 0.9710 - val_loss: 3.0670 - val_categorical_accuracy: 0.5361 - lr: 1.0000e-04\n",
      "Epoch 318/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0773 - categorical_accuracy: 0.9748 - val_loss: 3.0920 - val_categorical_accuracy: 0.5472 - lr: 1.0000e-04\n",
      "Epoch 319/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0781 - categorical_accuracy: 0.9731 - val_loss: 2.8995 - val_categorical_accuracy: 0.5335 - lr: 1.0000e-04\n",
      "Epoch 320/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0849 - categorical_accuracy: 0.9699 - val_loss: 3.0866 - val_categorical_accuracy: 0.5231 - lr: 1.0000e-04\n",
      "Epoch 321/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0787 - categorical_accuracy: 0.9736 - val_loss: 3.5109 - val_categorical_accuracy: 0.5192 - lr: 1.0000e-04\n",
      "Epoch 322/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0834 - categorical_accuracy: 0.9717 - val_loss: 3.0625 - val_categorical_accuracy: 0.5306 - lr: 1.0000e-04\n",
      "Epoch 323/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0817 - categorical_accuracy: 0.9715 - val_loss: 3.1407 - val_categorical_accuracy: 0.5174 - lr: 1.0000e-04\n",
      "Epoch 324/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0872 - categorical_accuracy: 0.9695 - val_loss: 2.8026 - val_categorical_accuracy: 0.5574 - lr: 1.0000e-04\n",
      "Epoch 325/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0838 - categorical_accuracy: 0.9740 - val_loss: 2.9759 - val_categorical_accuracy: 0.5527 - lr: 1.0000e-04\n",
      "Epoch 326/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0761 - categorical_accuracy: 0.9745 - val_loss: 3.0446 - val_categorical_accuracy: 0.5492 - lr: 1.0000e-04\n",
      "Epoch 327/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0817 - categorical_accuracy: 0.9711 - val_loss: 2.8483 - val_categorical_accuracy: 0.5517 - lr: 1.0000e-04\n",
      "Epoch 328/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0845 - categorical_accuracy: 0.9710 - val_loss: 2.9186 - val_categorical_accuracy: 0.5574 - lr: 1.0000e-04\n",
      "Epoch 329/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0755 - categorical_accuracy: 0.9749 - val_loss: 2.9582 - val_categorical_accuracy: 0.5398 - lr: 1.0000e-04\n",
      "Epoch 330/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0776 - categorical_accuracy: 0.9731 - val_loss: 2.9605 - val_categorical_accuracy: 0.5460 - lr: 1.0000e-04\n",
      "Epoch 331/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0806 - categorical_accuracy: 0.9715 - val_loss: 2.9839 - val_categorical_accuracy: 0.5490 - lr: 1.0000e-04\n",
      "Epoch 332/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0802 - categorical_accuracy: 0.9738 - val_loss: 3.3351 - val_categorical_accuracy: 0.5241 - lr: 1.0000e-04\n",
      "Epoch 333/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0802 - categorical_accuracy: 0.9726 - val_loss: 2.7954 - val_categorical_accuracy: 0.5394 - lr: 1.0000e-04\n",
      "Epoch 334/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0787 - categorical_accuracy: 0.9733 - val_loss: 2.9258 - val_categorical_accuracy: 0.5539 - lr: 1.0000e-04\n",
      "Epoch 335/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0839 - categorical_accuracy: 0.9716 - val_loss: 2.9164 - val_categorical_accuracy: 0.5607 - lr: 1.0000e-04\n",
      "Epoch 336/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0758 - categorical_accuracy: 0.9748 - val_loss: 2.6707 - val_categorical_accuracy: 0.5690 - lr: 1.0000e-04\n",
      "Epoch 337/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0800 - categorical_accuracy: 0.9721 - val_loss: 3.2995 - val_categorical_accuracy: 0.5196 - lr: 1.0000e-04\n",
      "Epoch 338/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0801 - categorical_accuracy: 0.9717 - val_loss: 2.9718 - val_categorical_accuracy: 0.5233 - lr: 1.0000e-04\n",
      "Epoch 339/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0756 - categorical_accuracy: 0.9745 - val_loss: 2.8317 - val_categorical_accuracy: 0.5572 - lr: 1.0000e-04\n",
      "Epoch 340/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0815 - categorical_accuracy: 0.9721 - val_loss: 2.9853 - val_categorical_accuracy: 0.5431 - lr: 1.0000e-04\n",
      "Epoch 341/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0761 - categorical_accuracy: 0.9741 - val_loss: 2.8498 - val_categorical_accuracy: 0.5268 - lr: 1.0000e-04\n",
      "Epoch 342/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0797 - categorical_accuracy: 0.9722 - val_loss: 2.9598 - val_categorical_accuracy: 0.5398 - lr: 1.0000e-04\n",
      "Epoch 343/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0784 - categorical_accuracy: 0.9731 - val_loss: 3.2422 - val_categorical_accuracy: 0.5421 - lr: 1.0000e-04\n",
      "Epoch 344/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0809 - categorical_accuracy: 0.9724 - val_loss: 3.0311 - val_categorical_accuracy: 0.5355 - lr: 1.0000e-04\n",
      "Epoch 345/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0789 - categorical_accuracy: 0.9741 - val_loss: 2.7294 - val_categorical_accuracy: 0.5502 - lr: 1.0000e-04\n",
      "Epoch 346/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0786 - categorical_accuracy: 0.9738 - val_loss: 3.0064 - val_categorical_accuracy: 0.5537 - lr: 1.0000e-04\n",
      "Epoch 347/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0758 - categorical_accuracy: 0.9733 - val_loss: 2.6864 - val_categorical_accuracy: 0.5560 - lr: 1.0000e-04\n",
      "Epoch 348/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0793 - categorical_accuracy: 0.9721 - val_loss: 3.0004 - val_categorical_accuracy: 0.5447 - lr: 1.0000e-04\n",
      "Epoch 349/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0781 - categorical_accuracy: 0.9735 - val_loss: 3.2110 - val_categorical_accuracy: 0.5210 - lr: 1.0000e-04\n",
      "Epoch 350/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0753 - categorical_accuracy: 0.9740 - val_loss: 2.8883 - val_categorical_accuracy: 0.5419 - lr: 1.0000e-04\n",
      "Epoch 351/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0762 - categorical_accuracy: 0.9743 - val_loss: 2.8170 - val_categorical_accuracy: 0.5654 - lr: 1.0000e-04\n",
      "Epoch 352/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0796 - categorical_accuracy: 0.9722 - val_loss: 2.9988 - val_categorical_accuracy: 0.5504 - lr: 1.0000e-04\n",
      "Epoch 353/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0764 - categorical_accuracy: 0.9745 - val_loss: 3.2009 - val_categorical_accuracy: 0.5576 - lr: 1.0000e-04\n",
      "Epoch 354/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0748 - categorical_accuracy: 0.9737 - val_loss: 3.0681 - val_categorical_accuracy: 0.5339 - lr: 1.0000e-04\n",
      "Epoch 355/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0793 - categorical_accuracy: 0.9725 - val_loss: 2.9636 - val_categorical_accuracy: 0.5525 - lr: 1.0000e-04\n",
      "Epoch 356/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0753 - categorical_accuracy: 0.9736 - val_loss: 2.7629 - val_categorical_accuracy: 0.5660 - lr: 1.0000e-04\n",
      "Epoch 357/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0742 - categorical_accuracy: 0.9750 - val_loss: 2.9194 - val_categorical_accuracy: 0.5458 - lr: 1.0000e-04\n",
      "Epoch 358/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0791 - categorical_accuracy: 0.9723 - val_loss: 3.1731 - val_categorical_accuracy: 0.5361 - lr: 1.0000e-04\n",
      "Epoch 359/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0714 - categorical_accuracy: 0.9761 - val_loss: 2.7538 - val_categorical_accuracy: 0.5672 - lr: 1.0000e-04\n",
      "Epoch 360/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0750 - categorical_accuracy: 0.9739 - val_loss: 2.6254 - val_categorical_accuracy: 0.5578 - lr: 1.0000e-04\n",
      "Epoch 361/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0763 - categorical_accuracy: 0.9732 - val_loss: 2.8721 - val_categorical_accuracy: 0.5590 - lr: 1.0000e-04\n",
      "Epoch 362/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0784 - categorical_accuracy: 0.9739 - val_loss: 2.7728 - val_categorical_accuracy: 0.5549 - lr: 1.0000e-04\n",
      "Epoch 363/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0745 - categorical_accuracy: 0.9738 - val_loss: 2.9630 - val_categorical_accuracy: 0.5464 - lr: 1.0000e-04\n",
      "Epoch 364/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0732 - categorical_accuracy: 0.9753 - val_loss: 3.3707 - val_categorical_accuracy: 0.5355 - lr: 1.0000e-04\n",
      "Epoch 365/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0761 - categorical_accuracy: 0.9744 - val_loss: 3.0679 - val_categorical_accuracy: 0.5243 - lr: 1.0000e-04\n",
      "Epoch 366/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0694 - categorical_accuracy: 0.9777 - val_loss: 3.1332 - val_categorical_accuracy: 0.5523 - lr: 1.0000e-04\n",
      "Epoch 367/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0760 - categorical_accuracy: 0.9735 - val_loss: 3.2271 - val_categorical_accuracy: 0.5221 - lr: 1.0000e-04\n",
      "Epoch 368/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0715 - categorical_accuracy: 0.9744 - val_loss: 3.2527 - val_categorical_accuracy: 0.5361 - lr: 1.0000e-04\n",
      "Epoch 369/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0831 - categorical_accuracy: 0.9712 - val_loss: 3.3791 - val_categorical_accuracy: 0.5257 - lr: 1.0000e-04\n",
      "Epoch 370/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0716 - categorical_accuracy: 0.9754 - val_loss: 3.1473 - val_categorical_accuracy: 0.5484 - lr: 1.0000e-04\n",
      "Epoch 371/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0745 - categorical_accuracy: 0.9753 - val_loss: 3.0414 - val_categorical_accuracy: 0.5386 - lr: 1.0000e-04\n",
      "Epoch 372/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0763 - categorical_accuracy: 0.9743 - val_loss: 2.9483 - val_categorical_accuracy: 0.5411 - lr: 1.0000e-04\n",
      "Epoch 373/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0796 - categorical_accuracy: 0.9721 - val_loss: 3.1181 - val_categorical_accuracy: 0.5500 - lr: 1.0000e-04\n",
      "Epoch 374/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0718 - categorical_accuracy: 0.9755 - val_loss: 2.9174 - val_categorical_accuracy: 0.5633 - lr: 1.0000e-04\n",
      "Epoch 375/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0734 - categorical_accuracy: 0.9747 - val_loss: 3.2187 - val_categorical_accuracy: 0.5237 - lr: 1.0000e-04\n",
      "Epoch 376/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0742 - categorical_accuracy: 0.9750 - val_loss: 3.3045 - val_categorical_accuracy: 0.5343 - lr: 1.0000e-04\n",
      "Epoch 377/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0715 - categorical_accuracy: 0.9757 - val_loss: 2.9818 - val_categorical_accuracy: 0.5378 - lr: 1.0000e-04\n",
      "Epoch 378/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0717 - categorical_accuracy: 0.9754 - val_loss: 2.8130 - val_categorical_accuracy: 0.5586 - lr: 1.0000e-04\n",
      "Epoch 379/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0686 - categorical_accuracy: 0.9771 - val_loss: 3.5401 - val_categorical_accuracy: 0.5361 - lr: 1.0000e-04\n",
      "Epoch 380/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0729 - categorical_accuracy: 0.9748 - val_loss: 3.1573 - val_categorical_accuracy: 0.5376 - lr: 1.0000e-04\n",
      "Epoch 381/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0765 - categorical_accuracy: 0.9734 - val_loss: 3.2421 - val_categorical_accuracy: 0.5521 - lr: 1.0000e-04\n",
      "Epoch 382/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0723 - categorical_accuracy: 0.9749 - val_loss: 3.3368 - val_categorical_accuracy: 0.5445 - lr: 1.0000e-04\n",
      "Epoch 383/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0723 - categorical_accuracy: 0.9748 - val_loss: 3.3651 - val_categorical_accuracy: 0.5225 - lr: 1.0000e-04\n",
      "Epoch 384/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0797 - categorical_accuracy: 0.9721 - val_loss: 3.1651 - val_categorical_accuracy: 0.5517 - lr: 1.0000e-04\n",
      "Epoch 385/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0728 - categorical_accuracy: 0.9752 - val_loss: 2.9592 - val_categorical_accuracy: 0.5474 - lr: 1.0000e-04\n",
      "Epoch 386/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0767 - categorical_accuracy: 0.9733 - val_loss: 2.9701 - val_categorical_accuracy: 0.5531 - lr: 1.0000e-04\n",
      "Epoch 387/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0664 - categorical_accuracy: 0.9779 - val_loss: 2.9688 - val_categorical_accuracy: 0.5713 - lr: 1.0000e-04\n",
      "Epoch 388/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0711 - categorical_accuracy: 0.9755 - val_loss: 3.1153 - val_categorical_accuracy: 0.5310 - lr: 1.0000e-04\n",
      "Epoch 389/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0728 - categorical_accuracy: 0.9751 - val_loss: 2.9544 - val_categorical_accuracy: 0.5458 - lr: 1.0000e-04\n",
      "Epoch 390/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0734 - categorical_accuracy: 0.9759 - val_loss: 2.9794 - val_categorical_accuracy: 0.5394 - lr: 1.0000e-04\n",
      "Epoch 391/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0727 - categorical_accuracy: 0.9746 - val_loss: 3.0356 - val_categorical_accuracy: 0.5543 - lr: 1.0000e-04\n",
      "Epoch 392/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0690 - categorical_accuracy: 0.9764 - val_loss: 3.0706 - val_categorical_accuracy: 0.5384 - lr: 1.0000e-04\n",
      "Epoch 393/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0693 - categorical_accuracy: 0.9763 - val_loss: 2.9747 - val_categorical_accuracy: 0.5554 - lr: 1.0000e-04\n",
      "Epoch 394/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0747 - categorical_accuracy: 0.9751 - val_loss: 2.7456 - val_categorical_accuracy: 0.5423 - lr: 1.0000e-04\n",
      "Epoch 395/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.0737 - categorical_accuracy: 0.9754 - val_loss: 3.1407 - val_categorical_accuracy: 0.5605 - lr: 1.0000e-04\n",
      "Epoch 396/500\n",
      "638/638 [==============================] - 13s 20ms/step - loss: 0.0771 - categorical_accuracy: 0.9743 - val_loss: 3.2536 - val_categorical_accuracy: 0.5384 - lr: 1.0000e-04\n",
      "Epoch 397/500\n",
      "638/638 [==============================] - 13s 20ms/step - loss: 0.0729 - categorical_accuracy: 0.9735 - val_loss: 2.8671 - val_categorical_accuracy: 0.5482 - lr: 1.0000e-04\n",
      "Epoch 398/500\n",
      "638/638 [==============================] - 14s 22ms/step - loss: 0.0697 - categorical_accuracy: 0.9762 - val_loss: 3.3180 - val_categorical_accuracy: 0.5439 - lr: 1.0000e-04\n",
      "Epoch 399/500\n",
      "638/638 [==============================] - 12s 19ms/step - loss: 0.0757 - categorical_accuracy: 0.9732 - val_loss: 3.0009 - val_categorical_accuracy: 0.5460 - lr: 1.0000e-04\n",
      "Epoch 400/500\n",
      "638/638 [==============================] - 14s 22ms/step - loss: 0.0739 - categorical_accuracy: 0.9754 - val_loss: 3.1616 - val_categorical_accuracy: 0.5323 - lr: 1.0000e-04\n",
      "Epoch 401/500\n",
      "638/638 [==============================] - 12s 19ms/step - loss: 0.0678 - categorical_accuracy: 0.9763 - val_loss: 3.1466 - val_categorical_accuracy: 0.5504 - lr: 1.0000e-04\n",
      "Epoch 402/500\n",
      "638/638 [==============================] - 12s 19ms/step - loss: 0.0695 - categorical_accuracy: 0.9764 - val_loss: 3.3383 - val_categorical_accuracy: 0.5386 - lr: 1.0000e-04\n",
      "Epoch 403/500\n",
      "638/638 [==============================] - 11s 17ms/step - loss: 0.0742 - categorical_accuracy: 0.9739 - val_loss: 3.0747 - val_categorical_accuracy: 0.5351 - lr: 1.0000e-04\n",
      "Epoch 404/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.0695 - categorical_accuracy: 0.9760 - val_loss: 3.4326 - val_categorical_accuracy: 0.5270 - lr: 1.0000e-04\n",
      "Epoch 405/500\n",
      "638/638 [==============================] - 9s 15ms/step - loss: 0.0712 - categorical_accuracy: 0.9757 - val_loss: 2.8512 - val_categorical_accuracy: 0.5598 - lr: 1.0000e-04\n",
      "Epoch 406/500\n",
      "638/638 [==============================] - 10s 15ms/step - loss: 0.0769 - categorical_accuracy: 0.9733 - val_loss: 2.9677 - val_categorical_accuracy: 0.5537 - lr: 1.0000e-04\n",
      "Epoch 407/500\n",
      "638/638 [==============================] - 10s 15ms/step - loss: 0.0705 - categorical_accuracy: 0.9763 - val_loss: 2.9635 - val_categorical_accuracy: 0.5529 - lr: 1.0000e-04\n",
      "Epoch 408/500\n",
      "638/638 [==============================] - 9s 15ms/step - loss: 0.0683 - categorical_accuracy: 0.9770 - val_loss: 3.4366 - val_categorical_accuracy: 0.5343 - lr: 1.0000e-04\n",
      "Epoch 409/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.0710 - categorical_accuracy: 0.9761 - val_loss: 2.8938 - val_categorical_accuracy: 0.5601 - lr: 1.0000e-04\n",
      "Epoch 410/500\n",
      "638/638 [==============================] - 9s 13ms/step - loss: 0.0700 - categorical_accuracy: 0.9765 - val_loss: 3.1411 - val_categorical_accuracy: 0.5300 - lr: 1.0000e-04\n",
      "Epoch 411/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.0739 - categorical_accuracy: 0.9753 - val_loss: 3.5210 - val_categorical_accuracy: 0.5182 - lr: 1.0000e-04\n",
      "Epoch 412/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.0757 - categorical_accuracy: 0.9741 - val_loss: 3.1967 - val_categorical_accuracy: 0.5347 - lr: 1.0000e-04\n",
      "Epoch 413/500\n",
      "638/638 [==============================] - 10s 16ms/step - loss: 0.0687 - categorical_accuracy: 0.9754 - val_loss: 3.0970 - val_categorical_accuracy: 0.5505 - lr: 1.0000e-04\n",
      "Epoch 414/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.0705 - categorical_accuracy: 0.9753 - val_loss: 2.9914 - val_categorical_accuracy: 0.5572 - lr: 1.0000e-04\n",
      "Epoch 415/500\n",
      "638/638 [==============================] - 10s 16ms/step - loss: 0.0689 - categorical_accuracy: 0.9753 - val_loss: 2.8593 - val_categorical_accuracy: 0.5668 - lr: 1.0000e-04\n",
      "Epoch 416/500\n",
      "638/638 [==============================] - 10s 15ms/step - loss: 0.0687 - categorical_accuracy: 0.9765 - val_loss: 3.1302 - val_categorical_accuracy: 0.5333 - lr: 1.0000e-04\n",
      "Epoch 417/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0698 - categorical_accuracy: 0.9766 - val_loss: 2.7720 - val_categorical_accuracy: 0.5519 - lr: 1.0000e-04\n",
      "Epoch 418/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0700 - categorical_accuracy: 0.9763 - val_loss: 3.0323 - val_categorical_accuracy: 0.5488 - lr: 1.0000e-04\n",
      "Epoch 419/500\n",
      "638/638 [==============================] - 10s 15ms/step - loss: 0.0688 - categorical_accuracy: 0.9765 - val_loss: 3.0642 - val_categorical_accuracy: 0.5263 - lr: 1.0000e-04\n",
      "Epoch 420/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0735 - categorical_accuracy: 0.9743 - val_loss: 3.0957 - val_categorical_accuracy: 0.5468 - lr: 1.0000e-04\n",
      "Epoch 421/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0722 - categorical_accuracy: 0.9746 - val_loss: 3.4288 - val_categorical_accuracy: 0.5590 - lr: 1.0000e-04\n",
      "Epoch 422/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0724 - categorical_accuracy: 0.9761 - val_loss: 3.0693 - val_categorical_accuracy: 0.5507 - lr: 1.0000e-04\n",
      "Epoch 423/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0661 - categorical_accuracy: 0.9781 - val_loss: 3.5957 - val_categorical_accuracy: 0.5110 - lr: 1.0000e-04\n",
      "Epoch 424/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0691 - categorical_accuracy: 0.9758 - val_loss: 3.2650 - val_categorical_accuracy: 0.5494 - lr: 1.0000e-04\n",
      "Epoch 425/500\n",
      "638/638 [==============================] - 9s 14ms/step - loss: 0.0722 - categorical_accuracy: 0.9755 - val_loss: 3.2698 - val_categorical_accuracy: 0.5400 - lr: 1.0000e-04\n",
      "Epoch 426/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0732 - categorical_accuracy: 0.9767 - val_loss: 3.6992 - val_categorical_accuracy: 0.5153 - lr: 1.0000e-04\n",
      "Epoch 427/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0685 - categorical_accuracy: 0.9772 - val_loss: 3.0179 - val_categorical_accuracy: 0.5531 - lr: 1.0000e-04\n",
      "Epoch 428/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0678 - categorical_accuracy: 0.9773 - val_loss: 3.3542 - val_categorical_accuracy: 0.5505 - lr: 1.0000e-04\n",
      "Epoch 429/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0702 - categorical_accuracy: 0.9761 - val_loss: 3.0628 - val_categorical_accuracy: 0.5517 - lr: 1.0000e-04\n",
      "Epoch 430/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0726 - categorical_accuracy: 0.9766 - val_loss: 2.9757 - val_categorical_accuracy: 0.5647 - lr: 1.0000e-04\n",
      "Epoch 431/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0685 - categorical_accuracy: 0.9772 - val_loss: 3.2917 - val_categorical_accuracy: 0.5464 - lr: 1.0000e-04\n",
      "Epoch 432/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0677 - categorical_accuracy: 0.9768 - val_loss: 3.4023 - val_categorical_accuracy: 0.5372 - lr: 1.0000e-04\n",
      "Epoch 433/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0700 - categorical_accuracy: 0.9761 - val_loss: 3.0541 - val_categorical_accuracy: 0.5529 - lr: 1.0000e-04\n",
      "Epoch 434/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0684 - categorical_accuracy: 0.9758 - val_loss: 2.9213 - val_categorical_accuracy: 0.5492 - lr: 1.0000e-04\n",
      "Epoch 435/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0662 - categorical_accuracy: 0.9770 - val_loss: 3.1722 - val_categorical_accuracy: 0.5400 - lr: 1.0000e-04\n",
      "Epoch 436/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0690 - categorical_accuracy: 0.9761 - val_loss: 3.0589 - val_categorical_accuracy: 0.5384 - lr: 1.0000e-04\n",
      "Epoch 437/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0690 - categorical_accuracy: 0.9755 - val_loss: 2.8324 - val_categorical_accuracy: 0.5621 - lr: 1.0000e-04\n",
      "Epoch 438/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0732 - categorical_accuracy: 0.9742 - val_loss: 3.2006 - val_categorical_accuracy: 0.5490 - lr: 1.0000e-04\n",
      "Epoch 439/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0658 - categorical_accuracy: 0.9778 - val_loss: 3.1320 - val_categorical_accuracy: 0.5547 - lr: 1.0000e-04\n",
      "Epoch 440/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0639 - categorical_accuracy: 0.9772 - val_loss: 3.1301 - val_categorical_accuracy: 0.5462 - lr: 1.0000e-04\n",
      "Epoch 441/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0637 - categorical_accuracy: 0.9782 - val_loss: 3.0943 - val_categorical_accuracy: 0.5572 - lr: 1.0000e-04\n",
      "Epoch 442/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0662 - categorical_accuracy: 0.9781 - val_loss: 3.1431 - val_categorical_accuracy: 0.5382 - lr: 1.0000e-04\n",
      "Epoch 443/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0667 - categorical_accuracy: 0.9764 - val_loss: 2.9289 - val_categorical_accuracy: 0.5492 - lr: 1.0000e-04\n",
      "Epoch 444/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0682 - categorical_accuracy: 0.9768 - val_loss: 3.1669 - val_categorical_accuracy: 0.5504 - lr: 1.0000e-04\n",
      "Epoch 445/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0658 - categorical_accuracy: 0.9770 - val_loss: 2.8570 - val_categorical_accuracy: 0.5517 - lr: 1.0000e-04\n",
      "Epoch 446/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0650 - categorical_accuracy: 0.9784 - val_loss: 3.2154 - val_categorical_accuracy: 0.5505 - lr: 1.0000e-04\n",
      "Epoch 447/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0677 - categorical_accuracy: 0.9768 - val_loss: 2.9675 - val_categorical_accuracy: 0.5627 - lr: 1.0000e-04\n",
      "Epoch 448/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0650 - categorical_accuracy: 0.9777 - val_loss: 3.0549 - val_categorical_accuracy: 0.5594 - lr: 1.0000e-04\n",
      "Epoch 449/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0655 - categorical_accuracy: 0.9783 - val_loss: 3.2513 - val_categorical_accuracy: 0.5343 - lr: 1.0000e-04\n",
      "Epoch 450/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0704 - categorical_accuracy: 0.9760 - val_loss: 2.8844 - val_categorical_accuracy: 0.5474 - lr: 1.0000e-04\n",
      "Epoch 451/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0667 - categorical_accuracy: 0.9774 - val_loss: 3.0288 - val_categorical_accuracy: 0.5541 - lr: 1.0000e-04\n",
      "Epoch 452/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0658 - categorical_accuracy: 0.9766 - val_loss: 3.3257 - val_categorical_accuracy: 0.5353 - lr: 1.0000e-04\n",
      "Epoch 453/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0654 - categorical_accuracy: 0.9773 - val_loss: 2.9762 - val_categorical_accuracy: 0.5415 - lr: 1.0000e-04\n",
      "Epoch 454/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0682 - categorical_accuracy: 0.9771 - val_loss: 3.5279 - val_categorical_accuracy: 0.5468 - lr: 1.0000e-04\n",
      "Epoch 455/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0707 - categorical_accuracy: 0.9757 - val_loss: 3.2504 - val_categorical_accuracy: 0.5664 - lr: 1.0000e-04\n",
      "Epoch 456/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0698 - categorical_accuracy: 0.9753 - val_loss: 3.7632 - val_categorical_accuracy: 0.5072 - lr: 1.0000e-04\n",
      "Epoch 457/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0642 - categorical_accuracy: 0.9776 - val_loss: 2.9163 - val_categorical_accuracy: 0.5558 - lr: 1.0000e-04\n",
      "Epoch 458/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0687 - categorical_accuracy: 0.9760 - val_loss: 3.1920 - val_categorical_accuracy: 0.5237 - lr: 1.0000e-04\n",
      "Epoch 459/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0677 - categorical_accuracy: 0.9773 - val_loss: 3.3066 - val_categorical_accuracy: 0.5480 - lr: 1.0000e-04\n",
      "Epoch 460/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0686 - categorical_accuracy: 0.9770 - val_loss: 2.9698 - val_categorical_accuracy: 0.5647 - lr: 1.0000e-04\n",
      "Epoch 461/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0681 - categorical_accuracy: 0.9776 - val_loss: 3.1786 - val_categorical_accuracy: 0.5505 - lr: 1.0000e-04\n",
      "Epoch 462/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0648 - categorical_accuracy: 0.9780 - val_loss: 2.9084 - val_categorical_accuracy: 0.5686 - lr: 1.0000e-04\n",
      "Epoch 463/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0691 - categorical_accuracy: 0.9751 - val_loss: 3.2860 - val_categorical_accuracy: 0.5496 - lr: 1.0000e-04\n",
      "Epoch 464/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0639 - categorical_accuracy: 0.9782 - val_loss: 2.8281 - val_categorical_accuracy: 0.5462 - lr: 1.0000e-04\n",
      "Epoch 465/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0691 - categorical_accuracy: 0.9780 - val_loss: 3.0984 - val_categorical_accuracy: 0.5609 - lr: 1.0000e-04\n",
      "Epoch 466/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0647 - categorical_accuracy: 0.9773 - val_loss: 3.0401 - val_categorical_accuracy: 0.5331 - lr: 1.0000e-04\n",
      "Epoch 467/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0637 - categorical_accuracy: 0.9783 - val_loss: 3.3172 - val_categorical_accuracy: 0.5609 - lr: 1.0000e-04\n",
      "Epoch 468/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0645 - categorical_accuracy: 0.9781 - val_loss: 3.6787 - val_categorical_accuracy: 0.5266 - lr: 1.0000e-04\n",
      "Epoch 469/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0640 - categorical_accuracy: 0.9779 - val_loss: 3.1768 - val_categorical_accuracy: 0.5572 - lr: 1.0000e-04\n",
      "Epoch 470/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0663 - categorical_accuracy: 0.9777 - val_loss: 3.1415 - val_categorical_accuracy: 0.5415 - lr: 1.0000e-04\n",
      "Epoch 471/500\n",
      "638/638 [==============================] - 8s 12ms/step - loss: 0.0660 - categorical_accuracy: 0.9784 - val_loss: 3.1100 - val_categorical_accuracy: 0.5576 - lr: 1.0000e-04\n",
      "Epoch 472/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0609 - categorical_accuracy: 0.9800 - val_loss: 3.5817 - val_categorical_accuracy: 0.5163 - lr: 1.0000e-04\n",
      "Epoch 473/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0665 - categorical_accuracy: 0.9772 - val_loss: 3.7431 - val_categorical_accuracy: 0.5112 - lr: 1.0000e-04\n",
      "Epoch 474/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0627 - categorical_accuracy: 0.9795 - val_loss: 3.3160 - val_categorical_accuracy: 0.5288 - lr: 1.0000e-04\n",
      "Epoch 475/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0651 - categorical_accuracy: 0.9774 - val_loss: 3.6524 - val_categorical_accuracy: 0.5282 - lr: 1.0000e-04\n",
      "Epoch 476/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0629 - categorical_accuracy: 0.9792 - val_loss: 3.0373 - val_categorical_accuracy: 0.5517 - lr: 1.0000e-04\n",
      "Epoch 477/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0613 - categorical_accuracy: 0.9793 - val_loss: 3.5868 - val_categorical_accuracy: 0.5251 - lr: 1.0000e-04\n",
      "Epoch 478/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0678 - categorical_accuracy: 0.9752 - val_loss: 2.8919 - val_categorical_accuracy: 0.5507 - lr: 1.0000e-04\n",
      "Epoch 479/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0636 - categorical_accuracy: 0.9784 - val_loss: 2.9250 - val_categorical_accuracy: 0.5549 - lr: 1.0000e-04\n",
      "Epoch 480/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0646 - categorical_accuracy: 0.9766 - val_loss: 3.2036 - val_categorical_accuracy: 0.5549 - lr: 1.0000e-04\n",
      "Epoch 481/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0634 - categorical_accuracy: 0.9782 - val_loss: 3.4338 - val_categorical_accuracy: 0.5672 - lr: 1.0000e-04\n",
      "Epoch 482/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0713 - categorical_accuracy: 0.9754 - val_loss: 3.6478 - val_categorical_accuracy: 0.5521 - lr: 1.0000e-04\n",
      "Epoch 483/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0613 - categorical_accuracy: 0.9790 - val_loss: 3.3972 - val_categorical_accuracy: 0.5343 - lr: 1.0000e-04\n",
      "Epoch 484/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0624 - categorical_accuracy: 0.9780 - val_loss: 3.1190 - val_categorical_accuracy: 0.5586 - lr: 1.0000e-04\n",
      "Epoch 485/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0642 - categorical_accuracy: 0.9779 - val_loss: 3.4119 - val_categorical_accuracy: 0.5384 - lr: 1.0000e-04\n",
      "Epoch 486/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0670 - categorical_accuracy: 0.9768 - val_loss: 3.4625 - val_categorical_accuracy: 0.5437 - lr: 1.0000e-04\n",
      "Epoch 487/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0629 - categorical_accuracy: 0.9781 - val_loss: 3.1732 - val_categorical_accuracy: 0.5515 - lr: 1.0000e-04\n",
      "Epoch 488/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0671 - categorical_accuracy: 0.9774 - val_loss: 3.3925 - val_categorical_accuracy: 0.5543 - lr: 1.0000e-04\n",
      "Epoch 489/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0612 - categorical_accuracy: 0.9795 - val_loss: 3.2715 - val_categorical_accuracy: 0.5478 - lr: 1.0000e-04\n",
      "Epoch 490/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0636 - categorical_accuracy: 0.9787 - val_loss: 3.2538 - val_categorical_accuracy: 0.5327 - lr: 1.0000e-04\n",
      "Epoch 491/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0649 - categorical_accuracy: 0.9784 - val_loss: 2.9942 - val_categorical_accuracy: 0.5560 - lr: 1.0000e-04\n",
      "Epoch 492/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0627 - categorical_accuracy: 0.9787 - val_loss: 2.9727 - val_categorical_accuracy: 0.5576 - lr: 1.0000e-04\n",
      "Epoch 493/500\n",
      "638/638 [==============================] - 8s 13ms/step - loss: 0.0597 - categorical_accuracy: 0.9790 - val_loss: 2.9209 - val_categorical_accuracy: 0.5551 - lr: 1.0000e-04\n",
      "Epoch 494/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0639 - categorical_accuracy: 0.9786 - val_loss: 3.2908 - val_categorical_accuracy: 0.5549 - lr: 1.0000e-04\n",
      "Epoch 495/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0620 - categorical_accuracy: 0.9793 - val_loss: 3.5480 - val_categorical_accuracy: 0.5378 - lr: 1.0000e-04\n",
      "Epoch 496/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0643 - categorical_accuracy: 0.9782 - val_loss: 3.3126 - val_categorical_accuracy: 0.5464 - lr: 1.0000e-04\n",
      "Epoch 497/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0610 - categorical_accuracy: 0.9784 - val_loss: 3.1321 - val_categorical_accuracy: 0.5511 - lr: 1.0000e-04\n",
      "Epoch 498/500\n",
      "638/638 [==============================] - 7s 12ms/step - loss: 0.0664 - categorical_accuracy: 0.9768 - val_loss: 3.0666 - val_categorical_accuracy: 0.5535 - lr: 1.0000e-04\n",
      "Epoch 499/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0643 - categorical_accuracy: 0.9788 - val_loss: 3.1133 - val_categorical_accuracy: 0.5443 - lr: 1.0000e-04\n",
      "Epoch 500/500\n",
      "638/638 [==============================] - 7s 11ms/step - loss: 0.0657 - categorical_accuracy: 0.9779 - val_loss: 3.0897 - val_categorical_accuracy: 0.5545 - lr: 1.0000e-04\n",
      "\n",
      "Training Complete!\n",
      "\n",
      "798/798 [==============================] - 4s 5ms/step - loss: 0.6423 - categorical_accuracy: 0.9040\n",
      "Action Classifier Evaluation:\n",
      "Loss: 0.6423447132110596\n",
      "Accuracy: 0.9040360450744629\n"
     ]
    }
   ],
   "source": [
    "action_classifer = train_action_classifier(sampled_data_classifier, sampled_labels_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./actionClassifier\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./actionClassifier\\assets\n"
     ]
    }
   ],
   "source": [
    "action_classifer.save('./actionClassifier', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az25davkJq9Y"
   },
   "source": [
    "### Running experiments with different hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimentations using Simple Multivariate Time Series Models (Simple RNN, LSTM, GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple LSTM model with Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN1 = BasicRNN(RNN_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN1, simple_RNN1 = resume_training(simple_RNN1, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN1_pred = simple_RNN1.predict(sampled_dataX_short_term)\n",
    "rnn1_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN1_pred)\n",
    "rnn1_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN1_pred)\n",
    "\n",
    "print('Simple LSTM Model with RELU activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn1_npss, rnn1_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple GRU model with Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN2 = BasicRNN(RNN_type=\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN2, simple_RNN2 = resume_training(simple_RNN2, sampled_dataX_short_term, sampled_dataY_short_term, epochs=10, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN2_pred = simple_RNN2.predict(sampled_dataX_short_term)\n",
    "rnn2_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN2_pred)\n",
    "rnn2_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN2_pred)\n",
    "\n",
    "print('Simple GRU Model with RELU activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn2_npss, rnn2_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple RNN model with Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN3 = BasicRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN3, simple_RNN3 = resume_training(simple_RNN3, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN3_pred = simple_RNN3.predict(sampled_dataX_short_term)\n",
    "rnn3_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN3_pred)\n",
    "rnn3_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN3_pred)\n",
    "\n",
    "print('Simple RNN Model with RELU activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn3_npss, rnn3_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple RNN model with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN4 = BasicRNN(activation=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN4, simple_RNN4 = resume_training(simple_RNN4, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN4_pred = simple_RNN4.predict(sampled_dataX_short_term)\n",
    "rnn4_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN4_pred)\n",
    "rnn4_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN4_pred)\n",
    "\n",
    "print('Simple RNN Model with Linear activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn4_npss, rnn4_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple LSTM model with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN5 = BasicRNN(RNN_type=\"LSTM\", activation=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN5, simple_RNN5 = resume_training(simple_RNN5, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN5_pred = simple_RNN5.predict(sampled_dataX_short_term)\n",
    "rnn5_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN5_pred)\n",
    "rnn5_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN5_pred)\n",
    "\n",
    "print('Simple LSTM Model with Linear activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn5_npss, rnn5_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple GRU model with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN6 = BasicRNN(RNN_type=\"GRU\", activation=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN6, simple_RNN6 = resume_training(simple_RNN6, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN6_pred = simple_RNN6.predict(sampled_dataX_short_term)\n",
    "rnn6_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN6_pred)\n",
    "rnn6_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN6_pred)\n",
    "\n",
    "print('Simple GRU Model with Linear activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn6_npss, rnn6_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GlocalNet Base Implementation Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment on Base Paper with Downsampled movable datapoints and MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model1, glocal_model1 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, dropout=0.0, epochs=1, \n",
    "                                               use_mse=True, output_diminsion=34, validation_split=0.2, interpolate_frames=2, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model1_pred = glocal_model1.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model1_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model1_pred)\n",
    "glocal_model1_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model1_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 1:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model1_npss, glocal_model1_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment on Base Paper with Downsampled movable datapoints and Joint loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model2, glocal_model2 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, dropout=0.0, epochs=50, \n",
    "                                               output_diminsion=34, validation_split=0.2, interpolate_frames=2, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model2_pred = glocal_model2.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model2_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model2_pred)\n",
    "glocal_model2_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model2_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 2:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model2_npss, glocal_model2_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with ONLY Glogen model and MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model3, glocal_model3 = run_experiment(preprocessed_downsampled_dataX_shortterm_movable, preprocessed_downsampled_dataY_shortterm_movable, epochs=50, only_glogen=True,\n",
    "                                                batch_size=20, output_diminsion=34, use_mse=True, validation_split=0.2, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model3_pred = glocal_model3.predict(preprocessed_downsampled_dataX_shortterm_movable)\n",
    "glocal_model3_npss = Metrics().NPSS(preprocessed_downsampled_dataY_shortterm_movable, glocal_model3_pred)\n",
    "glocal_model3_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_shortterm_movable, glocal_model3_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 3:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model3_npss, glocal_model3_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with ONLY Glogen model and Joint Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model4, glocal_model4 = run_experiment(preprocessed_downsampled_dataX_shortterm_movable, preprocessed_downsampled_dataY_shortterm_movable, epochs=50, only_glogen=True,\n",
    "                                                batch_size=20, output_diminsion=34, validation_split=0.2, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model4_pred = glocal_model4.predict(preprocessed_downsampled_dataX_shortterm_movable)\n",
    "glocal_model4_npss = Metrics().NPSS(preprocessed_downsampled_dataY_shortterm_movable, glocal_model4_pred)\n",
    "glocal_model4_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_shortterm_movable, glocal_model4_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 4:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model4_npss, glocal_model4_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with ONLY Glogen model, MSE Loss and Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model5, glocal_model5 = run_experiment(preprocessed_downsampled_dataX_shortterm_movable, preprocessed_downsampled_dataY_shortterm_movable, epochs=50, only_glogen=True,\n",
    "                                                batch_size=20, output_diminsion=34, use_mse=True, validation_split=0.2, self_attention=True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model5_pred = glocal_model5.predict(preprocessed_downsampled_dataX_shortterm_movable)\n",
    "glocal_model5_npss = Metrics().NPSS(preprocessed_downsampled_dataY_shortterm_movable, glocal_model5_pred)\n",
    "glocal_model5_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_shortterm_movable, glocal_model5_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 5:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model5_npss, glocal_model5_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with ONLY Glogen model, Joint Loss and Cross Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model6, glocal_model6 = run_experiment(preprocessed_downsampled_dataX_shortterm_movable, preprocessed_downsampled_dataY_shortterm_movable, epochs=50, only_glogen=True,\n",
    "                                                batch_size=20, output_diminsion=34, validation_split=0.2, cross_attention=True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model6_pred = glocal_model6.predict(preprocessed_downsampled_dataX_shortterm_movable)\n",
    "glocal_model6_npss = Metrics().NPSS(preprocessed_downsampled_dataY_shortterm_movable, glocal_model6_pred)\n",
    "glocal_model6_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_shortterm_movable, glocal_model6_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 6:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model6_npss, glocal_model6_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimentations with Self Attention Mechanism and MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model7, glocal_model7 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, dropout=0.0, epochs=50, interpolate_frames=2,\n",
    "                                               use_mse=True, output_diminsion=34, validation_split=0.2, self_attention = True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model7_pred = glocal_model7.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model7_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model7_pred)\n",
    "glocal_model7_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model7_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 7:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model7_npss, glocal_model7_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimentations with Self Attention Mechanism and Joint Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model8, glocal_model8 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, interpolate_frames=2,\n",
    "                                                 dropout=0.0, epochs=50, output_diminsion=34, validation_split=0.2, self_attention = True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model8_pred = glocal_model8.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model8_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model8_pred)\n",
    "glocal_model8_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model8_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 8:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model8_npss, glocal_model8_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimentations with Cross Attention Mechanism and MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model9, glocal_model9 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, dropout=0.0, epochs=50, interpolate_frames=2, \n",
    "                                               use_mse=True, output_diminsion=34, validation_split=0.2, cross_attention = True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model9_pred = glocal_model9.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model9_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model9_pred)\n",
    "glocal_model9_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model9_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 9:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model9_npss, glocal_model9_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimentations with Cross Attention Mechanism and Joint Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model10, glocal_model10 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, dropout=0.0, epochs=50, interpolate_frames=2,\n",
    "                                                 output_diminsion=34, validation_split=0.2, cross_attention = True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model10_pred = glocal_model10.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model10_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model10_pred)\n",
    "glocal_model10_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model10_pred, number_of_joints=17)\n",
    "\n",
    "print('Model 10:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model10_npss, glocal_model10_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import lineStyles\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(history_simple_RNN6.history['loss'][:10], linestyle='--', label='GRU with Linear Activation')\n",
    "\n",
    "plt.plot(history_model1.history['loss'][:10], label='Glogen with All Joints Without Preprocessing')\n",
    "plt.plot(history_model2.history['loss'][:10], label='GLogen with All Joints With Preprocessing')\n",
    "plt.plot(history_model3.history['loss'][:10], label='Glogen with Movable Joints With Preprocessing')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XaBTxF3Pbmo"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frames(glocal_model5_pred[4000], dynamic_joints_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frames(preprocessed_downsampled_dataY_movable[4000], dynamic_joints_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWZSPjKM9u-z"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] https://github.com/una-dinosauria/3d-pose-baseline/blob/master/src/data_utils.py\n",
    "\n",
    "[2] Gopalakrishnan, Anand, et al. \"A neural temporal model for human motion prediction.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019\n",
    "\n",
    "[3] https://github.com/cr7anand/neural_temporal_models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KTHffB1KrfBy",
    "H-ekHZVlrq1o",
    "hQg05uuoRtEi",
    "nozip8IVhhkg"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('srp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0e4e93b9d45a82e1228628a8d193dbf3f9bf0f6a1982bb9f6baccff302a2bd6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
