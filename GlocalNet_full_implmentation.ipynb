{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6oWUCJCnPmT",
        "outputId": "16123484-46f9-4547-fe0e-e7d3c9cd2adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.8.0\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model as Model_\n",
        "from tensorflow.keras.layers import Input, ReLU, LSTM, Dense, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "print(tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Need only to be used with google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuoR9zxVndMP",
        "outputId": "1e11d2ec-aceb-4a8d-e582-93f7e26ea71b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "class Dataset_Preprocessing:\n",
        "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
        "        \n",
        "        #Dataset Directory path\n",
        "        self.dir_path = dir_path\n",
        "        \n",
        "        #Which Dimension file to include, possible values: 2 and 3\n",
        "        self.include_dimension = include_dimension\n",
        "        \n",
        "        #Total frames in one Sample\n",
        "        self.sample_size = sample_size\n",
        "        \n",
        "        #Activity classes to include\n",
        "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
        "        \n",
        "        #Total activity classes\n",
        "        self.total_classes = len(self.classes)\n",
        "        \n",
        "        #Subject Folders names in the Dataset\n",
        "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
        "    \n",
        "    def read_dataset(self):\n",
        "        try:\n",
        "            #Contains all the different activity vectors\n",
        "            activity_vector = {}\n",
        "            \n",
        "            #Contains the overall dataset\n",
        "            sampled_data = None\n",
        "            \n",
        "            #Based on dimensions, which folder to use for extracting the dataset files\n",
        "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
        "            \n",
        "            #Checking if the dataset path is valid\n",
        "            if not os.path.exists(self.dir_path):\n",
        "                print('The Data Directory Does not Exist!')\n",
        "                return None\n",
        "\n",
        "            #Iterating over all the subject folders\n",
        "            for fld in self.internal_folders:\n",
        "                #Iterating for each file in the specified folder\n",
        "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
        "                    #Extracting the activity from the filename\n",
        "                    activity = self.__extract_activity(file)\n",
        "                    \n",
        "                    if activity not in self.classes:\n",
        "                        continue\n",
        "                    \n",
        "                    #Reading the CSV file using Pandas\n",
        "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
        "\n",
        "                    #Formulating the activity vector using one hot encoding\n",
        "                    if activity not in activity_vector:\n",
        "                        total_keys = len(activity_vector.keys())\n",
        "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
        "                        activity_vector[activity][total_keys] = 1\n",
        "                    vector = activity_vector[activity]\n",
        "                    \n",
        "                    #Sampling the dataset\n",
        "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
        "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
        "                    \n",
        "            return sampled_data\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __extract_activity(self, filename):\n",
        "        try:\n",
        "            #Extracting the filename and excluding the extension\n",
        "            name = os.path.splitext(filename)[0]\n",
        "            \n",
        "            #Substituting the empty string with characters other than english alphabets\n",
        "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
        "            return activity\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __group_samples(self, dataset, sample_size, activity):\n",
        "        try:\n",
        "            #Checking if the dataset is a Pandas Dataframe\n",
        "            if not isinstance(dataset, pd.DataFrame):\n",
        "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
        "                return None\n",
        "            \n",
        "            #Appending activity class to each row in the dataset\n",
        "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
        "            \n",
        "            #Reshaping the dataset into sample batches\n",
        "            total_samples = dataset.shape[0]//sample_size\n",
        "            total_features = dataset.shape[1]\n",
        "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
        "            \n",
        "            return grouped_rows\n",
        "        except Exception as e:\n",
        "            print(e)"
      ],
      "metadata": {
        "id": "ikRIhsrrnhlH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For long term prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
        "sampled_data = Dataset_Preprocessing('/content/drive/MyDrive/Colab Notebooks/H3.6csv', sample_size=60).read_dataset()"
      ],
      "metadata": {
        "id": "f8PgK1UVnkrw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batches = sampled_data.shape[0]"
      ],
      "metadata": {
        "id": "PvK-bUK7nsm4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_data = sampled_data[:total_batches-(total_batches%100)]"
      ],
      "metadata": {
        "id": "416CBxXNlTSq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
        "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
        "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
        "    labels = dataset[:,np.s_[input_sequance_size:], :64]\n",
        "    \n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "8nABIG0rnxl_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dataX, sampled_dataY = split_to_features_labels(sampled_data, input_sequance_size=10)"
      ],
      "metadata": {
        "id": "h6bSQTwdn2Fo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total Samples: {}'.format(sampled_dataY.shape[0]))\n",
        "print('Total Frames: {}'.format(sampled_dataY.shape[1]))\n",
        "print('Total Features: {}'.format(sampled_dataY.shape[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVXQiscVn4O2",
        "outputId": "57429efe-4ed9-438a-e8d0-7a3dc16e420c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Samples: 25500\n",
            "Total Frames: 50\n",
            "Total Features: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#returns a list of the interpolated frames between x0 and x1\n",
        "def interpolateFrames(glogen_sparse_output_predictions, batch_size=100, num_of_frames=5) :\n",
        "  timesteps = glogen_sparse_output_predictions.shape[1]\n",
        "  features = glogen_sparse_output_predictions.shape[2]\n",
        "  interpolated_frames = np.zeros((batch_size, timesteps, num_of_frames, features))\n",
        "  \n",
        "  for batch in range(batch_size) :\n",
        "    for t in range(glogen_sparse_output_predictions.shape[1]-1) :\n",
        "      for j in range(num_of_frames) :\n",
        "        X_i0 = glogen_sparse_output_predictions[batch, t]\n",
        "        X_i1 = glogen_sparse_output_predictions[batch, t+1]\n",
        "        \n",
        "        alpha_j = j/num_of_frames\n",
        "        current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
        "        interpolated_frames[batch, t, j] = current_frame\n",
        "\n",
        "  return interpolated_frames "
      ],
      "metadata": {
        "id": "fRqu0a1yrigj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InterpolationLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_frames=5):\n",
        "        super(InterpolationLayer, self).__init__()\n",
        "        self.num_frames = num_frames\n",
        "       \n",
        "    def interpolateFrames(self, inputs):\n",
        "      batch_size = inputs.shape[0]\n",
        "      timesteps = inputs.shape[1]\n",
        "      features = inputs.shape[2]\n",
        "      #interpolated_frames = tf.zeros([batch_size, timesteps, 0, features])\n",
        "      interpolated_frames = tf.zeros([0, features])\n",
        "\n",
        "      for batch in range(batch_size) :\n",
        "        for t in range(timesteps) :\n",
        "          for j in range(self.num_frames) :\n",
        "            X_i0 = inputs[batch, t]\n",
        "            if(t == timesteps-1) :\n",
        "              X_i1 = inputs[batch, t]\n",
        "            else :  \n",
        "              X_i1 = inputs[batch, t+1]\n",
        "            alpha_j = j/self.num_frames\n",
        "            current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
        "            current_frame = tf.reshape(current_frame, [1, features])\n",
        "            interpolated_frames = tf.concat((interpolated_frames, current_frame), axis=0)\n",
        "            #interpolated_frames[batch, t, j] = current_frame\n",
        "\n",
        "      print(interpolated_frames.shape)\n",
        "\n",
        "      interpolated_frames = tf.reshape(interpolated_frames,\n",
        "                                       [batch_size, (timesteps)*self.num_frames, features])\n",
        "      return interpolated_frames\n",
        "\n",
        "    def call(self, inputs):\n",
        "      return self.interpolateFrames(inputs)"
      ],
      "metadata": {
        "id": "QsfCMT-0zRTs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GloGen(Model_):\n",
        "    def __init__(self, enocder_hidden_state=200, decoder_hidden_state=200, output_diminsion=64, activation='relu'):\n",
        "        super(GloGen, self).__init__()\n",
        "        self.encoder = LSTM(enocder_hidden_state, return_state=True, return_sequences=True)\n",
        "        self.decoder = LSTM(decoder_hidden_state, return_sequences=True, return_state=True)\n",
        "        self.dense_layer = TimeDistributed(Dense(output_diminsion, activation=activation)) \n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoder_outputs, state_h, state_c = self.encoder(inputs)\n",
        "        encoder_states = [state_h, state_c]\n",
        "        output, _, _ = self.decoder(encoder_outputs, initial_state=encoder_states)\n",
        "        output = self.dense_layer(output)\n",
        "        return output"
      ],
      "metadata": {
        "id": "7PIJXvLaaEvY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=100"
      ],
      "metadata": {
        "id": "zTawxJE3m8c9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "\n",
        "encoder_inputs = Input(batch_size=batch_size,shape= (10, 74))\n",
        "encoder = LSTM(200, return_state=True, return_sequences=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_lstm = LSTM(200, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(encoder_outputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(64, activation='relu')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "#Interpolation phase before locgen network\n",
        "interpolation_layer = InterpolationLayer(num_frames=5)\n",
        "interpolated_frames = interpolation_layer(decoder_outputs)\n",
        "\n",
        "#start of locgen phase  \n",
        "encoder_inputs_locgen = interpolated_frames\n",
        "encoder_locgen = LSTM(200, return_state=True, return_sequences=True)\n",
        "encoder_outputs_locgen, state_h_locgen, state_c_locgen = encoder_locgen(encoder_inputs_locgen)\n",
        "\n",
        "encoder_states_locgen = [state_h_locgen, state_c_locgen]\n",
        "\n",
        "decoder_lstm_locgen = LSTM(200, return_sequences=True, return_state=True)\n",
        "decoder_outputs_locgen, _, _ = decoder_lstm_locgen(encoder_outputs_locgen,\n",
        "                                     initial_state=encoder_states_locgen)\n",
        "decoder_dense_locgen = Dense(64, activation='relu')\n",
        "decoder_outputs_locgen = decoder_dense_locgen(decoder_outputs_locgen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW-CF9MioMkX",
        "outputId": "180b1a22-6801-42c6-becb-9736900bfd6a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5000, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glolocal_model = tf.keras.Model(inputs=encoder_inputs, outputs=decoder_outputs_locgen)"
      ],
      "metadata": {
        "id": "JYnUHWKNqUGt"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glolocal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n",
        "                       loss=tf.keras.losses.mean_squared_error,\n",
        "                       metrics=[tf.keras.losses.mean_absolute_percentage_error])"
      ],
      "metadata": {
        "id": "XKHt37PodKnO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glolocal_model.fit(sampled_dataX, sampled_dataY, batch_size=100, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "jAH-QGnkdiF5",
        "outputId": "29557584-d126-499f-9d45-763587eafd39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "(5000, 64)\n",
            "(5000, 64)\n",
            "255/255 [==============================] - 475s 281ms/step - loss: 199398.0156 - mean_absolute_percentage_error: 89.0227\n",
            "Epoch 2/50\n",
            "255/255 [==============================] - 72s 281ms/step - loss: 132289.6406 - mean_absolute_percentage_error: 69.3423\n",
            "Epoch 3/50\n",
            "255/255 [==============================] - 72s 282ms/step - loss: 87301.1484 - mean_absolute_percentage_error: 52.9392\n",
            "Epoch 4/50\n",
            "255/255 [==============================] - 72s 283ms/step - loss: 58293.9453 - mean_absolute_percentage_error: 40.4844\n",
            "Epoch 5/50\n",
            "255/255 [==============================] - 73s 287ms/step - loss: 40580.2539 - mean_absolute_percentage_error: 32.0643\n",
            "Epoch 6/50\n",
            "255/255 [==============================] - 71s 280ms/step - loss: 30437.5391 - mean_absolute_percentage_error: 26.9430\n",
            "Epoch 7/50\n",
            "255/255 [==============================] - 71s 279ms/step - loss: 25061.9590 - mean_absolute_percentage_error: 24.2190\n",
            "Epoch 8/50\n",
            "255/255 [==============================] - 72s 281ms/step - loss: 22466.4824 - mean_absolute_percentage_error: 22.9843\n",
            "Epoch 9/50\n",
            "255/255 [==============================] - 71s 280ms/step - loss: 21342.3008 - mean_absolute_percentage_error: 22.5360\n",
            "Epoch 10/50\n",
            "255/255 [==============================] - 71s 280ms/step - loss: 20913.0391 - mean_absolute_percentage_error: 22.4644\n",
            "Epoch 11/50\n",
            "255/255 [==============================] - 71s 280ms/step - loss: 20774.1680 - mean_absolute_percentage_error: 22.5178\n",
            "Epoch 12/50\n",
            "255/255 [==============================] - 71s 279ms/step - loss: 20735.4766 - mean_absolute_percentage_error: 22.5644\n",
            "Epoch 13/50\n",
            "187/255 [=====================>........] - ETA: 18s - loss: 20736.2578 - mean_absolute_percentage_error: 22.6029"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-be07e445029d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mglolocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_dataX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_dataY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sQ-XPaUFd4D0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}