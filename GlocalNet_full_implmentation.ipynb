{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "KTHffB1KrfBy"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6oWUCJCnPmT",
        "outputId": "83206498-5fb1-4c3a-f4ec-1bf1a49f2b8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.8.0\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model as Model_\n",
        "from tensorflow.keras.layers import Input, ReLU, LSTM, Dense, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow_model_remediation.min_diff.losses.mmd_loss as MMD\n",
        "import tensorflow_model_remediation.min_diff.losses.adjusted_mmd_loss as adjustedMMD\n",
        "\n",
        "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "print(tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_remediation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "064sei98pTAM",
        "outputId": "6f3a7e85-a21f-498a-d370-6fce9df8e0d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow_model_remediation in /usr/local/lib/python3.7/dist-packages (0.1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (0.3.5.1)\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (4.0.3)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (0.12.0)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.6.3)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.21.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (2.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.14.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (4.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (0.26.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.48.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (3.17.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->tensorflow_model_remediation) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0.0->tensorflow_model_remediation) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (3.4.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (3.2.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_model_remediation) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_model_remediation) (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Need only to be used with google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuoR9zxVndMP",
        "outputId": "584147d2-4991-4563-a9ca-788b99d6cbeb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "class Dataset_Preprocessing:\n",
        "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
        "        \n",
        "        #Dataset Directory path\n",
        "        self.dir_path = dir_path\n",
        "        \n",
        "        #Which Dimension file to include, possible values: 2 and 3\n",
        "        self.include_dimension = include_dimension\n",
        "        \n",
        "        #Total frames in one Sample\n",
        "        self.sample_size = sample_size\n",
        "        \n",
        "        #Activity classes to include\n",
        "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
        "        \n",
        "        #Total activity classes\n",
        "        self.total_classes = len(self.classes)\n",
        "        \n",
        "        #Subject Folders names in the Dataset\n",
        "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
        "    \n",
        "    def read_dataset(self):\n",
        "        try:\n",
        "            #Contains all the different activity vectors\n",
        "            activity_vector = {}\n",
        "            \n",
        "            #Contains the overall dataset\n",
        "            sampled_data = None\n",
        "            \n",
        "            #Based on dimensions, which folder to use for extracting the dataset files\n",
        "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
        "            \n",
        "            #Checking if the dataset path is valid\n",
        "            if not os.path.exists(self.dir_path):\n",
        "                print('The Data Directory Does not Exist!')\n",
        "                return None\n",
        "\n",
        "            #Iterating over all the subject folders\n",
        "            for fld in self.internal_folders:\n",
        "                #Iterating for each file in the specified folder\n",
        "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
        "                    #Extracting the activity from the filename\n",
        "                    activity = self.__extract_activity(file)\n",
        "                    \n",
        "                    if activity not in self.classes:\n",
        "                        continue\n",
        "                    \n",
        "                    #Reading the CSV file using Pandas\n",
        "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
        "\n",
        "                    #Formulating the activity vector using one hot encoding\n",
        "                    if activity not in activity_vector:\n",
        "                        total_keys = len(activity_vector.keys())\n",
        "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
        "                        activity_vector[activity][total_keys] = 1\n",
        "                    vector = activity_vector[activity]\n",
        "                    \n",
        "                    #Sampling the dataset\n",
        "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
        "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
        "                    \n",
        "            return sampled_data\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __extract_activity(self, filename):\n",
        "        try:\n",
        "            #Extracting the filename and excluding the extension\n",
        "            name = os.path.splitext(filename)[0]\n",
        "            \n",
        "            #Substituting the empty string with characters other than english alphabets\n",
        "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
        "            return activity\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __group_samples(self, dataset, sample_size, activity):\n",
        "        try:\n",
        "            #Checking if the dataset is a Pandas Dataframe\n",
        "            if not isinstance(dataset, pd.DataFrame):\n",
        "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
        "                return None\n",
        "            \n",
        "            #Appending activity class to each row in the dataset\n",
        "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
        "            \n",
        "            #Reshaping the dataset into sample batches\n",
        "            total_samples = dataset.shape[0]//sample_size\n",
        "            total_features = dataset.shape[1]\n",
        "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
        "            \n",
        "            return grouped_rows\n",
        "        except Exception as e:\n",
        "            print(e)"
      ],
      "metadata": {
        "id": "ikRIhsrrnhlH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For long term prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
        "sampled_data = Dataset_Preprocessing('/content/drive/MyDrive/Colab Notebooks/H3.6csv', sample_size=60).read_dataset()"
      ],
      "metadata": {
        "id": "f8PgK1UVnkrw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To make the data divisible for batch size of hunderd\n",
        "total_batches = sampled_data.shape[0]\n",
        "sampled_data = sampled_data[:total_batches-(total_batches%100)]"
      ],
      "metadata": {
        "id": "416CBxXNlTSq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
        "    \"\"\"\n",
        "    Function for splitting the data into features(with sequance size=iput_sequance_size)\n",
        "    and labels which should be the remainder of the sample length \n",
        "    \"\"\"\n",
        "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
        "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
        "    labels = dataset[:,np.s_[input_sequance_size:], :64]\n",
        "    \n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "8nABIG0rnxl_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dataX, sampled_dataY = split_to_features_labels(sampled_data, input_sequance_size=10)"
      ],
      "metadata": {
        "id": "h6bSQTwdn2Fo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total Samples: {}'.format(sampled_dataY.shape[0]))\n",
        "print('Total Frames: {}'.format(sampled_dataY.shape[1]))\n",
        "print('Total Features: {}'.format(sampled_dataY.shape[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVXQiscVn4O2",
        "outputId": "565d9101-020c-464b-bc6a-1956ffdf014d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Samples: 25500\n",
            "Total Frames: 50\n",
            "Total Features: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InterpolationLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom interpolation layer extending the keras layer class\n",
        "    it has one attribute num_frames to be interpolated between each two consecutive \n",
        "    timesteps\n",
        "    it has one main function interpolateFrames  \n",
        "    \"\"\"\n",
        "  \n",
        "    def __init__(self, num_frames=5):\n",
        "        super(InterpolationLayer, self).__init__()\n",
        "        self.num_frames = num_frames\n",
        "       \n",
        "    def interpolateFrames(self, inputs):\n",
        "      \"\"\"\n",
        "      Takes input tensors of shape(batch_size, timesteps, features)\n",
        "      returns interpolated frames with shape(batch_size, timesteps*num_frames, features)\n",
        "      \"\"\"\n",
        "      batch_size = inputs.shape[0]\n",
        "      timesteps = inputs.shape[1]\n",
        "      features = inputs.shape[2]\n",
        "      #interpolated_frames = tf.zeros([batch_size, timesteps, 0, features])\n",
        "      interpolated_frames = tf.zeros([0, features])\n",
        "\n",
        "      for batch in range(batch_size) :\n",
        "        for t in range(timesteps) :\n",
        "          for j in range(self.num_frames) :\n",
        "            X_i0 = inputs[batch, t]\n",
        "            if(t == timesteps-1) :\n",
        "              X_i1 = inputs[batch, t]\n",
        "            else :  \n",
        "              X_i1 = inputs[batch, t+1]\n",
        "            alpha_j = j/self.num_frames\n",
        "            current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
        "            current_frame = tf.reshape(current_frame, [1, features])\n",
        "            interpolated_frames = tf.concat((interpolated_frames, current_frame), axis=0)\n",
        "            \n",
        "      interpolated_frames = tf.reshape(interpolated_frames,\n",
        "                                       [batch_size, (timesteps)*self.num_frames, features])\n",
        "      return interpolated_frames\n",
        "\n",
        "    def call(self, inputs):\n",
        "      return self.interpolateFrames(inputs)"
      ],
      "metadata": {
        "id": "QsfCMT-0zRTs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GlocalNet(Model_):\n",
        "    \"\"\"\n",
        "    A full GlocalNet implementation include the three main stages\n",
        "    Glogen generating initial sparse frames\n",
        "    Interpolation layer generating dense frames from Glogen output\n",
        "    Locgen generating the final output by smoothing the interpolated frames\n",
        "    \"\"\"\n",
        "    def __init__(self, enocder_hidden_state=200, decoder_hidden_state=200, \n",
        "                 output_diminsion=64, activation='relu', interpolation_frames=5):\n",
        "        super(GlocalNet, self).__init__()\n",
        "        #Glogen layers\n",
        "        self.glogen_encoder = LSTM(enocder_hidden_state, return_state=True, return_sequences=True)\n",
        "        self.glogen_decoder = LSTM(decoder_hidden_state, return_sequences=True, return_state=True)\n",
        "        self.glogen_dense_layer = TimeDistributed(Dense(output_diminsion, activation=activation)) \n",
        "        \n",
        "        #Interpolation layer\n",
        "        self.interpolation_layer = InterpolationLayer(num_frames=interpolation_frames)\n",
        "\n",
        "        #Locgen layers\n",
        "        self.locgen_encoder = LSTM(enocder_hidden_state, return_sequences=True, return_state=True)\n",
        "        self.locgen_decoder = LSTM(decoder_hidden_state, return_sequences=True, return_state=True)\n",
        "        self.locgen_dense_layer = TimeDistributed(Dense(output_diminsion, activation=activation)) \n",
        "        \n",
        "    def call(self, inputs):\n",
        "        #Glogen calls      \n",
        "        encoder_outputs, state_h, state_c = self.glogen_encoder(inputs)\n",
        "        encoder_states = [state_h, state_c]\n",
        "        output, _, _ = self.glogen_decoder(encoder_outputs, initial_state=encoder_states)\n",
        "        glogen_output = self.glogen_dense_layer(output)\n",
        "\n",
        "        #Interpolation call\n",
        "        interpolated_frames = self.interpolation_layer(glogen_output)\n",
        "        \n",
        "        #Locgen calls\n",
        "        locgen_encoder_outputs, locgen_state_h, locgen_state_c = self.locgen_encoder(interpolated_frames)\n",
        "        locgen_encoder_states = [locgen_state_h, locgen_state_c]\n",
        "        locgen_output, _, _ = self.locgen_decoder(locgen_encoder_outputs, initial_state=locgen_encoder_states)\n",
        "        final_output = self.locgen_dense_layer(locgen_output)\n",
        "        return final_output"
      ],
      "metadata": {
        "id": "7PIJXvLaaEvY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class JointLoss() :\n",
        "  \"\"\"\n",
        "  Joint loss class with two weight attributes for two different losses\n",
        "  first one is the loss joint and the second is the loss_motion_flow\n",
        "  \"\"\"\n",
        "  def __init__(self, lambda1=0.5, lambda2=0.5) :\n",
        "    self.lambda1 = lambda1\n",
        "    self.lambda2 = lambda2\n",
        "\n",
        "  def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
        "      \"\"\"\n",
        "      Loss between the joint positions and its corresponding counterparts in the groundtruth\n",
        "      \"\"\"\n",
        "      diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
        "      return tf.reduce_sum(diff_norm_2, axis=1) \n",
        "\n",
        "  def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
        "      \"\"\"\n",
        "      Loss between the motion flow of predicted sequance and the ground truth\n",
        "      where the motion flow is the euclidean distance between each two consecutive frames\n",
        "      \"\"\"\n",
        "      predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
        "      real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
        "      prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
        "      return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
        "\n",
        "\n",
        "  def total_loss(self, target_sequance_batch, predicted_sequance_batch) :\n",
        "      \"\"\"\n",
        "      calculating the total loss through a combination of the joint_loss and motion_flow_loss\n",
        "      \"\"\"\n",
        "      joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
        "      motion_flow_loss = self.loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
        "      return self.lambda1*joints_loss + self.lambda2*motion_flow_loss"
      ],
      "metadata": {
        "id": "bVUFJsv3fVYh"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(learning_rate=0.002, lambda1=0.5, lambda2=0.5, use_mse=False,\n",
        "                   use_MMD=False, metrics=None, batch_size=100, epochs=50,\n",
        "                   validation_split=0.2) :\n",
        "  \"\"\"\n",
        "  Method takes all hyperparameters as input paramters and returns the model and history as\n",
        "  a result\n",
        "  \"\"\"\n",
        "  glocal_model = GlocalNet()\n",
        "  if use_mse :\n",
        "    loss_function = tf.keras.losses.mean_squared_error\n",
        "  elif use_MMD :\n",
        "    loss_function = adjustedMMD.AdjustedMMDLoss()\n",
        "  else :\n",
        "    loss_function = JointLoss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
        "  \n",
        "  glocal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                       loss=loss_function, metrics=metrics)\n",
        "  history = glocal_model.fit(sampled_dataX, sampled_dataY,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs)\n",
        "  return history, glocal_model"
      ],
      "metadata": {
        "id": "labA65KEakkZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment Running for MSE and joint loss"
      ],
      "metadata": {
        "id": "KTHffB1KrfBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_mse, glocal_model_mse = run_experiment(epochs=10, use_mse=True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQ-XPaUFd4D0",
        "outputId": "bf808cf3-cea2-4283-e53f-4d4e2dbba85b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "255/255 [==============================] - 415s 240ms/step - loss: 197673.3438 - mean_absolute_percentage_error: 88.2866\n",
            "Epoch 2/10\n",
            "255/255 [==============================] - 63s 248ms/step - loss: 128336.4766 - mean_absolute_percentage_error: 67.4349\n",
            "Epoch 3/10\n",
            "255/255 [==============================] - 62s 243ms/step - loss: 82020.9609 - mean_absolute_percentage_error: 50.1100\n",
            "Epoch 4/10\n",
            "255/255 [==============================] - 64s 250ms/step - loss: 52302.0547 - mean_absolute_percentage_error: 37.0668\n",
            "Epoch 5/10\n",
            "255/255 [==============================] - 61s 240ms/step - loss: 34254.2656 - mean_absolute_percentage_error: 28.3228\n",
            "Epoch 6/10\n",
            "255/255 [==============================] - 61s 241ms/step - loss: 23976.3828 - mean_absolute_percentage_error: 23.0670\n",
            "Epoch 7/10\n",
            "255/255 [==============================] - 63s 245ms/step - loss: 18552.9414 - mean_absolute_percentage_error: 20.3186\n",
            "Epoch 8/10\n",
            "255/255 [==============================] - 61s 241ms/step - loss: 15939.6357 - mean_absolute_percentage_error: 19.0722\n",
            "Epoch 9/10\n",
            "255/255 [==============================] - 62s 242ms/step - loss: 14812.7559 - mean_absolute_percentage_error: 18.6462\n",
            "Epoch 10/10\n",
            "255/255 [==============================] - 62s 245ms/step - loss: 14385.7510 - mean_absolute_percentage_error: 18.5867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history_jointLoss, glocal_model_jointLoss = run_experiment(epochs=10, lambda1=0.5, lambda2=0.5, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7FQ-HQ4bPVi",
        "outputId": "89e882cb-f5fd-4d5f-d5af-1497efaba172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "255/255 [==============================] - 419s 248ms/step - loss: 318235136.0000 - mean_absolute_percentage_error: 88.8408\n",
            "Epoch 2/10\n",
            "255/255 [==============================] - 63s 248ms/step - loss: 209204064.0000 - mean_absolute_percentage_error: 68.7124\n",
            "Epoch 3/10\n",
            "255/255 [==============================] - 64s 252ms/step - loss: 136290848.0000 - mean_absolute_percentage_error: 52.0030\n",
            "Epoch 4/10\n",
            "255/255 [==============================] - 63s 248ms/step - loss: 89421984.0000 - mean_absolute_percentage_error: 39.3298\n",
            "Epoch 5/10\n",
            "255/255 [==============================] - 64s 251ms/step - loss: 60899012.0000 - mean_absolute_percentage_error: 30.7696\n",
            "Epoch 6/10\n",
            "255/255 [==============================] - 65s 254ms/step - loss: 44627976.0000 - mean_absolute_percentage_error: 25.5873\n",
            "Epoch 7/10\n",
            "255/255 [==============================] - 64s 250ms/step - loss: 36028060.0000 - mean_absolute_percentage_error: 22.8682\n",
            "Epoch 8/10\n",
            "255/255 [==============================] - 64s 251ms/step - loss: 31884704.0000 - mean_absolute_percentage_error: 21.6249\n",
            "Epoch 9/10\n",
            "255/255 [==============================] - 63s 248ms/step - loss: 30101348.0000 - mean_absolute_percentage_error: 21.2003\n",
            "Epoch 10/10\n",
            "255/255 [==============================] - 64s 252ms/step - loss: 29427098.0000 - mean_absolute_percentage_error: 21.1448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running experiment with MMD"
      ],
      "metadata": {
        "id": "H-ekHZVlrq1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_mmd, glocal_model_mmd = run_experiment(epochs=10, use_MMD=True, metrics=[tf.keras.losses.mean_absolute_percentage_error])"
      ],
      "metadata": {
        "id": "KayDheMuk7oE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        },
        "outputId": "1ffdc5ce-819b-43e6-e9dd-641ab88419ab"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cef35840a7d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_mmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglocal_model_mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_MMD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_absolute_percentage_error\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-3a7221e4ca7d>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(learning_rate, lambda1, lambda2, use_mse, use_MMD, metrics, batch_size, epochs, validation_split)\u001b[0m\n\u001b[1;32m     18\u001b[0m   history = glocal_model.fit(sampled_dataX, sampled_dataY,\n\u001b[1;32m     19\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m           epochs=epochs)\n\u001b[0m\u001b[1;32m     21\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglocal_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 919, in compute_loss\n        y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.7/dist-packages/tensorflow_model_remediation/min_diff/losses/base_loss.py\", line 124, in __call__\n        loss = self.call(membership, predictions, sample_weight)\n    File \"/usr/local/lib/python3.7/dist-packages/tensorflow_model_remediation/min_diff/losses/adjusted_mmd_loss.py\", line 139, in call\n        predictions_kernel, normed_weights, pos_mask, neg_mask)\n    File \"/usr/local/lib/python3.7/dist-packages/tensorflow_model_remediation/min_diff/losses/mmd_loss.py\", line 99, in _calculate_mean\n        pos_mean_mask = tf.matmul(pos_mask, tf.transpose(pos_mask))\n\n    ValueError: Dimensions must be equal, but are 64 and 50 for '{{node adjusted_mmd_loss_inputs/MatMul_1}} = BatchMatMulV2[T=DT_FLOAT, adj_x=false, adj_y=false](adjusted_mmd_loss_inputs/Cast_1, adjusted_mmd_loss_inputs/transpose_1)' with input shapes: [100,50,64], [64,50,100].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glocalNet_model = GlocalNet()"
      ],
      "metadata": {
        "id": "hsi6FE8Tp4nK"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = glocalNet_model(sampled_dataX[:100])"
      ],
      "metadata": {
        "id": "zsJ44_f6ttrP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions.shape)\n",
        "print(sampled_dataY[:100].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jrwod1BdwxLF",
        "outputId": "6407df05-92f5-49c8-bfb1-62c649df0229"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 50, 64)\n",
            "(100, 50, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mmd_loss = MMD.MMDLoss()"
      ],
      "metadata": {
        "id": "IumYb0UTt1TV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmd_loss(tf.reshape(predictions, [100, 50*64]),tf.reshape(sampled_dataY[:100], [100, 50*64]))"
      ],
      "metadata": {
        "id": "ymePx4WOt8Gt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3wKSm6QMuGuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}