{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5bGejq7QySe"
   },
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6oWUCJCnPmT",
    "outputId": "7c5b49ce-0b4d-4f9a-eac8-f05ee626a96d"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model as Model_\n",
    "from tensorflow.keras.layers import Input, ReLU, LSTM, GRU, SimpleRNN, Dense, TimeDistributed, Bidirectional, GaussianNoise \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_remediation.min_diff.losses.mmd_loss as MMD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tUdJibeQWdF"
   },
   "source": [
    "## Dataset Reading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing all the movable joints in the human skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYTyMttRQVHV"
   },
   "outputs": [],
   "source": [
    "#Was done in the preprocessing in [1]\n",
    "# Joints in H3.6M -- data has 32 joints, but only 17 that move; these are the indices.\n",
    "H36M_NAMES = ['']*32\n",
    "H36M_NAMES[0]  = 'Hip'\n",
    "H36M_NAMES[1]  = 'RHip'\n",
    "H36M_NAMES[2]  = 'RKnee'\n",
    "H36M_NAMES[3]  = 'RFoot'\n",
    "H36M_NAMES[6]  = 'LHip'\n",
    "H36M_NAMES[7]  = 'LKnee'\n",
    "H36M_NAMES[8]  = 'LFoot'\n",
    "H36M_NAMES[12] = 'Spine'\n",
    "H36M_NAMES[13] = 'Thorax'\n",
    "H36M_NAMES[14] = 'Neck/Nose'\n",
    "H36M_NAMES[15] = 'Head'\n",
    "H36M_NAMES[17] = 'LShoulder'\n",
    "H36M_NAMES[18] = 'LElbow'\n",
    "H36M_NAMES[19] = 'LWrist'\n",
    "H36M_NAMES[25] = 'RShoulder'\n",
    "H36M_NAMES[26] = 'RElbow'\n",
    "H36M_NAMES[27] = 'RWrist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class to Read and Combine all the Dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikRIhsrrnhlH"
   },
   "outputs": [],
   "source": [
    "class Dataset_loading:\n",
    "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50,\n",
    "                 total_classes = 17, datatype = 'float32', include_movable_joints = False, batch_size = 20, \n",
    "                 include_action_labels = True, return_action_labels = False):\n",
    "        \n",
    "        #Dataset Directory path\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "        #Which Dimension file to include, possible values: 2 and 3\n",
    "        self.include_dimension = include_dimension\n",
    "        \n",
    "        #Total frames in one Sample\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        #Default Datatype for all the samples\n",
    "        self.datatype = datatype\n",
    "        \n",
    "        #Batch Size of the dataset for experimentation \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Boolean value to indicate whether to include action class in each frames\n",
    "        self.include_action_labels = include_action_labels\n",
    "        \n",
    "        #Whether to return action labels with data\n",
    "        self.return_action_labels = return_action_labels\n",
    "        \n",
    "        #Activity classes to include\n",
    "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
    "        \n",
    "        #Total activity classes\n",
    "        self.total_classes = len(self.classes)\n",
    "        \n",
    "        #Subject Folders names in the Dataset\n",
    "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
    "\n",
    "        #Boolean value indicating whether to include all joints or only the movable joints.\n",
    "        self.include_movable_joints = include_movable_joints\n",
    "        \n",
    "        self.movable_joints = [0, 1, 2, 3, 6, 7, 8, 12, 13, 14, 15, 17, 18, 19, 25, 26, 27]\n",
    "    \n",
    "    def read_dataset(self):\n",
    "        try:\n",
    "            #Contains all the different activity vectors\n",
    "            activity_vector = {}\n",
    "            \n",
    "            #Contains the overall dataset\n",
    "            sampled_data = None\n",
    "            sampled_labels = None\n",
    "            \n",
    "            #Based on dimensions, which folder to use for extracting the dataset files\n",
    "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
    "            \n",
    "            #Checking if the dataset path is valid\n",
    "            if not os.path.exists(self.dir_path):\n",
    "                print('The Data Directory Does not Exist!')\n",
    "                return None\n",
    "\n",
    "            #Iterating over all the subject folders\n",
    "            for fld in self.internal_folders:\n",
    "                #Iterating for each file in the specified folder\n",
    "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
    "                    #Extracting the activity from the filename\n",
    "                    activity = self.__extract_activity(file)\n",
    "                    \n",
    "                    if activity not in self.classes:\n",
    "                        continue\n",
    "                    \n",
    "                    #Reading the CSV file using Pandas\n",
    "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
    "\n",
    "                    #Formulating the activity vector using one hot encoding\n",
    "                    if activity not in activity_vector:\n",
    "                        total_keys = len(activity_vector.keys())\n",
    "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
    "                        activity_vector[activity][total_keys] = 1\n",
    "                    vector = activity_vector[activity]\n",
    "                    \n",
    "                    #Sampling the dataset\n",
    "                    grouped_sample, grouped_activity = self.__group_samples(data, self.sample_size, vector)\n",
    "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
    "                    sampled_labels = grouped_activity if sampled_labels is None else np.append(sampled_labels, grouped_activity, axis=0)\n",
    "            \n",
    "            #Changing the Datatype\n",
    "            sampled_data = sampled_data.astype(self.datatype)\n",
    "            \n",
    "            #To make the data divisible for batch size\n",
    "            total_batches = sampled_data.shape[0]\n",
    "            sampled_data = sampled_data[:total_batches - (total_batches % self.batch_size)]\n",
    "            sampled_labels = sampled_labels[:total_batches - (total_batches % self.batch_size)]\n",
    "            \n",
    "            if self.return_action_labels:\n",
    "                return sampled_data, sampled_labels\n",
    "            \n",
    "            return sampled_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __extract_activity(self, filename):\n",
    "        try:\n",
    "            #Extracting the filename and excluding the extension\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            #Substituting the empty string with characters other than english alphabets\n",
    "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
    "            return activity\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __group_samples(self, dataset, sample_size, activity):\n",
    "        try:\n",
    "            #Checking if the dataset is a Pandas Dataframe\n",
    "            if not isinstance(dataset, pd.DataFrame):\n",
    "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
    "                return None\n",
    "            \n",
    "            if self.include_movable_joints:\n",
    "                joints = list(chain.from_iterable((jt*2, (jt*2)+1) for jt in self.movable_joints))\n",
    "                dataset = dataset.iloc[: , joints].copy()\n",
    "\n",
    "            #Appending activity class to each row in the dataset\n",
    "            if self.include_action_labels:\n",
    "                dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
    "            \n",
    "            #Reshaping the dataset into sample batches\n",
    "            total_samples = dataset.shape[0]//sample_size\n",
    "            total_features = dataset.shape[1]\n",
    "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
    "            \n",
    "            grouped_activity = np.tile(activity, (dataset.shape[0]//self.sample_size, 1))\n",
    "            grouped_activity = grouped_activity[:total_samples*self.sample_size].reshape((-1, len(activity)))\n",
    "            \n",
    "            return grouped_rows, grouped_activity\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to split Dataset into Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nABIG0rnxl_"
   },
   "outputs": [],
   "source": [
    "def split_to_features_labels(dataset, input_sequance_size=10, total_features=64):\n",
    "    \"\"\"\n",
    "    Function for splitting the data into features(with sequance size=iput_sequance_size)\n",
    "    and labels which should be the remainder of the sample length \n",
    "    \"\"\"\n",
    "    assert input_sequance_size < dataset.shape[1], f\"input sequence should be smaller than the total sample size\"\n",
    "    \n",
    "    #Dividing the dataset into features and labels by splitting the Time Frame Dimension\n",
    "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
    "    labels = dataset[:,np.s_[input_sequance_size:], :total_features]\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5HkmNJJSKg3"
   },
   "source": [
    "### A function for downsampling the dataset on number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wNJmfjiGSDY2"
   },
   "outputs": [],
   "source": [
    "def downsampling(sampled_data, downsample_technique = 'skip'):\n",
    "    \"\"\"\n",
    "    The function used to down-sample the data using two different techniques. In Skip, one frame is skipped consecutively and\n",
    "    in the mean technique, two frames are averaged consecutively.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert downsample_technique in ['skip', 'mean'], f\"Only Skip and Mean techniques are available\"\n",
    "    \n",
    "    #Creating an empty variable to store Downsampled data when the technique is Mean\n",
    "    samples_per_batch = int(sampled_data.shape[1] / 2)\n",
    "    total_features = sampled_data.shape[2]\n",
    "    downsampled_data = np.empty(shape=(0, samples_per_batch, total_features))\n",
    "    \n",
    "    #In Skip technique, we skip 2 frames consecutively.\n",
    "    if downsample_technique == 'skip':\n",
    "        downsampled_data = sampled_data[:,::2,:]\n",
    "    else:\n",
    "        #Iterating over batches\n",
    "        for batch in sampled_data:\n",
    "    \n",
    "            averaged_batch = np.empty(shape=(0, total_features))\n",
    "    \n",
    "            #In each iteration, averaging 2 Frames and appending it to the variable\n",
    "            for i in range(0, batch.shape[0], 2):\n",
    "                averaged_batch = np.append(averaged_batch, np.mean(batch[i:i+2, :], axis = 0).reshape((1, total_features)), axis = 0)\n",
    "            \n",
    "            #Appending the whole batched averaged downsampled data to the new variable created before\n",
    "            downsampled_data = np.append(downsampled_data, averaged_batch.reshape((1, samples_per_batch, total_features)), axis = 0)\n",
    "    \n",
    "    return downsampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLpG7OlQSUjW"
   },
   "source": [
    "### Adding more preprocessing steps (Normalization and gussian noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VzjeqMm1KNjQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(sampled_dataX, sampled_dataY, normalize=True, add_noise=True\n",
    "                    , stddev=0.05) :\n",
    "    \"\"\"\n",
    "    Function to preprocess data by normalizing input features and adding guassian\n",
    "    noise to increase model robustness\n",
    "    \"\"\"  \n",
    "    if normalize :\n",
    "        sampled_dataX =  tf.keras.utils.normalize(sampled_dataX, axis=2)\n",
    "    \n",
    "    if add_noise :\n",
    "        guassian_noise_layer = tf.keras.layers.GaussianNoise(stddev=stddev)\n",
    "        sampled_dataX = guassian_noise_layer(sampled_dataX)\n",
    "    \n",
    "    return sampled_dataX, sampled_dataY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwCrXubfJka4"
   },
   "source": [
    "## Defining different components of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Interpolation Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QsfCMT-0zRTs"
   },
   "outputs": [],
   "source": [
    "class InterpolationLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom interpolation layer extending the keras layer class\n",
    "    it has one attribute num_frames to be interpolated between each two consecutive \n",
    "    timesteps\n",
    "    it has one main function interpolateFrames  \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, num_frames=5):\n",
    "        super(InterpolationLayer, self).__init__()\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def interpolateFrames(self, inputs):\n",
    "        \"\"\"\n",
    "        Takes input tensors of shape(batch_size, timesteps, features)\n",
    "        returns interpolated frames with shape(batch_size, timesteps*num_frames, features)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = inputs.shape[0]\n",
    "        timesteps = inputs.shape[1]\n",
    "        features = inputs.shape[2]\n",
    "        interpolated_frames = tf.zeros([0, features])\n",
    "        \n",
    "        for batch in tf.range(tf.shape(inputs)[0]) :\n",
    "            tf.autograph.experimental.set_loop_options(\n",
    "            shape_invariants=[(interpolated_frames, tf.TensorShape([None, features]))])\n",
    "            for t in range(timesteps) :\n",
    "                for j in range(self.num_frames) :\n",
    "                    X_i0 = inputs[batch, t]\n",
    "                    if(t == timesteps-1) :\n",
    "                        X_i1 = inputs[batch, t]\n",
    "                    else :  \n",
    "                        X_i1 = inputs[batch, t+1]\n",
    "                    alpha_j = j/self.num_frames\n",
    "                    current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
    "                    current_frame = tf.reshape(current_frame, [1, features])\n",
    "                    interpolated_frames = tf.concat((interpolated_frames, current_frame), axis=0)\n",
    "\n",
    "        interpolated_frames = tf.reshape(interpolated_frames,[tf.shape(inputs)[0], (timesteps)*self.num_frames, features])\n",
    "        return interpolated_frames\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.interpolateFrames(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9xiHeHMaQRH"
   },
   "source": [
    "### Trying to create the Keras GlocalNet model through a custom function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "70zZuaX0Y6Ag"
   },
   "outputs": [],
   "source": [
    "def create_glocalNet_model(enocder_hidden_state=200, decoder_hidden_state=200, \n",
    "                 output_diminsion=64, input_diminsions=74, LSTM_dropout=0.25, dense_activation='relu',\n",
    "                 interpolation_frames=5, exclude_locgen=False) :\n",
    "    #Glogen encoder\n",
    "    encoder_inputs = Input(shape=(10, input_diminsions))\n",
    "    encoder = Bidirectional(LSTM(enocder_hidden_state, return_sequences=True, return_state=True))\n",
    "    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    #Glogen decoder\n",
    "    decoder_lstm = Bidirectional(LSTM(decoder_hidden_state, return_sequences=True, return_state=True))\n",
    "    decoder_outputs, _, _ = decoder_lstm(encoder_outputs,\n",
    "                                        initial_state=encoder_states)\n",
    "    decoder_dense = TimeDistributed(Dense(output_diminsion, activation=dense_activation))\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "    #Interpolation layer\n",
    "    interpolation_layer = InterpolationLayer(num_frames=interpolation_frames)\n",
    "    interpolation_output = interpolation_layer(decoder_outputs)\n",
    "    #return the model if execlude_locgen is true\n",
    "    if(exclude_locgen) :\n",
    "        return Model_(encoder_inputs, interpolation_output)\n",
    "    \n",
    "    #Locgen encoder\n",
    "    encoder_locgen = Bidirectional(LSTM(enocder_hidden_state, return_sequences=True, return_state=True))\n",
    "    encoder_outputs_locgen, state_h_locgen, state_c_locgen = encoder_locgen(interpolation_output)\n",
    "    encoder_states_locgen = [state_h_locgen, state_c_locgen]\n",
    "    #Locgen decoder\n",
    "    decoder_lstm_locgen = Bidirectional(LSTM(decoder_hidden_state, return_sequences=True, return_state=True))\n",
    "    decoder_outputs_locgen, _, _ = decoder_lstm(encoder_outputs_locgen,\n",
    "                                        initial_state=encoder_states_locgen)\n",
    "    decoder_dense_locgen = TimeDistributed(Dense(output_diminsion, activation=dense_activation))\n",
    "    glocalNet_output = decoder_dense_locgen(decoder_outputs_locgen)\n",
    "    return Model_(encoder_inputs, glocalNet_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ccVrCxin9KA"
   },
   "source": [
    "### Custom GlocalNet class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7PIJXvLaaEvY"
   },
   "outputs": [],
   "source": [
    "class GlocalNet(Model_):\n",
    "    \"\"\"\n",
    "    A full GlocalNet implementation include the three main stages\n",
    "    Glogen generating initial sparse frames\n",
    "    Interpolation layer generating dense frames from Glogen output\n",
    "    Locgen generating the final output by smoothing the interpolated frames\n",
    "    \"\"\"\n",
    "    def __init__(self, enocder_hidden_state=200, decoder_hidden_state=200, \n",
    "                 output_diminsion=64, LSTM_dropout=0.25, dense_activation='relu',\n",
    "                 interpolation_frames=5, exclude_locgen=False, only_glogen = False,\n",
    "                 include_attention = False):\n",
    "        super(GlocalNet, self).__init__()\n",
    "        \n",
    "        self.exclude_locgen = exclude_locgen\n",
    "        self.only_glogen = only_glogen\n",
    "        self.include_attention = include_attention\n",
    "        \n",
    "        #Glogen layers\n",
    "        self.glogen_encoder = Bidirectional(LSTM(enocder_hidden_state, return_state=True\n",
    "                                   , return_sequences=True, dropout=LSTM_dropout))\n",
    "        self.glogen_decoder = Bidirectional(LSTM(decoder_hidden_state, return_sequences=True,\n",
    "                                   return_state=True, dropout=LSTM_dropout))\n",
    "        #Locgen layers\n",
    "        self.locgen_encoder = Bidirectional(LSTM(enocder_hidden_state, return_sequences=True,\n",
    "                                   return_state=True, dropout=LSTM_dropout))\n",
    "        self.locgen_decoder = Bidirectional(LSTM(decoder_hidden_state, return_sequences=True,\n",
    "                                   return_state=True, dropout=LSTM_dropout))\n",
    "        #Glogen dense layer\n",
    "        self.glogen_dense_layer = TimeDistributed(Dense(output_diminsion,\n",
    "                                                        activation=dense_activation)) \n",
    "        #Interpolation layer\n",
    "        self.interpolation_layer = InterpolationLayer(num_frames=interpolation_frames)\n",
    "        #Locgen dense layer\n",
    "        self.locgen_dense_layer = TimeDistributed(Dense(output_diminsion,\n",
    "                                                        activation=dense_activation)) \n",
    "        \n",
    "        self.attention_layer = tf.keras.layers.Attention()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #Glogen calls      \n",
    "        encoder_outputs, fwd_state_h, fwd_state_c ,back_state_h, back_state_c= self.glogen_encoder(inputs)\n",
    "        encoder_states = [fwd_state_h, fwd_state_c ,back_state_h, back_state_c]\n",
    "        glogen_decoder_input = encoder_outputs\n",
    "        \n",
    "        if self.include_attention:\n",
    "            glogen_decoder_input = self.attention_layer(inputs = [encoder_outputs, encoder_outputs])\n",
    "        \n",
    "        output, _, _,_,_ = self.glogen_decoder(glogen_decoder_input, initial_state=encoder_states)\n",
    "        glogen_output = self.glogen_dense_layer(output)\n",
    "        \n",
    "        if self.only_glogen:\n",
    "            return glogen_output\n",
    "\n",
    "        #Interpolation call\n",
    "        interpolated_frames = self.interpolation_layer(glogen_output)\n",
    "        \n",
    "        if self.exclude_locgen :\n",
    "            return interpolated_frames\n",
    "\n",
    "        #Locgen calls\n",
    "        locgen_encoder_outputs, locgen_fwd_state_h, locgen_fwd_state_c,locgen_back_state_h, locgen_back_state_c = self.locgen_encoder(interpolated_frames)\n",
    "        locgen_encoder_states = [locgen_fwd_state_h, locgen_fwd_state_c,locgen_back_state_h, locgen_back_state_c]\n",
    "        locgen_output, _, _,_,_  = self.locgen_decoder(locgen_encoder_outputs, initial_state=locgen_encoder_states)\n",
    "        final_output = self.locgen_dense_layer(locgen_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionClassifier(Model_):\n",
    "    def __init__(self):\n",
    "        super(ActionClassifier, self).__init__()\n",
    "        \n",
    "        self.conv1 = tf.keras.layers.Conv1D(filters=128, kernel_size=8)\n",
    "        self.conv2 = tf.keras.layers.Conv1D(filters=256, kernel_size=5)\n",
    "        self.conv3 = tf.keras.layers.Conv1D(filters=128, kernel_size=3)\n",
    "        self.denseLayer = tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "        self.batchNormalization1 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchNormalization2 = tf.keras.layers.BatchNormalization()\n",
    "        self.batchNormalization3 = tf.keras.layers.BatchNormalization()\n",
    "        self.globalAveragePooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.relu3 = tf.keras.layers.ReLU()\n",
    "        \n",
    "    def call(self, input):      \n",
    "        output = self.conv1(input)\n",
    "        output = self.batchNormalization1(output)\n",
    "        output = self.relu1(output)\n",
    "        \n",
    "        output = self.conv2(output)\n",
    "        output = self.batchNormalization2(output)\n",
    "        output = self.relu2(output)\n",
    "        \n",
    "        output = self.conv3(output)\n",
    "        output = self.batchNormalization3(output)\n",
    "        output = self.relu3(output)\n",
    "        \n",
    "        output = self.globalAveragePooling(output)\n",
    "        output = self.denseLayer(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining different Types of Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bVUFJsv3fVYh"
   },
   "outputs": [],
   "source": [
    "class Loss() :\n",
    "    \"\"\"\n",
    "    Joint loss class with two weight attributes for two different losses\n",
    "    first one is the loss joint and the second is the loss_motion_flow\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda1=0.5, lambda2=0.5) :\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "\n",
    "    def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        \"\"\"\n",
    "        Loss between the joint positions and its corresponding counterparts in the groundtruth\n",
    "        \"\"\"\n",
    "        diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
    "        return tf.reduce_sum(diff_norm_2, axis=1) \n",
    "\n",
    "    def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        \"\"\"\n",
    "        Loss between the motion flow of predicted sequance and the ground truth\n",
    "        where the motion flow is the euclidean distance between each two consecutive frames\n",
    "        \"\"\"\n",
    "        predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
    "        real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
    "        prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
    "        return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
    "\n",
    "    def total_loss(self, target_sequance_batch, predicted_sequance_batch) :\n",
    "        \"\"\"\n",
    "        calculating the total loss through a combination of the joint_loss and motion_flow_loss\n",
    "        \"\"\"\n",
    "        joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
    "        motion_flow_loss = self.loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
    "        return self.lambda1*joints_loss + self.lambda2*motion_flow_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining different types of Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \"\"\"\n",
    "    A class containing different types of Evaluation Metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mmd_kernel='gaussian') :\n",
    "        self.mmd_kernel = mmd_kernel\n",
    "    \n",
    "    def custom_sequence_MMD_loss(self, target_sequance_batch, predicted_sequance_batch):\n",
    "        \"\"\"\n",
    "        Calculating the Sequence MMD Loss between prediction and the ground Truth.\n",
    "        Additionally combining the last two dimensions \n",
    "        \"\"\"\n",
    "        mmd_loss = MMD.MMDLoss(kernel=self.mmd_kernel)\n",
    "        \n",
    "        total_batches = predicted_sequance_batch.shape[0]\n",
    "        frames_per_batch = predicted_sequance_batch.shape[1] * predicted_sequance_batch.shape[2]\n",
    "        \n",
    "        return mmd_loss(tf.reshape(predicted_sequance_batch, [total_batches, frames_per_batch]),\n",
    "                        tf.reshape(target_sequance_batch, [total_batches, frames_per_batch]))\n",
    "        \n",
    "    def MPJPE2(self, y_true, y_pred, number_of_joints = 32):\n",
    "        \"\"\"\n",
    "        Calculating the Mean Per Joint Position Error (MPJPE) between prediction and the ground Truth.\n",
    "        \"\"\"\n",
    "        yt= y_true.reshape((-1,number_of_joints,2))\n",
    "        yp= y_pred.reshape((-1,number_of_joints,2))\n",
    "        dist= np.zeros(10)\n",
    "        \n",
    "        for i in range(10):\n",
    "            dist[i] = np.linalg.norm(yt[i] - yp[i])\n",
    "        \n",
    "        return np.mean(dist)\n",
    "    \n",
    "    def NPSS(self, euler_gt_sequences, euler_pred_sequences):\n",
    "        \"\"\"\n",
    "        A function to compute the Normalized Power Spectrum Similarity (NPSS) metric between predictions and the ground Truth [2] and [3].\n",
    "        \"\"\"        \n",
    "        # computing 1) fourier coeffs 2)power of fft 3) normalizing power of fft dim-wise 4) cumsum over freq. 5) EMD \n",
    "        gt_fourier_coeffs = np.zeros(euler_gt_sequences.shape, dtype = 'complex_')\n",
    "        pred_fourier_coeffs = np.zeros(euler_pred_sequences.shape, dtype = 'complex_')\n",
    "\n",
    "        # power vars\n",
    "        gt_power = np.zeros((gt_fourier_coeffs.shape))\n",
    "        pred_power = np.zeros((gt_fourier_coeffs.shape))\n",
    "\n",
    "        # normalizing power vars\n",
    "        gt_norm_power = np.zeros(gt_fourier_coeffs.shape)\n",
    "        pred_norm_power = np.zeros(gt_fourier_coeffs.shape)\n",
    "\n",
    "        cdf_gt_power = np.zeros(gt_norm_power.shape)\n",
    "        cdf_pred_power = np.zeros(pred_norm_power.shape)\n",
    "\n",
    "        emd = np.zeros(cdf_pred_power.shape[0:3:2])\n",
    "\n",
    "        # used to store powers of feature_dims and sequences used for avg later\n",
    "        seq_feature_power = np.zeros(euler_gt_sequences.shape[0:3:2])\n",
    "        power_weighted_emd = 0\n",
    "\n",
    "        for s in range(euler_gt_sequences.shape[0]):\n",
    "\n",
    "            for d in range(euler_gt_sequences.shape[2]):\n",
    "                gt_fourier_coeffs[s,:,d] = np.fft.fft(euler_gt_sequences[s,:,d]) # slice is 1D array\n",
    "                pred_fourier_coeffs[s,:,d] = np.fft.fft(euler_pred_sequences[s,:,d])\n",
    "\n",
    "                # computing power of fft per sequence per dim\n",
    "                gt_power[s,:,d] = np.square(np.absolute(gt_fourier_coeffs[s,:,d]))\n",
    "                pred_power[s,:,d] = np.square(np.absolute(pred_fourier_coeffs[s,:,d]))\n",
    "\n",
    "                # matching power of gt and pred sequences\n",
    "                gt_total_power = np.sum(gt_power[s,:,d])\n",
    "                pred_total_power = np.sum(pred_power[s,:,d])\n",
    "                #power_diff = gt_total_power - pred_total_power\n",
    "\n",
    "                # adding power diff to zero freq of pred seq\n",
    "                #pred_power[s,0,d] = pred_power[s,0,d] + power_diff\n",
    "\n",
    "                # computing seq_power and feature_dims power \n",
    "                seq_feature_power[s,d] = gt_total_power\n",
    "\n",
    "                # normalizing power per sequence per dim\n",
    "                if gt_total_power != 0:\n",
    "                    gt_norm_power[s,:,d] = gt_power[s,:,d] / gt_total_power \n",
    "\n",
    "                if pred_total_power !=0:\n",
    "                    pred_norm_power[s,:,d] = pred_power[s,:,d] / pred_total_power\n",
    "\n",
    "                # computing cumsum over freq\n",
    "                cdf_gt_power[s,:,d] = np.cumsum(gt_norm_power[s,:,d]) # slice is 1D\n",
    "                cdf_pred_power[s,:,d] = np.cumsum(pred_norm_power[s,:,d])\n",
    "\n",
    "                # computing EMD \n",
    "                emd[s,d] = np.linalg.norm((cdf_pred_power[s,:,d] - cdf_gt_power[s,:,d]), ord=1)\n",
    "\n",
    "        # computing weighted emd (by sequence and feature powers)\n",
    "        power_weighted_emd = np.average(emd, weights=seq_feature_power) \n",
    "\n",
    "        return power_weighted_emd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to start the experiment of training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "labA65KEakkZ"
   },
   "outputs": [],
   "source": [
    "def run_experiment(sampled_dataX, sampled_dataY, learning_rate=0.002, lambda1=0.5,\n",
    "                   lambda2=0.5, use_mse=False, use_MMD=False, metrics=None, output_diminsion=64,\n",
    "                   batch_size=100, epochs=50, validation_split=0.2, activation=\"relu\",\n",
    "                   dropout=0.25, exclude_locgen=False, interpolate_frames = 5, only_glogen = False,\n",
    "                   include_attention = False) :\n",
    "    \"\"\"\n",
    "    Method takes all hyperparameters as input paramters and returns the model and history as\n",
    "    a result\n",
    "    \"\"\"\n",
    "    glocal_model = GlocalNet(dense_activation=activation, LSTM_dropout=dropout,\n",
    "                             exclude_locgen=exclude_locgen, only_glogen=only_glogen,\n",
    "                             output_diminsion=output_diminsion, \n",
    "                             interpolation_frames = interpolate_frames,\n",
    "                             include_attention=include_attention)\n",
    "    if use_mse :\n",
    "        loss_function = tf.keras.losses.mean_squared_error\n",
    "    elif use_MMD :\n",
    "        loss_function = Loss().custom_sequence_MMD_loss\n",
    "    else :\n",
    "        loss_function = Loss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
    "\n",
    "    glocal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), \n",
    "                         loss=loss_function, metrics=metrics, run_eagerly=False)\n",
    "    \n",
    "    history = glocal_model.fit(sampled_dataX, sampled_dataY, batch_size=batch_size, \n",
    "                               epochs=epochs, validation_split=validation_split)\n",
    "    \n",
    "    return history, glocal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to Resume the experiment of training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eq3GlZ4NvHHz"
   },
   "outputs": [],
   "source": [
    "def resume_training(glocal_model, sampled_dataX, sampled_dataY, learning_rate=0.002, lambda1=0.5,\n",
    "                   lambda2=0.5, use_mse=False, use_MMD=False, metrics=None,\n",
    "                   batch_size=100, epochs=50, validation_split=0.2) :\n",
    "    \"\"\"\n",
    "    function to resume training of a model\n",
    "    \"\"\"\n",
    "    if use_mse :\n",
    "        loss_function = tf.keras.losses.mean_squared_error\n",
    "    elif use_MMD :\n",
    "        loss_function = Loss().custom_sequence_MMD_loss\n",
    "    else :\n",
    "        loss_function = Loss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
    "\n",
    "    glocal_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                      loss=loss_function, metrics=metrics)\n",
    "    \n",
    "    history = glocal_model.fit(sampled_dataX, sampled_dataY,\n",
    "                            batch_size=batch_size,\n",
    "                            epochs=epochs, validation_split=validation_split)\n",
    "    \n",
    "    return history, glocal_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to Train Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_action_classifier(data, label, \n",
    "                            optimizer = tf.keras.optimizers.Adam(0.001), \n",
    "                            loss = tf.keras.losses.CategoricalCrossentropy(), \n",
    "                            metrics = [tf.keras.metrics.CategoricalAccuracy()], \n",
    "                            epochs = 500, batch_size = 32, validation_split = 0.2):\n",
    "    \"\"\"\n",
    "    The function trains the Action Classifier and evaluate it on the dataset.\n",
    "    \"\"\"\n",
    "    print('Starting to Train Action Classifier...')\n",
    "    \n",
    "    #Initializing the action classifier model\n",
    "    action_classifer = ActionClassifier()\n",
    "    \n",
    "    #Compiling the model\n",
    "    action_classifer.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    \n",
    "    #Fitting the model on the dataset\n",
    "    action_classifer.fit(data, label, epochs=epochs, batch_size=batch_size, verbose = 1, \n",
    "                         validation_split=validation_split, \n",
    "                         callbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=20, min_lr=0.0001)])\n",
    "    \n",
    "    print('\\nTraining Complete!\\n')\n",
    "    \n",
    "    #Evaluating the action classifier\n",
    "    score = action_classifer.evaluate(data, label)\n",
    "    print('Action Classifier Evaluation:\\nLoss: {}\\nAccuracy: {}'.format(score[0], score[1]))\n",
    "    \n",
    "    #Returning the trained action classifier\n",
    "    return action_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to visualize certain frames from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(lenght = 10, path_to_save = ''):\n",
    "    \"\"\"\n",
    "    Function to create and save a GIF from different number of frames\n",
    "    \"\"\"\n",
    "    list=[]\n",
    "    for l in range(lenght):\n",
    "        list.append(f'{path_to_save}_frame{l}.png')\n",
    "\n",
    "    with imageio.get_writer(f'{path_to_save}.gif', mode='I', duration=0.1) as writer:\n",
    "        for filename in list:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIHR8VGhPfHf"
   },
   "outputs": [],
   "source": [
    "def visualize_frames(sample, dynamic_joints_only=False, num_frames_to_visualize=10, \n",
    "                    path_to_save=\"\", save_gif=False,\n",
    "                    joints_to_ignore=[4,5,9,10,11,16,20,21,22,23,24,28,29,30,31]) :\n",
    "    \"\"\"\n",
    "    Visualization function to draw a certain number of frames in a given sample\n",
    "    ignoring the joints mentioned in joints_to_ignore array\n",
    "    \"\"\"\n",
    "    assert num_frames_to_visualize <= sample.shape[0], f\"number of frames should be less than or equal to the total frames in the sample\"\n",
    "    \n",
    "    fig, axs = plt.subplots(ncols=num_frames_to_visualize, figsize=(40, 10))\n",
    "    fig.tight_layout(pad=1.0)\n",
    "    \n",
    "    for t in range(num_frames_to_visualize) :\n",
    "        #Removing unnecessary joints for visualization\n",
    "        if(dynamic_joints_only) :\n",
    "            #Check if no joints needs to be removed\n",
    "            truncated_frame = sample[t]\n",
    "        else :\n",
    "            #Removing the joints based on joints_to_ignore\n",
    "            joints_to_ignore_2d = [element * 2 for element in joints_to_ignore]\n",
    "            for i in range(len(joints_to_ignore_2d)) :\n",
    "                joints_to_ignore_2d.append(joints_to_ignore_2d[i]+1)\n",
    "            truncated_frame = np.delete(sample[t], joints_to_ignore_2d)   \n",
    "\n",
    "        #In case of including only moving joints for Human3.6M(17 joints)      \n",
    "        x_axis_array = truncated_frame[0:34:2]\n",
    "        y_axis_array = truncated_frame[1:35:2]\n",
    "        #Scattering all the 17 joints\n",
    "        axs[t].scatter(x_axis_array, y_axis_array)\n",
    "        #Plotting right leg\n",
    "        axs[t].plot(x_axis_array[:4], y_axis_array[:4], \"tab:blue\")\n",
    "        #plotting left leg\n",
    "        axs[t].plot(x_axis_array[[0, 4, 5, 6]], y_axis_array[[0, 4, 5, 6]])\n",
    "        #plotting from hip to head\n",
    "        axs[t].plot(x_axis_array[[0, 7, 8, 9, 10]], y_axis_array[[0, 7, 8, 9, 10]])\n",
    "        #plotting from neck to left shoulder\n",
    "        axs[t].plot(x_axis_array[[9, 11, 12, 13]], y_axis_array[[9, 11, 12, 13]])\n",
    "        #plotting from neck to right shoulder\n",
    "        axs[t].plot(x_axis_array[[9, 14, 15, 16]], y_axis_array[[9, 14, 15, 16]])\n",
    "        axs[t].invert_yaxis()\n",
    "        axs[t].set_xticks([])\n",
    "        axs[t].set_yticks([])\n",
    "        \n",
    "        if(len(path_to_save) > 0) :\n",
    "            extent = axs[t].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "            fig.savefig(f'{path_to_save}_frame{t}.png', bbox_inches=extent)\n",
    "    \n",
    "    if save_gif==True:\n",
    "        create_gif(lenght=num_frames_to_visualize,path_to_save=path_to_save)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Baseline Model for short term predictions (10 frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicRNN(Model_):\n",
    "    \"\"\"\n",
    "    simple many to many RNN model\n",
    "    \"\"\"\n",
    "    def __init__(self, output_diminsions=64, RNN_type=\"classical\", activation=\"relu\"):\n",
    "        super(BasicRNN, self).__init__()\n",
    "        possible_RNN_types = [\"classical\", \"LSTM\", \"GRU\"]\n",
    "        assert RNN_type in possible_RNN_types, f\"RNN_type should be one of the valid values ['classical', 'LSTM', 'GRU']\"\n",
    "        if(RNN_type == \"classical\") :\n",
    "            self.ruccernt_layer = SimpleRNN(output_diminsions, return_sequences=True)\n",
    "        elif(RNN_type == \"LSTM\") : \n",
    "            self.ruccernt_layer = LSTM(output_diminsions, return_sequences=True)\n",
    "        else :\n",
    "            self.ruccernt_layer = GRU(output_diminsions, return_sequences=True)\n",
    "        self.dense_layer = TimeDistributed(Dense(output_diminsions,\n",
    "                                                        activation=activation)) \n",
    "\n",
    "    def call(self, input) :\n",
    "        output = self.ruccernt_layer(input)\n",
    "        output = self.dense_layer(output)\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to save Custom Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveGlocalNetWeights(model, file_path) :\n",
    "    \"\"\"\n",
    "    A function to save all layers weights except for interpolation layer\n",
    "    \"\"\"\n",
    "    model_layers = np.array([], dtype=object)\n",
    "    for layer in model.layers :\n",
    "        model_layers = np.append(model_layers, layer.get_weights())\n",
    "    np.save(file_path, model_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createGlocalNetFromFile(model : GlocalNet, model_weights_file, sample_input, output_diminsions=64,\n",
    "interpolation_frames=2, dense_activation=\"relu\") :\n",
    "    \"\"\"\n",
    "    function use presaved weights from file_path(model_weights_file) and uses sample_input to \n",
    "    build the model\n",
    "    \"\"\"\n",
    "    #Create an empty glocalNet model\n",
    "    model = GlocalNet(exclude_locgen=model.exclude_locgen, output_diminsion=output_diminsions,\n",
    "     interpolation_frames=interpolation_frames, dense_activation=dense_activation)\n",
    "    model_weights = np.load(model_weights_file, allow_pickle=True)\n",
    "    model(sample_input)\n",
    "    if(model.exclude_locgen) :\n",
    "        #Setting Glogen layers\n",
    "        layer0_weights = model_weights[:3]\n",
    "        layer1_weights = model_weights[3:6]\n",
    "        layer4_weights = model_weights[6:]\n",
    "        #Setting the Glogen layers\n",
    "        model.layers[0].set_weights(layer0_weights)\n",
    "        model.layers[1].set_weights(layer1_weights)\n",
    "        model.layers[4].set_weights(layer4_weights)\n",
    "    else :    \n",
    "        #Setting Glogen layers\n",
    "        layer0_weights = model_weights[:3]\n",
    "        layer1_weights = model_weights[3:6]\n",
    "        layer4_weights = model_weights[12:14]\n",
    "        #Setting Locgen layers\n",
    "        layer2_weights = model_weights[6:9]\n",
    "        layer3_weights = model_weights[9:12]\n",
    "        layer6_weights = model_weights[14:]\n",
    "            #Building the model to be able to set the layers' weights\n",
    "        model(sample_input)\n",
    "        #Setting the Glogen layers\n",
    "        model.layers[0].set_weights(layer0_weights)\n",
    "        model.layers[1].set_weights(layer1_weights)\n",
    "        model.layers[4].set_weights(layer4_weights)\n",
    "        #Setting locgen layers\n",
    "        model.layers[2].set_weights(layer2_weights)\n",
    "        model.layers[3].set_weights(layer3_weights)\n",
    "        model.layers[6].set_weights(layer6_weights)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the whole dataset for shortterm predictions\n",
    "sampled_data_short_term = Dataset_loading('./H3.6csv', sample_size=20, include_movable_joints=False).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading only the movable dataset points for shortterm predictions\n",
    "sampled_data_short_term_movable = Dataset_loading('./H3.6csv', sample_size=20, include_movable_joints=True).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f8PgK1UVnkrw"
   },
   "outputs": [],
   "source": [
    "#For long term prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
    "sampled_data_all = Dataset_loading('./H3.6csv', sample_size=60, include_movable_joints=False).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For long term movable joints prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
    "sampled_data_movable = Dataset_loading('./H3.6csv', sample_size=60, include_movable_joints=True).read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset for Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_classifier, sampled_labels_classifier = Dataset_loading('./H3.6csv', sample_size=60, include_movable_joints=False, \n",
    "                                                                     include_action_labels=False, return_action_labels=True).read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6bSQTwdn2Fo"
   },
   "outputs": [],
   "source": [
    "sampled_dataX_all, sampled_dataY_all = split_to_features_labels(sampled_data_all, input_sequance_size=10, total_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_movable, sampled_dataY_movable = split_to_features_labels(sampled_data_movable, input_sequance_size=10, total_features=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_short_term, sampled_dataY_short_term = split_to_features_labels(sampled_data_short_term, input_sequance_size=10, total_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_short_term_movable, sampled_dataY_short_term_movable = split_to_features_labels(sampled_data_short_term_movable, input_sequance_size=10, total_features=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset with all features(short term)')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_short_term.shape[0],\n",
    "                                                                       sampled_dataY_short_term.shape[1],\n",
    "                                                                       sampled_dataY_short_term.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset with Movable features(short term)')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_short_term_movable.shape[0],\n",
    "                                                                       sampled_dataY_short_term_movable.shape[1],\n",
    "                                                                       sampled_dataY_short_term_movable.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVXQiscVn4O2",
    "outputId": "dffaa998-a452-4358-d205-9754186aba0b"
   },
   "outputs": [],
   "source": [
    "print('Dataset with all features')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_all.shape[0],\n",
    "                                                                       sampled_dataY_all.shape[1],\n",
    "                                                                       sampled_dataY_all.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dataset with only movable features')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_movable.shape[0],\n",
    "                                                                       sampled_dataY_movable.shape[1],\n",
    "                                                                       sampled_dataY_movable.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snGEPtNOKLvg"
   },
   "source": [
    "### Adding Noise and Downsampling to improve model performance and robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only Preprocessed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bR0E_RfMlHg"
   },
   "outputs": [],
   "source": [
    "preprocessed_sampled_dataX_all, preprocessed_sampled_dataY_all = preprocess_data(sampled_dataX_all, sampled_dataY_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sampled_dataX_movable, preprocessed_sampled_dataY_movable = preprocess_data(sampled_dataX_movable, sampled_dataY_movable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sampled_dataX_shortterm_movable, preprocessed_sampled_dataY_shortterm_movable = preprocess_data(sampled_dataX_short_term_movable, sampled_dataY_short_term_movable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed and Downsampled Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9Bkt02CSh5P"
   },
   "outputs": [],
   "source": [
    "downsampled_data_all = downsampling(sampled_data_all, 'skip')\n",
    "downsampled_data_movable = downsampling(sampled_data_movable, 'skip')\n",
    "downsampled_data_shortterm_movable = downsampling(sampled_data_short_term_movable, 'skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsR__kGeSmNG"
   },
   "outputs": [],
   "source": [
    "downsampled_dataX_all, downsampled_dataY_all = split_to_features_labels(downsampled_data_all, input_sequance_size=10, total_features=64)\n",
    "preprocessed_downsampled_dataX_all, preprocessed_downsampled_dataY_all = preprocess_data(downsampled_dataX_all, downsampled_dataY_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataX_movable, downsampled_dataY_movable = split_to_features_labels(downsampled_data_movable, input_sequance_size=10, total_features=34)\n",
    "preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable = preprocess_data(downsampled_dataX_movable, downsampled_dataY_movable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataX_shortterm_movable, downsampled_dataX_shortterm_movable = split_to_features_labels(downsampled_data_shortterm_movable, input_sequance_size=5, total_features=34)\n",
    "preprocessed_downsampled_dataX_shortterm_movable, preprocessed_downsampled_dataY_shortterm_movable = preprocess_data(downsampled_dataX_shortterm_movable, downsampled_dataX_shortterm_movable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_classifer = train_action_classifier(sampled_data_classifier, sampled_labels_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az25davkJq9Y"
   },
   "source": [
    "### Running experiments with different hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimentations using Simple Multivariate Time Series Models (Simple RNN, LSTM, GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple LSTM model with Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN1 = BasicRNN(RNN_type=\"LSTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN1, simple_RNN1 = resume_training(simple_RNN1, sampled_dataX_short_term, sampled_dataY_short_term, epochs=10, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN1_pred = simple_RNN1.predict(sampled_dataX_short_term)\n",
    "rnn1_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN1_pred)\n",
    "rnn1_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN1_pred)\n",
    "\n",
    "print('Simple LSTM Model with RELU activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn1_npss, rnn1_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple GRU model with Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN2 = BasicRNN(RNN_type=\"GRU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN2, simple_RNN2 = resume_training(simple_RNN2, sampled_dataX_short_term, sampled_dataY_short_term, epochs=10, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN2_pred = simple_RNN2.predict(sampled_dataX_short_term)\n",
    "rnn2_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN2_pred)\n",
    "rnn2_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN2_pred)\n",
    "\n",
    "print('Simple GRU Model with RELU activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn2_npss, rnn2_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple RNN model with Relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN3 = BasicRNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN3, simple_RNN3 = resume_training(simple_RNN3, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN3_pred = simple_RNN3.predict(sampled_dataX_short_term)\n",
    "rnn3_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN3_pred)\n",
    "rnn3_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN3_pred)\n",
    "\n",
    "print('Simple RNN Model with RELU activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn3_npss, rnn3_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple RNN model with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN4 = BasicRNN(activation=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN4, simple_RNN4 = resume_training(simple_RNN4, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN4_pred = simple_RNN4.predict(sampled_dataX_short_term)\n",
    "rnn4_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN4_pred)\n",
    "rnn4_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN4_pred)\n",
    "\n",
    "print('Simple RNN Model with Linear activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn4_npss, rnn4_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple LSTM model with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN5 = BasicRNN(RNN_type=\"LSTM\", activation=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN5, simple_RNN5 = resume_training(simple_RNN5, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN5_pred = simple_RNN5.predict(sampled_dataX_short_term)\n",
    "rnn5_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN5_pred)\n",
    "rnn5_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN5_pred)\n",
    "\n",
    "print('Simple LSTM Model with Linear activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn5_npss, rnn5_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running simple GRU model with linear activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN6 = BasicRNN(RNN_type=\"GRU\", activation=\"linear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_simple_RNN6, simple_RNN6 = resume_training(simple_RNN6, sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                                  metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN6_pred = simple_RNN6.predict(sampled_dataX_short_term)\n",
    "rnn6_npss = Metrics().NPSS(sampled_dataY_short_term, simple_RNN6_pred)\n",
    "rnn6_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, simple_RNN6_pred)\n",
    "\n",
    "print('Simple GRU Model with Linear activation:\\nNPSS: {}\\nMPJPE: {}'.format(rnn6_npss, rnn6_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GlocalNet Base Implementation Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTHffB1KrfBy"
   },
   "source": [
    "##### Experiment Running with MSE Loss and MMD metric with Guassian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model1, glocal_model1 = run_experiment(sampled_dataX_all, sampled_dataY_all, epochs=50,\n",
    "                                                 batch_size=20, use_mse=True, validation_split=0.0, metrics=[tf.keras.losses.mean_absolute_percentage_error,\n",
    "                                                 Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model1_pred = glocal_model1.predict(sampled_dataX_all)\n",
    "glocal_model1_npss = Metrics().NPSS(sampled_dataY_all, glocal_model1_pred)\n",
    "glocal_model1_mpjpe = Metrics().MPJPE2(sampled_dataY_all, glocal_model1_pred)\n",
    "\n",
    "print('Model 1:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model1_npss, glocal_model1_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Running with Joint Loss and MMD metric with Guassian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7FQ-HQ4bPVi",
    "outputId": "89e882cb-f5fd-4d5f-d5af-1497efaba172"
   },
   "outputs": [],
   "source": [
    "history_model2, glocal_model2 = run_experiment(sampled_dataX_all, sampled_dataY_all, epochs=50, batch_size=20, lambda1=0.5, lambda2=0.5, \n",
    "                                               validation_split=0.0, metrics=[tf.keras.losses.mean_absolute_percentage_error,\n",
    "                                                 Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model2_pred = glocal_model2.predict(sampled_dataX_all)\n",
    "glocal_model2_npss = Metrics().NPSS(sampled_dataY_all, glocal_model2_pred)\n",
    "glocal_model2_mpjpe = Metrics().MPJPE2(sampled_dataY_all, glocal_model2_pred)\n",
    "\n",
    "print('Model 2:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model2_npss, glocal_model2_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Running with MSE and MMD metric with Laplacian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wKSm6QMuGuc",
    "outputId": "e4be73cf-1c86-4ec8-d4d1-36b52558e78f"
   },
   "outputs": [],
   "source": [
    "history_model3, glocal_model3 = run_experiment(sampled_dataX_all, sampled_dataY_all, epochs=50, batch_size=20, use_mse=True, validation_split=0.0,\n",
    "                                               metrics=[tf.keras.losses.mean_absolute_percentage_error, \n",
    "                                                        Metrics(mmd_kernel=\"laplacian\").custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model3_pred = glocal_model3.predict(sampled_dataX_all)\n",
    "glocal_model3_npss = Metrics().NPSS(sampled_dataY_all, glocal_model3_pred)\n",
    "glocal_model3_mpjpe = Metrics().MPJPE2(sampled_dataY_all, glocal_model3_pred)\n",
    "\n",
    "print('Model 3:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model3_npss, glocal_model3_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Running with Joint Loss and MMD metric with Laplacian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model4, glocal_model4 = run_experiment(sampled_dataX_all, sampled_dataY_all, epochs=50, batch_size=20, lambda1=0.5, lambda2=0.5, \n",
    "                                               validation_split=0.0, metrics=[tf.keras.losses.mean_absolute_percentage_error, \n",
    "                                                                              Metrics(mmd_kernel=\"laplacian\").custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model4_pred = glocal_model4.predict(sampled_dataX_all)\n",
    "glocal_model4_npss = Metrics().NPSS(sampled_dataY_all, glocal_model4_pred)\n",
    "glocal_model4_mpjpe = Metrics().MPJPE2(sampled_dataY_all, glocal_model4_pred)\n",
    "\n",
    "print('Model 4:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model4_npss, glocal_model4_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experiment Running with MSE, MMD metric and without Locgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model5, glocal_model5 = run_experiment(sampled_dataX_all, sampled_dataY_all, epochs=10, batch_size=20, use_mse=True, validation_split=0.0, \n",
    "                                               exclude_locgen=True, metrics=[tf.keras.losses.mean_absolute_percentage_error, \n",
    "                                                                             Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model5_pred = glocal_model5.predict(sampled_dataX_all)\n",
    "glocal_model5_npss = Metrics().NPSS(sampled_dataY_all, glocal_model5_pred)\n",
    "glocal_model5_mpjpe = Metrics().MPJPE2(sampled_dataY_all, glocal_model5_pred)\n",
    "\n",
    "print('Model 5:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model5_npss, glocal_model5_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advancements in the Base Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQg05uuoRtEi"
   },
   "source": [
    "##### Running Experiment with whole Preprocessed Data and without Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "On6oHsxuRxFS",
    "outputId": "649b7b9d-9b2f-449e-f315-57fa90653182"
   },
   "outputs": [],
   "source": [
    "history_model6, glocal_model6 = run_experiment(preprocessed_sampled_dataX_all, preprocessed_sampled_dataY_all, batch_size=20, dropout=0.0,\n",
    "                                               epochs=50, use_mse=True, validation_split=0.0, metrics=[Metrics().custom_sequence_MMD_loss, \n",
    "                                                                                                       tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model6_pred = glocal_model6.predict(preprocessed_sampled_dataX_all)\n",
    "glocal_model6_npss = Metrics().NPSS(preprocessed_sampled_dataY_all, glocal_model6_pred)\n",
    "glocal_model6_mpjpe = Metrics().MPJPE2(preprocessed_sampled_dataY_all, glocal_model6_pred)\n",
    "\n",
    "print('Model 6:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model6_npss, glocal_model6_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with whole Preprocessed Data and without Downsampling and Locgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDY6p981x_s3",
    "outputId": "39c3be43-5832-48f4-fdc5-f50d04b2ceff"
   },
   "outputs": [],
   "source": [
    "history_model7, glocal_model7 = run_experiment(preprocessed_sampled_dataX_all, preprocessed_sampled_dataY_all, batch_size=20, dropout=0.0, \n",
    "                                               exclude_locgen=True, epochs=50, use_mse=True, validation_split=0.0,\n",
    "                                               metrics=[Metrics().custom_sequence_MMD_loss, tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model7_pred = glocal_model7.predict(preprocessed_sampled_dataX_all)\n",
    "glocal_model7_npss = Metrics().NPSS(preprocessed_sampled_dataY_all, glocal_model7_pred)\n",
    "glocal_model7_mpjpe = Metrics().MPJPE2(preprocessed_sampled_dataY_all, glocal_model7_pred)\n",
    "\n",
    "print('Model 7:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model7_npss, glocal_model7_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with Preprocessed movable Datapoints and without Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model8, glocal_model8 = run_experiment(preprocessed_sampled_dataX_movable, preprocessed_sampled_dataY_movable, batch_size=20, dropout=0.0,\n",
    "                                               epochs=50, use_mse=True, validation_split=0.0,\n",
    "                                               metrics=[Metrics().custom_sequence_MMD_loss, tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model8_pred = glocal_model8.predict(preprocessed_sampled_dataX_movable)\n",
    "glocal_model8_npss = Metrics().NPSS(preprocessed_sampled_dataY_movable, glocal_model8_pred)\n",
    "glocal_model8_mpjpe = Metrics().MPJPE2(preprocessed_sampled_dataY_movable, glocal_model8_pred)\n",
    "\n",
    "print('Model 8:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model8_npss, glocal_model8_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with Preprocessed movable Datapoints and without Downsampling and Locgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model9, glocal_model9 = run_experiment(preprocessed_sampled_dataX_movable, preprocessed_sampled_dataY_movable, batch_size=20, dropout=0.0, \n",
    "                                               exclude_locgen=True, epochs=50, use_mse=True, validation_split=0.0, output_diminsion=34,\n",
    "                                               metrics=[Metrics().custom_sequence_MMD_loss, tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model9_pred = glocal_model9.predict(preprocessed_sampled_dataX_movable)\n",
    "glocal_model9_npss = Metrics().NPSS(preprocessed_sampled_dataY_movable, glocal_model9_pred)\n",
    "glocal_model9_mpjpe = Metrics().MPJPE2(preprocessed_sampled_dataY_movable, glocal_model9_pred)\n",
    "\n",
    "print('Model 9:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model9_npss, glocal_model9_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3usu4ibvUV2t"
   },
   "source": [
    "##### Running Experiment with whole Downsampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K8uq_0kfTUDp",
    "outputId": "b27fc7b6-922a-43e4-b45d-c99a1c289bd1"
   },
   "outputs": [],
   "source": [
    "history_model10, glocal_model10 = run_experiment(preprocessed_downsampled_dataX_all, preprocessed_downsampled_dataY_all, batch_size=20, dropout=0.0,\n",
    "                                                 epochs=50, use_mse=True, output_diminsion=64, validation_split=0.0, interpolate_frames = 2,\n",
    "                                                 metrics=[Metrics().custom_sequence_MMD_loss, tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model10_pred = glocal_model10.predict(preprocessed_downsampled_dataX_all)\n",
    "glocal_model10_npss = Metrics().NPSS(preprocessed_downsampled_dataY_all, glocal_model10_pred)\n",
    "glocal_model10_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_all, glocal_model10_pred)\n",
    "\n",
    "print('Model 10:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model10_npss, glocal_model10_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with whole Downsampled data and without Locgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OvcEDqawr6dS",
    "outputId": "bb35a0d1-79e8-4322-d83f-8e534e7ed99e"
   },
   "outputs": [],
   "source": [
    "history_model11, glocal_model11 = run_experiment(preprocessed_downsampled_dataX_all, preprocessed_downsampled_dataY_all, batch_size=20, dropout=0.0, \n",
    "                                                 exclude_locgen=True, epochs=50, use_mse=True, output_diminsion=64, validation_split=0.0, \n",
    "                                                 interpolate_frames = 2, metrics=[Metrics().custom_sequence_MMD_loss, \n",
    "                                                                                 tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model11_pred = glocal_model11.predict(preprocessed_downsampled_dataX_all)\n",
    "glocal_model11_npss = Metrics().NPSS(preprocessed_downsampled_dataY_all, glocal_model11_pred)\n",
    "glocal_model11_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_all, glocal_model11_pred)\n",
    "\n",
    "print('Model 11:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model11_npss, glocal_model11_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with Downsampled movable datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model12, glocal_model12 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, \n",
    "                                                 dropout=0.0, epochs=50, use_mse=True, output_diminsion=34, validation_split=0.0, \n",
    "                                                 interpolate_frames = 2, metrics=[Metrics().custom_sequence_MMD_loss, \n",
    "                                                                                  tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model12_pred = glocal_model12.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model12_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model12_pred)\n",
    "glocal_model12_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model12_pred)\n",
    "\n",
    "print('Model 12:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model12_npss, glocal_model12_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with Downsampled movable datapoints and without Locgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model13, glocal_model13 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, \n",
    "                                                 dropout=0.0, exclude_locgen=True, epochs=50, use_mse=True, output_diminsion=34,validation_split=0.0, \n",
    "                                                 interpolate_frames = 2, metrics=[Metrics().custom_sequence_MMD_loss, \n",
    "                                                                                  tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model13_pred = glocal_model13.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model13_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model13_pred)\n",
    "glocal_model13_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model13_pred)\n",
    "\n",
    "print('Model 13:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model13_npss, glocal_model13_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment with ONLY Glogen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model14, glocal_model14 = run_experiment(sampled_dataX_short_term, sampled_dataY_short_term, epochs=50, only_glogen=True,\n",
    "                                                 batch_size=20, use_mse=True, validation_split=0.0, metrics=[tf.keras.losses.mean_absolute_percentage_error,\n",
    "                                                 Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model14_pred = glocal_model14.predict(sampled_dataX_short_term)\n",
    "glocal_model14_npss = Metrics().NPSS(sampled_dataY_short_term, glocal_model14_pred)\n",
    "glocal_model14_mpjpe = Metrics().MPJPE2(sampled_dataY_short_term, glocal_model14_pred)\n",
    "\n",
    "print('Model 14:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model14_npss, glocal_model14_mpjpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model15, glocal_model15 = run_experiment(preprocessed_sampled_dataX_shortterm_movable, preprocessed_sampled_dataY_shortterm_movable, epochs=50, only_glogen=True,\n",
    "                                                 batch_size=20, output_diminsion=34, interpolate_frames=2, use_mse=True, validation_split=0.0, \n",
    "                                                 metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model15_pred = glocal_model15.predict(preprocessed_sampled_dataX_shortterm_movable)\n",
    "glocal_model15_npss = Metrics().NPSS(preprocessed_sampled_dataY_shortterm_movable, glocal_model15_pred)\n",
    "glocal_model15_mpjpe = Metrics().MPJPE2(preprocessed_sampled_dataY_shortterm_movable, glocal_model15_pred)\n",
    "\n",
    "print('Model 15:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model15_npss, glocal_model15_mpjpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model16, glocal_model16 = run_experiment(preprocessed_downsampled_dataX_shortterm_movable, preprocessed_downsampled_dataY_shortterm_movable, epochs=50, only_glogen=True,\n",
    "                                                 batch_size=20, output_diminsion=34, interpolate_frames=2, use_mse=True, validation_split=0.0, \n",
    "                                                 metrics=[tf.keras.losses.mean_absolute_percentage_error, Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model16_pred = glocal_model16.predict(preprocessed_downsampled_dataX_shortterm_movable)\n",
    "glocal_model16_npss = Metrics().NPSS(preprocessed_downsampled_dataY_shortterm_movable, glocal_model16_pred)\n",
    "glocal_model16_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_shortterm_movable, glocal_model16_pred)\n",
    "\n",
    "print('Model 16:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model16_npss, glocal_model16_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Experimentations with Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model17, glocal_model17 = run_experiment(sampled_dataX_all, sampled_dataY_all, epochs=50, include_attention=True,\n",
    "                                                 batch_size=20, use_mse=True, validation_split=0.0, metrics=[tf.keras.losses.mean_absolute_percentage_error,\n",
    "                                                 Metrics().custom_sequence_MMD_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model17_pred = glocal_model17.predict(sampled_dataX_all)\n",
    "glocal_model17_npss = Metrics().NPSS(sampled_dataY_all, glocal_model17_pred)\n",
    "glocal_model17_mpjpe = Metrics().MPJPE2(sampled_dataY_all, glocal_model17_pred)\n",
    "\n",
    "print('Model 17:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model17_npss, glocal_model17_mpjpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model18, glocal_model18 = run_experiment(preprocessed_downsampled_dataX_all, preprocessed_downsampled_dataY_all, batch_size=20, dropout=0.0,\n",
    "                                                 epochs=50, use_mse=True, output_diminsion=64, validation_split=0.0, interpolate_frames = 2, include_attention=True,\n",
    "                                                 metrics=[Metrics().custom_sequence_MMD_loss, tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model18_pred = glocal_model18.predict(preprocessed_downsampled_dataX_all)\n",
    "glocal_model18_npss = Metrics().NPSS(preprocessed_downsampled_dataY_all, glocal_model18_pred)\n",
    "glocal_model18_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_all, glocal_model18_pred)\n",
    "\n",
    "print('Model 18:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model18_npss, glocal_model18_mpjpe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_model19, glocal_model19 = run_experiment(preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable, batch_size=20, \n",
    "                                                 dropout=0.0, epochs=50, use_mse=True, output_diminsion=34, validation_split=0.0, include_attention=True,\n",
    "                                                 interpolate_frames = 2, metrics=[Metrics().custom_sequence_MMD_loss, \n",
    "                                                                                  tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glocal_model19_pred = glocal_model18.predict(preprocessed_downsampled_dataX_movable)\n",
    "glocal_model19_npss = Metrics().NPSS(preprocessed_downsampled_dataY_movable, glocal_model19_pred)\n",
    "glocal_model19_mpjpe = Metrics().MPJPE2(preprocessed_downsampled_dataY_movable, glocal_model19_pred)\n",
    "\n",
    "print('Model 19:\\nNPSS: {}\\nMPJPE: {}'.format(glocal_model19_npss, glocal_model19_mpjpe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import lineStyles\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "plt.plot(history_simple_RNN6.history['loss'][:10], linestyle='--', label='GRU with Linear Activation')\n",
    "\n",
    "plt.plot(history_model14.history['loss'][:10], label='Glogen with All Joints Without Preprocessing')\n",
    "plt.plot(history_model15.history['loss'][:10], label='GLogen with All Joints With Preprocessing')\n",
    "plt.plot(history_model16.history['loss'][:10], label='Glogen with Movable Joints With Preprocessing')\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XaBTxF3Pbmo"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frames(sampled_dataY_short_term[50000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWZSPjKM9u-z"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] https://github.com/una-dinosauria/3d-pose-baseline/blob/master/src/data_utils.py\n",
    "\n",
    "[2] Gopalakrishnan, Anand, et al. \"A neural temporal model for human motion prediction.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019\n",
    "\n",
    "[3] https://github.com/cr7anand/neural_temporal_models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KTHffB1KrfBy",
    "H-ekHZVlrq1o",
    "hQg05uuoRtEi",
    "nozip8IVhhkg"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
