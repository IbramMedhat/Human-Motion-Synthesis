{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QImRR28hJtIK",
    "outputId": "909a64cb-9575-4854-c6a4-e51ad456d692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Model as Model_\n",
    "from tensorflow.keras.layers import Input, PReLU, LeakyReLU, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, Conv2D, Flatten, Reshape, Conv1D, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Dense, BatchNormalization\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "5xx23v8EJzGR"
   },
   "outputs": [],
   "source": [
    "direction_csv_file = pd.read_csv(\"Directions 1.54138969.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "ngWhbiCqLbiq",
    "outputId": "5d8ff197-dfc7-47ac-f682-6996f3916aad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>473.6836</td>\n",
       "      <td>444.9424</td>\n",
       "      <td>500.9961</td>\n",
       "      <td>448.0299</td>\n",
       "      <td>479.8393</td>\n",
       "      <td>530.7856</td>\n",
       "      <td>506.2184</td>\n",
       "      <td>622.5688</td>\n",
       "      <td>493.6608</td>\n",
       "      <td>621.9954</td>\n",
       "      <td>...</td>\n",
       "      <td>515.4715</td>\n",
       "      <td>456.4298</td>\n",
       "      <td>515.4715</td>\n",
       "      <td>456.4298</td>\n",
       "      <td>499.2511</td>\n",
       "      <td>448.2281</td>\n",
       "      <td>515.0607</td>\n",
       "      <td>479.1209</td>\n",
       "      <td>515.0607</td>\n",
       "      <td>479.1209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>473.6876</td>\n",
       "      <td>444.9526</td>\n",
       "      <td>500.9847</td>\n",
       "      <td>448.0307</td>\n",
       "      <td>479.8167</td>\n",
       "      <td>530.7855</td>\n",
       "      <td>506.2182</td>\n",
       "      <td>622.5726</td>\n",
       "      <td>493.6592</td>\n",
       "      <td>621.9958</td>\n",
       "      <td>...</td>\n",
       "      <td>514.9991</td>\n",
       "      <td>456.2006</td>\n",
       "      <td>514.9991</td>\n",
       "      <td>456.2006</td>\n",
       "      <td>498.6736</td>\n",
       "      <td>447.8242</td>\n",
       "      <td>514.2874</td>\n",
       "      <td>478.6892</td>\n",
       "      <td>514.2874</td>\n",
       "      <td>478.6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>473.6931</td>\n",
       "      <td>444.9663</td>\n",
       "      <td>500.9775</td>\n",
       "      <td>448.0403</td>\n",
       "      <td>479.7976</td>\n",
       "      <td>530.7957</td>\n",
       "      <td>506.2265</td>\n",
       "      <td>622.5868</td>\n",
       "      <td>493.6538</td>\n",
       "      <td>621.9990</td>\n",
       "      <td>...</td>\n",
       "      <td>514.7042</td>\n",
       "      <td>456.0592</td>\n",
       "      <td>514.7042</td>\n",
       "      <td>456.0592</td>\n",
       "      <td>498.3504</td>\n",
       "      <td>447.4883</td>\n",
       "      <td>513.7126</td>\n",
       "      <td>478.3876</td>\n",
       "      <td>513.7126</td>\n",
       "      <td>478.3876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>473.7039</td>\n",
       "      <td>444.9854</td>\n",
       "      <td>500.9819</td>\n",
       "      <td>448.0659</td>\n",
       "      <td>479.7655</td>\n",
       "      <td>530.8015</td>\n",
       "      <td>506.2534</td>\n",
       "      <td>622.5867</td>\n",
       "      <td>493.6528</td>\n",
       "      <td>622.0079</td>\n",
       "      <td>...</td>\n",
       "      <td>514.5616</td>\n",
       "      <td>455.9726</td>\n",
       "      <td>514.5616</td>\n",
       "      <td>455.9726</td>\n",
       "      <td>498.1750</td>\n",
       "      <td>447.2817</td>\n",
       "      <td>513.3622</td>\n",
       "      <td>478.2127</td>\n",
       "      <td>513.3622</td>\n",
       "      <td>478.2127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>473.7261</td>\n",
       "      <td>445.0067</td>\n",
       "      <td>501.0016</td>\n",
       "      <td>448.0924</td>\n",
       "      <td>479.7466</td>\n",
       "      <td>530.8076</td>\n",
       "      <td>506.2579</td>\n",
       "      <td>622.5981</td>\n",
       "      <td>493.6506</td>\n",
       "      <td>622.0058</td>\n",
       "      <td>...</td>\n",
       "      <td>514.6009</td>\n",
       "      <td>455.9780</td>\n",
       "      <td>514.6009</td>\n",
       "      <td>455.9780</td>\n",
       "      <td>498.1767</td>\n",
       "      <td>447.2362</td>\n",
       "      <td>513.2590</td>\n",
       "      <td>478.2043</td>\n",
       "      <td>513.2590</td>\n",
       "      <td>478.2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1378</td>\n",
       "      <td>508.8683</td>\n",
       "      <td>435.9992</td>\n",
       "      <td>531.6415</td>\n",
       "      <td>432.5145</td>\n",
       "      <td>531.7281</td>\n",
       "      <td>525.6750</td>\n",
       "      <td>536.5560</td>\n",
       "      <td>619.7991</td>\n",
       "      <td>507.8583</td>\n",
       "      <td>621.3291</td>\n",
       "      <td>...</td>\n",
       "      <td>552.5535</td>\n",
       "      <td>450.4184</td>\n",
       "      <td>552.5535</td>\n",
       "      <td>450.4184</td>\n",
       "      <td>535.6805</td>\n",
       "      <td>447.9043</td>\n",
       "      <td>557.0486</td>\n",
       "      <td>475.3463</td>\n",
       "      <td>557.0486</td>\n",
       "      <td>475.3463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1379</td>\n",
       "      <td>508.8894</td>\n",
       "      <td>435.9756</td>\n",
       "      <td>531.7077</td>\n",
       "      <td>432.5069</td>\n",
       "      <td>531.7758</td>\n",
       "      <td>525.6630</td>\n",
       "      <td>536.5682</td>\n",
       "      <td>619.7924</td>\n",
       "      <td>507.8712</td>\n",
       "      <td>621.3207</td>\n",
       "      <td>...</td>\n",
       "      <td>551.9902</td>\n",
       "      <td>450.3370</td>\n",
       "      <td>551.9902</td>\n",
       "      <td>450.3370</td>\n",
       "      <td>535.2592</td>\n",
       "      <td>447.4315</td>\n",
       "      <td>556.0175</td>\n",
       "      <td>475.3270</td>\n",
       "      <td>556.0175</td>\n",
       "      <td>475.3270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>508.9090</td>\n",
       "      <td>435.9860</td>\n",
       "      <td>531.6987</td>\n",
       "      <td>432.5204</td>\n",
       "      <td>531.7960</td>\n",
       "      <td>525.6733</td>\n",
       "      <td>536.5767</td>\n",
       "      <td>619.8024</td>\n",
       "      <td>507.8655</td>\n",
       "      <td>621.3173</td>\n",
       "      <td>...</td>\n",
       "      <td>551.3666</td>\n",
       "      <td>450.2951</td>\n",
       "      <td>551.3666</td>\n",
       "      <td>450.2951</td>\n",
       "      <td>534.9061</td>\n",
       "      <td>446.8693</td>\n",
       "      <td>554.7869</td>\n",
       "      <td>475.3963</td>\n",
       "      <td>554.7869</td>\n",
       "      <td>475.3963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1381</td>\n",
       "      <td>508.9323</td>\n",
       "      <td>435.9888</td>\n",
       "      <td>531.6935</td>\n",
       "      <td>432.5264</td>\n",
       "      <td>531.8196</td>\n",
       "      <td>525.6773</td>\n",
       "      <td>536.5802</td>\n",
       "      <td>619.8096</td>\n",
       "      <td>507.8681</td>\n",
       "      <td>621.3137</td>\n",
       "      <td>...</td>\n",
       "      <td>550.6077</td>\n",
       "      <td>450.2511</td>\n",
       "      <td>550.6077</td>\n",
       "      <td>450.2511</td>\n",
       "      <td>534.4236</td>\n",
       "      <td>446.4702</td>\n",
       "      <td>553.4522</td>\n",
       "      <td>475.5750</td>\n",
       "      <td>553.4522</td>\n",
       "      <td>475.5750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1382</td>\n",
       "      <td>508.9461</td>\n",
       "      <td>435.9793</td>\n",
       "      <td>531.6711</td>\n",
       "      <td>432.5156</td>\n",
       "      <td>531.7913</td>\n",
       "      <td>525.6635</td>\n",
       "      <td>536.5890</td>\n",
       "      <td>619.7971</td>\n",
       "      <td>507.8838</td>\n",
       "      <td>621.3116</td>\n",
       "      <td>...</td>\n",
       "      <td>549.7629</td>\n",
       "      <td>450.2849</td>\n",
       "      <td>549.7629</td>\n",
       "      <td>450.2849</td>\n",
       "      <td>533.8069</td>\n",
       "      <td>446.1633</td>\n",
       "      <td>552.0135</td>\n",
       "      <td>475.7899</td>\n",
       "      <td>552.0135</td>\n",
       "      <td>475.7899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1383 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     473.6836  444.9424  500.9961  448.0299  479.8393  530.7856  506.2184   \n",
       "1     473.6876  444.9526  500.9847  448.0307  479.8167  530.7855  506.2182   \n",
       "2     473.6931  444.9663  500.9775  448.0403  479.7976  530.7957  506.2265   \n",
       "3     473.7039  444.9854  500.9819  448.0659  479.7655  530.8015  506.2534   \n",
       "4     473.7261  445.0067  501.0016  448.0924  479.7466  530.8076  506.2579   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1378  508.8683  435.9992  531.6415  432.5145  531.7281  525.6750  536.5560   \n",
       "1379  508.8894  435.9756  531.7077  432.5069  531.7758  525.6630  536.5682   \n",
       "1380  508.9090  435.9860  531.6987  432.5204  531.7960  525.6733  536.5767   \n",
       "1381  508.9323  435.9888  531.6935  432.5264  531.8196  525.6773  536.5802   \n",
       "1382  508.9461  435.9793  531.6711  432.5156  531.7913  525.6635  536.5890   \n",
       "\n",
       "            7         8         9   ...        54        55        56  \\\n",
       "0     622.5688  493.6608  621.9954  ...  515.4715  456.4298  515.4715   \n",
       "1     622.5726  493.6592  621.9958  ...  514.9991  456.2006  514.9991   \n",
       "2     622.5868  493.6538  621.9990  ...  514.7042  456.0592  514.7042   \n",
       "3     622.5867  493.6528  622.0079  ...  514.5616  455.9726  514.5616   \n",
       "4     622.5981  493.6506  622.0058  ...  514.6009  455.9780  514.6009   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1378  619.7991  507.8583  621.3291  ...  552.5535  450.4184  552.5535   \n",
       "1379  619.7924  507.8712  621.3207  ...  551.9902  450.3370  551.9902   \n",
       "1380  619.8024  507.8655  621.3173  ...  551.3666  450.2951  551.3666   \n",
       "1381  619.8096  507.8681  621.3137  ...  550.6077  450.2511  550.6077   \n",
       "1382  619.7971  507.8838  621.3116  ...  549.7629  450.2849  549.7629   \n",
       "\n",
       "            57        58        59        60        61        62        63  \n",
       "0     456.4298  499.2511  448.2281  515.0607  479.1209  515.0607  479.1209  \n",
       "1     456.2006  498.6736  447.8242  514.2874  478.6892  514.2874  478.6892  \n",
       "2     456.0592  498.3504  447.4883  513.7126  478.3876  513.7126  478.3876  \n",
       "3     455.9726  498.1750  447.2817  513.3622  478.2127  513.3622  478.2127  \n",
       "4     455.9780  498.1767  447.2362  513.2590  478.2043  513.2590  478.2043  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1378  450.4184  535.6805  447.9043  557.0486  475.3463  557.0486  475.3463  \n",
       "1379  450.3370  535.2592  447.4315  556.0175  475.3270  556.0175  475.3270  \n",
       "1380  450.2951  534.9061  446.8693  554.7869  475.3963  554.7869  475.3963  \n",
       "1381  450.2511  534.4236  446.4702  553.4522  475.5750  553.4522  475.5750  \n",
       "1382  450.2849  533.8069  446.1633  552.0135  475.7899  552.0135  475.7899  \n",
       "\n",
       "[1383 rows x 64 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direction_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_numpy = np.asarray(direction_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_data = downsample_dataset(direction_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to downsample the dataset to half of its size\n",
    "def downsample_dataset(dataset) :\n",
    "    return np.delete(dataset, np.s_[::2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "rgfhc4kTmO8Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class Dataset_Preprocessing:\n",
    "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
    "        self.dir_path = dir_path\n",
    "        self.include_dimension = include_dimension\n",
    "        self.sample_size = sample_size\n",
    "        self.total_classes = total_classes\n",
    "        self.internal_folders = ['S1','S5','S6','S7','S8','S9','S11']\n",
    "    \n",
    "    def read_dataset(self):\n",
    "        try:\n",
    "            activity_vector = {}\n",
    "            sampled_data = {}\n",
    "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
    "            if not os.path.exists(self.dir_path):\n",
    "                print('The Data Directory Does not Exist!')\n",
    "                return None\n",
    "            \n",
    "            for fld in self.internal_folders:\n",
    "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
    "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
    "                    activity = self.__extract_activity(file)\n",
    "                    if activity not in activity_vector:\n",
    "                        total_keys = len(activity_vector.keys())\n",
    "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
    "                        activity_vector[activity][total_keys] = 1\n",
    "                    vector = activity_vector[activity]\n",
    "                    if (fld,activity) in sampled_data:\n",
    "                        sampled_data[(fld,activity)].append(self.__group_samples(data, self.sample_size, vector))\n",
    "                    else:\n",
    "                        sampled_data[(fld,activity)] = self.__group_samples(data, self.sample_size, vector)\n",
    "            return sampled_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __extract_activity(self, filename):\n",
    "        try:\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
    "            return activity\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __group_samples(self, dataset, sample_size, activity):\n",
    "        try:\n",
    "            if not isinstance(dataset, pd.DataFrame):\n",
    "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
    "                return None\n",
    "            \n",
    "            grouped_rows = []\n",
    "            grouped_rows_activity = []\n",
    "            starting_index = 0\n",
    "            for i in range(0, dataset.shape[0]//sample_size):\n",
    "                gr = dataset.iloc[starting_index:starting_index + sample_size,:]\n",
    "                grouped_rows.append(np.asarray(gr))\n",
    "                starting_index += sample_size\n",
    "            \n",
    "            for grp in grouped_rows:\n",
    "                ngrp = []\n",
    "                for row in grp:\n",
    "                    ngrp.append(row.tolist() + activity.tolist())\n",
    "                grouped_rows_activity.append(ngrp)\n",
    "            return grouped_rows_activity\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3nQyeFUmO8b",
    "outputId": "e9787e30-e737-45ac-888d-12a77ad1ba28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Data Directory Does not Exist!\n"
     ]
    }
   ],
   "source": [
    "sampled_data = Dataset_Preprocessing('H3.6csv').read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "t_H6gABPa_PQ"
   },
   "outputs": [],
   "source": [
    "#Convert to numpy array\n",
    "direction_data_array = np.asarray(direction_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "icyPO6IynFLA",
    "outputId": "6c008ce2-369a-4edb-d0b9-60c55232b5df"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/content/drive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "KAwzUZ8KLdJS"
   },
   "outputs": [],
   "source": [
    "class GloGen_Bidirectional_RNN_Encoder(Model_):\n",
    "  def __init__(self, num_recurrent_neurons=200):\n",
    "    super(GloGen_Bidirectional_RNN_Encoder, self).__init__()\n",
    "\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "  def call(self, input_x):\n",
    "    output = self.recurrent_layer(input_x)\n",
    "    #output = tf.one_hot(tf.argmax(output, axis = 1), depth=3)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "TqFfWtIpSXIC"
   },
   "outputs": [],
   "source": [
    "#Not sure of the output shape from the glonet\n",
    "class GloGen_Bidirectional_RNN_decoder(Model_):\n",
    "  def __init__(self, num_recurrent_neurons=64):\n",
    "    super(GloGen_Bidirectional_RNN_decoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "  def call(self, encoder_output):\n",
    "    glogen_output = self.recurrent_layer(encoder_output)\n",
    "    #output = tf.one_hot(tf.argmax(output, axis = 1), depth=3)\n",
    "    return glogen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "JQ3DZUkxLAQA"
   },
   "outputs": [],
   "source": [
    "class LocGen_Bidirectional_RNN_encoder(Model_) :\n",
    "  def __init__(self, num_recurrent_neurons=200) :\n",
    "    super(LocGen_Bidirectional_RNN_encoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "\n",
    "  def call(self, interpolated_output) :\n",
    "    predictions = self.recurrent_layer(interpolated_output)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "zZPdNS_Em6dX"
   },
   "outputs": [],
   "source": [
    "class LocGen_Bidirectional_RNN_decoder(Model_) :\n",
    "  def __init__(self, num_recurrent_neurons=64) :\n",
    "    super(LocGen_Bidirectional_RNN_decoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "  def call(self, encoder_output) :\n",
    "    final_predictions = self.recurrent_layer(encoder_output)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "QizN2JrjaobT"
   },
   "outputs": [],
   "source": [
    "glogen_bidirectional_RNN_Encoder = GloGen_Bidirectional_RNN_Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "2qHmJmIx612d"
   },
   "outputs": [],
   "source": [
    "#Reshaping direction data array\n",
    "direction_data_array = np.reshape(direction_data_array[:100], (10, 10, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SyfF1RfGbkgE",
    "outputId": "19e9d2c3-bb8f-4958-8bcd-228d156ae1ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Should have diminsions 100x60x64 as batch_siZe x timesteps x features\n",
    "direction_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "MNaOr-LGauZP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer glo_gen__bidirectional_rnn__encoder_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simulating applying the glogen encoder on an input batch with batch size \"10\"\n",
    "output_encoder = glogen_bidirectional_RNN_Encoder(direction_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqNyaMUPa8LA",
    "outputId": "bf568f88-734f-442c-d96e-94304e932b64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10, 200])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of 10x200 predictions for each sample inside the batch\n",
    "output_encoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "B7kOinVXfTKt"
   },
   "outputs": [],
   "source": [
    "#Simulating the output shape for the decoder\n",
    "glogen_decoder = GloGen_Bidirectional_RNN_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "oLxF5BCB9pTX"
   },
   "outputs": [],
   "source": [
    "glogen_sparse_output_predictions = glogen_decoder(output_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQT9RhgJ90rB",
    "outputId": "e9983a73-40b3-4264-93bc-069a9ffd6d29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10, 64])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction for 32 joints position for the next 10 timesteps for each sample inside the batch\n",
    "glogen_sparse_output_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ZG4LavnDCeyA"
   },
   "outputs": [],
   "source": [
    "#returns a list of the interpolated frames between x0 and x1\n",
    "def interpolateFrames(glogen_sparse_output_predictions, num_of_frames=5) :\n",
    "  batch_size = glogen_sparse_output_predictions.shape[0]\n",
    "  timesteps = glogen_sparse_output_predictions.shape[1]\n",
    "  features = glogen_sparse_output_predictions.shape[2]\n",
    "  interpolated_frames = np.zeros((batch_size, timesteps, num_of_frames, features))\n",
    "  for batch in range(glogen_sparse_output_predictions.shape[0]) :\n",
    "    for t in range(glogen_sparse_output_predictions.shape[1]-1) :\n",
    "      for j in range(num_of_frames) :\n",
    "        X_i0 = glogen_sparse_output_predictions[batch, t]\n",
    "        X_i1 = glogen_sparse_output_predictions[batch, t+1]\n",
    "        \n",
    "        alpha_j = j/num_of_frames\n",
    "        current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
    "        interpolated_frames[batch, t, j] = current_frame\n",
    "\n",
    "  return interpolated_frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "80Iu58hSH-ms"
   },
   "outputs": [],
   "source": [
    "interpolated_frames = interpolateFrames(glogen_sparse_output_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DZXeYTXJWup",
    "outputId": "5084801d-4593-4ea9-b2ae-fea1aeb346de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 5, 64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of interpolated frames should be 10(batch size) x 10(timesteps)\n",
    "#x 5(num of interpolated frames between these timesteps) x 64(num_features)\n",
    "interpolated_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "NNiC2u_vMgHa"
   },
   "outputs": [],
   "source": [
    "#timesteps and interpolated frames diminsion can be flattened to 10(batch size) x 50(dense frames) x 64(features)\n",
    "interpolated_frames = np.reshape(interpolated_frames, (10, 50, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDsfIXJf93EL",
    "outputId": "77fd2bcf-2aa6-4fef-81f0-3a1d522ee316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer loc_gen__bidirectional_rnn_encoder_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "loc gen encoder output is  (10, 50, 200)\n",
      "final predictions shape is  (10, 50, 64)\n"
     ]
    }
   ],
   "source": [
    "#Simulating the locgen phase with batches as well\n",
    "locgen_encoder = LocGen_Bidirectional_RNN_encoder()\n",
    "locgen_encoder_output = locgen_encoder(interpolated_frames)\n",
    "print(\"loc gen encoder output is \", locgen_encoder_output.shape)\n",
    "\n",
    "locgen_decoder = LocGen_Bidirectional_RNN_decoder()\n",
    "final_predictions = locgen_decoder(locgen_encoder_output)\n",
    "\n",
    "print(\"final predictions shape is \", final_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "ggz_bg8soyjJ"
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, mb = 8, lr = 0.00001, loss = tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam, \n",
    "               lambda_1=0.5, lambda_2=0.5):\n",
    "        self.model     = model\n",
    "        self.loss      = loss() #tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = opt(learning_rate = lr)\n",
    "        self.mb        = mb\n",
    "\n",
    "        self.lambda_1 = lambda_1\n",
    "        self.lambda_2 = lambda_2\n",
    "\n",
    "        self.train_loss     = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "        self.test_loss     = tf.keras.metrics.Mean(name='test_loss')\n",
    "        self.test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "  \n",
    "    @tf.function\n",
    "    def train_step(self, x , y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x)\n",
    "            loss = self.loss(y, predictions)\n",
    "\n",
    "        #print(self.model.trainable_variables)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(y, predictions)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x , y):\n",
    "        predictions = self.model(x)\n",
    "        loss = self.loss(y, predictions)\n",
    "        self.test_loss(loss)\n",
    "        self.test_accuracy(y, predictions)\n",
    "\n",
    "    def train(self):\n",
    "        for mbX, mbY in self.train_ds:\n",
    "            self.train_step(mbX, mbY)\n",
    "\n",
    "    def test(self):\n",
    "        for mbX, mbY in self.test_ds:\n",
    "            self.test_step(mbX, mbY)    \n",
    "\n",
    "    def run(self, dataX, dataY, testX, testY, epochs, verbose=2):\n",
    "        historyTR = []\n",
    "        historyTS = []\n",
    "        template = '{} {}, {}: {}, {}: {}'\n",
    "        self.train_ds = tf.data.Dataset.from_tensor_slices((dataX, dataY)).shuffle(16000).batch(self.mb)\n",
    "        self.test_ds  = tf.data.Dataset.from_tensor_slices((testX,testY)).batch(self.mb)\n",
    "        for i in range(epochs):\n",
    "\n",
    "            self.train ()\n",
    "            #   print(lossTR)\n",
    "            self.test  ()\n",
    "            if verbose > 0:\n",
    "                print(template.format(\"epoch: \", i+1,\n",
    "                              #\" TRAIN LOSS: \", self.train_loss.result(),\n",
    "                              #\" TEST LOSS: \" , self.test_loss.result()))\n",
    "                               \" TRAIN ACC: \" , self.train_accuracy.result()*100,\n",
    "                               \" TEST ACC: \"  , self.test_accuracy.result()*100) )\n",
    "\n",
    "            temp = '{}'\n",
    "            historyTR.append(float(temp.format(self.train_loss.result())))\n",
    "            historyTS.append(float(temp.format(self.test_loss.result() )))\n",
    "\n",
    "            self.train_loss.reset_states()\n",
    "            self.train_accuracy.reset_states()\n",
    "            self.test_loss.reset_states()\n",
    "            self.test_accuracy.reset_states()\n",
    "        return historyTR, historyTS\n",
    "\n",
    "    #Defining the loss function utilities\n",
    "    def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        diff_norm_2 = np.sum(np.square(np.subtract(final_predictions, real_sequance)), axis=2)\n",
    "        return np.sum(diff_norm_2, axis=1) \n",
    "\n",
    "    def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        predictions_tomporal_diffs = np.diff(predicted_sequance_batch, axis=1)\n",
    "        real_tomporal_diffs = np.diff(target_sequance_batch, axis=1)\n",
    "        prediction_motion_flow_diff_norm_2 = np.sum(np.square(np.subtract(final_predictions, real_sequance)), axis=2)\n",
    "        return np.sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
    "\n",
    "    def total_loss(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
    "        motion_flow_loss = self.motion_flow_loss(predicted_sequance_batch, target_sequance_batch)\n",
    "        return joints_loss*self.lambda_1 + motion_flow_loss*self.lambda_2\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Glocal_Net_Reimplementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
