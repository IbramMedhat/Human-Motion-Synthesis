{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QImRR28hJtIK",
    "outputId": "909a64cb-9575-4854-c6a4-e51ad456d692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Model as Model_\n",
    "from tensorflow.keras.layers import Input, PReLU, LeakyReLU, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, Conv2D, Flatten, Reshape, Conv1D, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2DTranspose, Dense, BatchNormalization\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "5xx23v8EJzGR"
   },
   "outputs": [],
   "source": [
    "direction_csv_file = pd.read_csv(\"Directions 1.54138969.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "ngWhbiCqLbiq",
    "outputId": "5d8ff197-dfc7-47ac-f682-6996f3916aad"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>473.6836</td>\n",
       "      <td>444.9424</td>\n",
       "      <td>500.9961</td>\n",
       "      <td>448.0299</td>\n",
       "      <td>479.8393</td>\n",
       "      <td>530.7856</td>\n",
       "      <td>506.2184</td>\n",
       "      <td>622.5688</td>\n",
       "      <td>493.6608</td>\n",
       "      <td>621.9954</td>\n",
       "      <td>...</td>\n",
       "      <td>515.4715</td>\n",
       "      <td>456.4298</td>\n",
       "      <td>515.4715</td>\n",
       "      <td>456.4298</td>\n",
       "      <td>499.2511</td>\n",
       "      <td>448.2281</td>\n",
       "      <td>515.0607</td>\n",
       "      <td>479.1209</td>\n",
       "      <td>515.0607</td>\n",
       "      <td>479.1209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>473.6876</td>\n",
       "      <td>444.9526</td>\n",
       "      <td>500.9847</td>\n",
       "      <td>448.0307</td>\n",
       "      <td>479.8167</td>\n",
       "      <td>530.7855</td>\n",
       "      <td>506.2182</td>\n",
       "      <td>622.5726</td>\n",
       "      <td>493.6592</td>\n",
       "      <td>621.9958</td>\n",
       "      <td>...</td>\n",
       "      <td>514.9991</td>\n",
       "      <td>456.2006</td>\n",
       "      <td>514.9991</td>\n",
       "      <td>456.2006</td>\n",
       "      <td>498.6736</td>\n",
       "      <td>447.8242</td>\n",
       "      <td>514.2874</td>\n",
       "      <td>478.6892</td>\n",
       "      <td>514.2874</td>\n",
       "      <td>478.6892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>473.6931</td>\n",
       "      <td>444.9663</td>\n",
       "      <td>500.9775</td>\n",
       "      <td>448.0403</td>\n",
       "      <td>479.7976</td>\n",
       "      <td>530.7957</td>\n",
       "      <td>506.2265</td>\n",
       "      <td>622.5868</td>\n",
       "      <td>493.6538</td>\n",
       "      <td>621.9990</td>\n",
       "      <td>...</td>\n",
       "      <td>514.7042</td>\n",
       "      <td>456.0592</td>\n",
       "      <td>514.7042</td>\n",
       "      <td>456.0592</td>\n",
       "      <td>498.3504</td>\n",
       "      <td>447.4883</td>\n",
       "      <td>513.7126</td>\n",
       "      <td>478.3876</td>\n",
       "      <td>513.7126</td>\n",
       "      <td>478.3876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>473.7039</td>\n",
       "      <td>444.9854</td>\n",
       "      <td>500.9819</td>\n",
       "      <td>448.0659</td>\n",
       "      <td>479.7655</td>\n",
       "      <td>530.8015</td>\n",
       "      <td>506.2534</td>\n",
       "      <td>622.5867</td>\n",
       "      <td>493.6528</td>\n",
       "      <td>622.0079</td>\n",
       "      <td>...</td>\n",
       "      <td>514.5616</td>\n",
       "      <td>455.9726</td>\n",
       "      <td>514.5616</td>\n",
       "      <td>455.9726</td>\n",
       "      <td>498.1750</td>\n",
       "      <td>447.2817</td>\n",
       "      <td>513.3622</td>\n",
       "      <td>478.2127</td>\n",
       "      <td>513.3622</td>\n",
       "      <td>478.2127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>473.7261</td>\n",
       "      <td>445.0067</td>\n",
       "      <td>501.0016</td>\n",
       "      <td>448.0924</td>\n",
       "      <td>479.7466</td>\n",
       "      <td>530.8076</td>\n",
       "      <td>506.2579</td>\n",
       "      <td>622.5981</td>\n",
       "      <td>493.6506</td>\n",
       "      <td>622.0058</td>\n",
       "      <td>...</td>\n",
       "      <td>514.6009</td>\n",
       "      <td>455.9780</td>\n",
       "      <td>514.6009</td>\n",
       "      <td>455.9780</td>\n",
       "      <td>498.1767</td>\n",
       "      <td>447.2362</td>\n",
       "      <td>513.2590</td>\n",
       "      <td>478.2043</td>\n",
       "      <td>513.2590</td>\n",
       "      <td>478.2043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1378</td>\n",
       "      <td>508.8683</td>\n",
       "      <td>435.9992</td>\n",
       "      <td>531.6415</td>\n",
       "      <td>432.5145</td>\n",
       "      <td>531.7281</td>\n",
       "      <td>525.6750</td>\n",
       "      <td>536.5560</td>\n",
       "      <td>619.7991</td>\n",
       "      <td>507.8583</td>\n",
       "      <td>621.3291</td>\n",
       "      <td>...</td>\n",
       "      <td>552.5535</td>\n",
       "      <td>450.4184</td>\n",
       "      <td>552.5535</td>\n",
       "      <td>450.4184</td>\n",
       "      <td>535.6805</td>\n",
       "      <td>447.9043</td>\n",
       "      <td>557.0486</td>\n",
       "      <td>475.3463</td>\n",
       "      <td>557.0486</td>\n",
       "      <td>475.3463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1379</td>\n",
       "      <td>508.8894</td>\n",
       "      <td>435.9756</td>\n",
       "      <td>531.7077</td>\n",
       "      <td>432.5069</td>\n",
       "      <td>531.7758</td>\n",
       "      <td>525.6630</td>\n",
       "      <td>536.5682</td>\n",
       "      <td>619.7924</td>\n",
       "      <td>507.8712</td>\n",
       "      <td>621.3207</td>\n",
       "      <td>...</td>\n",
       "      <td>551.9902</td>\n",
       "      <td>450.3370</td>\n",
       "      <td>551.9902</td>\n",
       "      <td>450.3370</td>\n",
       "      <td>535.2592</td>\n",
       "      <td>447.4315</td>\n",
       "      <td>556.0175</td>\n",
       "      <td>475.3270</td>\n",
       "      <td>556.0175</td>\n",
       "      <td>475.3270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>508.9090</td>\n",
       "      <td>435.9860</td>\n",
       "      <td>531.6987</td>\n",
       "      <td>432.5204</td>\n",
       "      <td>531.7960</td>\n",
       "      <td>525.6733</td>\n",
       "      <td>536.5767</td>\n",
       "      <td>619.8024</td>\n",
       "      <td>507.8655</td>\n",
       "      <td>621.3173</td>\n",
       "      <td>...</td>\n",
       "      <td>551.3666</td>\n",
       "      <td>450.2951</td>\n",
       "      <td>551.3666</td>\n",
       "      <td>450.2951</td>\n",
       "      <td>534.9061</td>\n",
       "      <td>446.8693</td>\n",
       "      <td>554.7869</td>\n",
       "      <td>475.3963</td>\n",
       "      <td>554.7869</td>\n",
       "      <td>475.3963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1381</td>\n",
       "      <td>508.9323</td>\n",
       "      <td>435.9888</td>\n",
       "      <td>531.6935</td>\n",
       "      <td>432.5264</td>\n",
       "      <td>531.8196</td>\n",
       "      <td>525.6773</td>\n",
       "      <td>536.5802</td>\n",
       "      <td>619.8096</td>\n",
       "      <td>507.8681</td>\n",
       "      <td>621.3137</td>\n",
       "      <td>...</td>\n",
       "      <td>550.6077</td>\n",
       "      <td>450.2511</td>\n",
       "      <td>550.6077</td>\n",
       "      <td>450.2511</td>\n",
       "      <td>534.4236</td>\n",
       "      <td>446.4702</td>\n",
       "      <td>553.4522</td>\n",
       "      <td>475.5750</td>\n",
       "      <td>553.4522</td>\n",
       "      <td>475.5750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1382</td>\n",
       "      <td>508.9461</td>\n",
       "      <td>435.9793</td>\n",
       "      <td>531.6711</td>\n",
       "      <td>432.5156</td>\n",
       "      <td>531.7913</td>\n",
       "      <td>525.6635</td>\n",
       "      <td>536.5890</td>\n",
       "      <td>619.7971</td>\n",
       "      <td>507.8838</td>\n",
       "      <td>621.3116</td>\n",
       "      <td>...</td>\n",
       "      <td>549.7629</td>\n",
       "      <td>450.2849</td>\n",
       "      <td>549.7629</td>\n",
       "      <td>450.2849</td>\n",
       "      <td>533.8069</td>\n",
       "      <td>446.1633</td>\n",
       "      <td>552.0135</td>\n",
       "      <td>475.7899</td>\n",
       "      <td>552.0135</td>\n",
       "      <td>475.7899</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1383 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0     473.6836  444.9424  500.9961  448.0299  479.8393  530.7856  506.2184   \n",
       "1     473.6876  444.9526  500.9847  448.0307  479.8167  530.7855  506.2182   \n",
       "2     473.6931  444.9663  500.9775  448.0403  479.7976  530.7957  506.2265   \n",
       "3     473.7039  444.9854  500.9819  448.0659  479.7655  530.8015  506.2534   \n",
       "4     473.7261  445.0067  501.0016  448.0924  479.7466  530.8076  506.2579   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1378  508.8683  435.9992  531.6415  432.5145  531.7281  525.6750  536.5560   \n",
       "1379  508.8894  435.9756  531.7077  432.5069  531.7758  525.6630  536.5682   \n",
       "1380  508.9090  435.9860  531.6987  432.5204  531.7960  525.6733  536.5767   \n",
       "1381  508.9323  435.9888  531.6935  432.5264  531.8196  525.6773  536.5802   \n",
       "1382  508.9461  435.9793  531.6711  432.5156  531.7913  525.6635  536.5890   \n",
       "\n",
       "            7         8         9   ...        54        55        56  \\\n",
       "0     622.5688  493.6608  621.9954  ...  515.4715  456.4298  515.4715   \n",
       "1     622.5726  493.6592  621.9958  ...  514.9991  456.2006  514.9991   \n",
       "2     622.5868  493.6538  621.9990  ...  514.7042  456.0592  514.7042   \n",
       "3     622.5867  493.6528  622.0079  ...  514.5616  455.9726  514.5616   \n",
       "4     622.5981  493.6506  622.0058  ...  514.6009  455.9780  514.6009   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1378  619.7991  507.8583  621.3291  ...  552.5535  450.4184  552.5535   \n",
       "1379  619.7924  507.8712  621.3207  ...  551.9902  450.3370  551.9902   \n",
       "1380  619.8024  507.8655  621.3173  ...  551.3666  450.2951  551.3666   \n",
       "1381  619.8096  507.8681  621.3137  ...  550.6077  450.2511  550.6077   \n",
       "1382  619.7971  507.8838  621.3116  ...  549.7629  450.2849  549.7629   \n",
       "\n",
       "            57        58        59        60        61        62        63  \n",
       "0     456.4298  499.2511  448.2281  515.0607  479.1209  515.0607  479.1209  \n",
       "1     456.2006  498.6736  447.8242  514.2874  478.6892  514.2874  478.6892  \n",
       "2     456.0592  498.3504  447.4883  513.7126  478.3876  513.7126  478.3876  \n",
       "3     455.9726  498.1750  447.2817  513.3622  478.2127  513.3622  478.2127  \n",
       "4     455.9780  498.1767  447.2362  513.2590  478.2043  513.2590  478.2043  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1378  450.4184  535.6805  447.9043  557.0486  475.3463  557.0486  475.3463  \n",
       "1379  450.3370  535.2592  447.4315  556.0175  475.3270  556.0175  475.3270  \n",
       "1380  450.2951  534.9061  446.8693  554.7869  475.3963  554.7869  475.3963  \n",
       "1381  450.2511  534.4236  446.4702  553.4522  475.5750  553.4522  475.5750  \n",
       "1382  450.2849  533.8069  446.1633  552.0135  475.7899  552.0135  475.7899  \n",
       "\n",
       "[1383 rows x 64 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direction_csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction_numpy = np.asarray(direction_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_data = downsample_dataset(direction_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to downsample the dataset to half of its size\n",
    "def downsample_dataset(dataset) :\n",
    "    return np.delete(dataset, np.s_[::2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "rgfhc4kTmO8Z"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "class Dataset_Preprocessing:\n",
    "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
    "        \n",
    "        #Dataset Directory path\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "        #Which Dimension file to include, possible values: 2 and 3\n",
    "        self.include_dimension = include_dimension\n",
    "        \n",
    "        #Total frames in one Sample\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        #Activity classes to include\n",
    "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
    "        \n",
    "        #Total activity classes\n",
    "        self.total_classes = len(self.classes)\n",
    "        \n",
    "        #Subject Folders names in the Dataset\n",
    "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
    "    \n",
    "    def read_dataset(self):\n",
    "        try:\n",
    "            #Contains all the different activity vectors\n",
    "            activity_vector = {}\n",
    "            \n",
    "            #Contains the overall dataset\n",
    "            sampled_data = None\n",
    "            \n",
    "            #Based on dimensions, which folder to use for extracting the dataset files\n",
    "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
    "            \n",
    "            #Checking if the dataset path is valid\n",
    "            if not os.path.exists(self.dir_path):\n",
    "                print('The Data Directory Does not Exist!')\n",
    "                return None\n",
    "\n",
    "            #Iterating over all the subject folders\n",
    "            for fld in self.internal_folders:\n",
    "                #Iterating for each file in the specified folder\n",
    "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
    "                    #Extracting the activity from the filename\n",
    "                    activity = self.__extract_activity(file)\n",
    "                    \n",
    "                    if activity not in self.classes:\n",
    "                        continue\n",
    "                    \n",
    "                    #Reading the CSV file using Pandas\n",
    "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
    "\n",
    "                    #Formulating the activity vector using one hot encoding\n",
    "                    if activity not in activity_vector:\n",
    "                        total_keys = len(activity_vector.keys())\n",
    "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
    "                        activity_vector[activity][total_keys] = 1\n",
    "                    vector = activity_vector[activity]\n",
    "                    \n",
    "                    #Sampling the dataset\n",
    "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
    "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
    "                    \n",
    "            return sampled_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __extract_activity(self, filename):\n",
    "        try:\n",
    "            #Extracting the filename and excluding the extension\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            #Substituting the empty string with characters other than english alphabets\n",
    "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
    "            return activity\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __group_samples(self, dataset, sample_size, activity):\n",
    "        try:\n",
    "            #Checking if the dataset is a Pandas Dataframe\n",
    "            if not isinstance(dataset, pd.DataFrame):\n",
    "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
    "                return None\n",
    "            \n",
    "            #Appending activity class to each row in the dataset\n",
    "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
    "            \n",
    "            #Reshaping the dataset into sample batches\n",
    "            total_samples = dataset.shape[0]//sample_size\n",
    "            total_features = dataset.shape[1]\n",
    "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
    "            \n",
    "            return grouped_rows\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3nQyeFUmO8b",
    "outputId": "e9787e30-e737-45ac-888d-12a77ad1ba28"
   },
   "outputs": [],
   "source": [
    "sampled_data = Dataset_Preprocessing('H3.6csv').read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30688, 50, 74)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 30688\n",
      "Total Frames: 50\n",
      "Total Features: 74\n"
     ]
    }
   ],
   "source": [
    "print('Total Samples: {}'.format(sampled_data.shape[0]))\n",
    "print('Total Frames: {}'.format(sampled_data.shape[1]))\n",
    "print('Total Features: {}'.format(sampled_data.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = sampled_data[np.s_[::10], :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3069, 50, 74)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training, validation and test \n",
    "def split_data(data_array, validation_size=0.1, test_size=0.1) :\n",
    "    assert validation_size+test_size < 0.5 , f\"total size of validation and testing set should not exceed half of the dataset\"\n",
    "    assert validation_size > 0, f\"validation size should be greater than zero\"\n",
    "    assert test_size > 0, f\"test size should be greater than zero\"\n",
    "    \n",
    "    validation_step = int(1/validation_size)\n",
    "    test_step = int(1/test_size)\n",
    "    \n",
    "    \n",
    "    mask = np.ones(data_array.shape, dtype=bool)\n",
    "    validation_data = data_array[np.s_[::validation_step], :, :]\n",
    "    mask[np.s_[1::validation_step],:,:] = False\n",
    "    test_data = data_array[np.s_[1::test_step], :, :]\n",
    "    mask[np.s_[::test_step],:,:] = False\n",
    "    training_data = data_array[mask]\n",
    "    training_data_size = data_array.shape[0] - (validation_data.shape[0]+test_data.shape[0])\n",
    "    training_data = training_data.reshape((training_data_size, data_array.shape[1], data_array.shape[2]))\n",
    "    return training_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
    "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
    "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
    "    labels = dataset[:,np.s_[input_sequance_size:], :]\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30688, 50, 74)\n",
      "(30688, 50, 74)\n"
     ]
    }
   ],
   "source": [
    "training_data, validation_data, test_data = split_data(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape is  (24550, 50, 74)\n",
      "Validation data shape is  (3069, 50, 74)\n",
      "Test data shape is  (3069, 50, 74)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data shape is \", training_data.shape)\n",
    "print(\"Validation data shape is \", validation_data.shape)\n",
    "print(\"Test data shape is \", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataX, training_dataY = split_to_features_labels(training_data, input_sequance_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24550, 10, 74)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "KAwzUZ8KLdJS"
   },
   "outputs": [],
   "source": [
    "class GloGen_Bidirectional_RNN_encoder(Model_):\n",
    "    def __init__(self, num_recurrent_neurons=200):\n",
    "        super(GloGen_Bidirectional_RNN_encoder, self).__init__()\n",
    "\n",
    "        #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "        self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "def call(self, input_x):\n",
    "    output = self.recurrent_layer(input_x)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "TqFfWtIpSXIC"
   },
   "outputs": [],
   "source": [
    "#Not sure of the output shape from the glonet\n",
    "class GloGen_Bidirectional_RNN_decoder(Model_):\n",
    "  def __init__(self, num_recurrent_neurons=64):\n",
    "    super(GloGen_Bidirectional_RNN_decoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "  def call(self, encoder_output):\n",
    "    glogen_output = self.recurrent_layer(encoder_output)\n",
    "    #output = tf.one_hot(tf.argmax(output, axis = 1), depth=3)\n",
    "    return glogen_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "JQ3DZUkxLAQA"
   },
   "outputs": [],
   "source": [
    "class LocGen_Bidirectional_RNN_encoder(Model_) :\n",
    "  def __init__(self, num_recurrent_neurons=200) :\n",
    "    super(LocGen_Bidirectional_RNN_encoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "\n",
    "  def call(self, interpolated_output) :\n",
    "    predictions = self.recurrent_layer(interpolated_output)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "zZPdNS_Em6dX"
   },
   "outputs": [],
   "source": [
    "class LocGen_Bidirectional_RNN_decoder(Model_) :\n",
    "  def __init__(self, num_recurrent_neurons=64) :\n",
    "    super(LocGen_Bidirectional_RNN_decoder, self).__init__()\n",
    "    #Return Sequances=True to assure return of output corresponding to each timestep\n",
    "    self.recurrent_layer = LSTM(num_recurrent_neurons, return_sequences=True)\n",
    "\n",
    "  def call(self, encoder_output) :\n",
    "    final_predictions = self.recurrent_layer(encoder_output)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "QizN2JrjaobT"
   },
   "outputs": [],
   "source": [
    "glogen_bidirectional_RNN_Encoder = GloGen_Bidirectional_RNN_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "2qHmJmIx612d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24550, 50, 74)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reshaping direction data array\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SyfF1RfGbkgE",
    "outputId": "19e9d2c3-bb8f-4958-8bcd-228d156ae1ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Should have diminsions 100x60x64 as batch_siZe x timesteps x features\n",
    "direction_data_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "MNaOr-LGauZP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer glo_gen__bidirectional_rnn__encoder_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#simulating applying the glogen encoder on an input batch with batch size \"10\"\n",
    "output_encoder = glogen_bidirectional_RNN_Encoder(direction_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IqNyaMUPa8LA",
    "outputId": "bf568f88-734f-442c-d96e-94304e932b64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10, 200])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#shape of 10x200 predictions for each sample inside the batch\n",
    "output_encoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "B7kOinVXfTKt"
   },
   "outputs": [],
   "source": [
    "#Simulating the output shape for the decoder\n",
    "glogen_decoder = GloGen_Bidirectional_RNN_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "oLxF5BCB9pTX"
   },
   "outputs": [],
   "source": [
    "glogen_sparse_output_predictions = glogen_decoder(output_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQT9RhgJ90rB",
    "outputId": "e9983a73-40b3-4264-93bc-069a9ffd6d29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 10, 64])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction for 32 joints position for the next 10 timesteps for each sample inside the batch\n",
    "glogen_sparse_output_predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ZG4LavnDCeyA"
   },
   "outputs": [],
   "source": [
    "#returns a list of the interpolated frames between x0 and x1\n",
    "def interpolateFrames(glogen_sparse_output_predictions, num_of_frames=5) :\n",
    "  batch_size = glogen_sparse_output_predictions.shape[0]\n",
    "  timesteps = glogen_sparse_output_predictions.shape[1]\n",
    "  features = glogen_sparse_output_predictions.shape[2]\n",
    "  interpolated_frames = np.zeros((batch_size, timesteps, num_of_frames, features))\n",
    "  for batch in range(glogen_sparse_output_predictions.shape[0]) :\n",
    "    for t in range(glogen_sparse_output_predictions.shape[1]-1) :\n",
    "      for j in range(num_of_frames) :\n",
    "        X_i0 = glogen_sparse_output_predictions[batch, t]\n",
    "        X_i1 = glogen_sparse_output_predictions[batch, t+1]\n",
    "        \n",
    "        alpha_j = j/num_of_frames\n",
    "        current_frame = alpha_j*X_i0 + (1-alpha_j)*X_i1\n",
    "        interpolated_frames[batch, t, j] = current_frame\n",
    "\n",
    "  return interpolated_frames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "80Iu58hSH-ms"
   },
   "outputs": [],
   "source": [
    "interpolated_frames = interpolateFrames(glogen_sparse_output_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3DZXeYTXJWup",
    "outputId": "5084801d-4593-4ea9-b2ae-fea1aeb346de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 5, 64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of interpolated frames should be 10(batch size) x 10(timesteps)\n",
    "#x 5(num of interpolated frames between these timesteps) x 64(num_features)\n",
    "interpolated_frames.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "NNiC2u_vMgHa"
   },
   "outputs": [],
   "source": [
    "#timesteps and interpolated frames diminsion can be flattened to 10(batch size) x 50(dense frames) x 64(features)\n",
    "interpolated_frames = np.reshape(interpolated_frames, (10, 50, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wDsfIXJf93EL",
    "outputId": "77fd2bcf-2aa6-4fef-81f0-3a1d522ee316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer loc_gen__bidirectional_rnn_encoder_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "loc gen encoder output is  (10, 50, 200)\n",
      "final predictions shape is  (10, 50, 64)\n"
     ]
    }
   ],
   "source": [
    "#Simulating the locgen phase with batches as well\n",
    "locgen_encoder = LocGen_Bidirectional_RNN_encoder()\n",
    "locgen_encoder_output = locgen_encoder(interpolated_frames)\n",
    "print(\"loc gen encoder output is \", locgen_encoder_output.shape)\n",
    "\n",
    "locgen_decoder = LocGen_Bidirectional_RNN_decoder()\n",
    "final_predictions = locgen_decoder(locgen_encoder_output)\n",
    "\n",
    "print(\"final predictions shape is \", final_predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "ggz_bg8soyjJ"
   },
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, model, mb = 8, lr = 0.00001, loss = tf.keras.losses.MeanSquaredError, opt=tf.keras.optimizers.Adam, \n",
    "               lambda_1=0.5, lambda_2=0.5):\n",
    "        self.model     = model\n",
    "        self.loss      = loss() #tf.keras.losses.MeanSquaredError()\n",
    "        self.optimizer = opt(learning_rate = lr)\n",
    "        self.mb        = mb\n",
    "\n",
    "        self.lambda_1 = lambda_1\n",
    "        self.lambda_2 = lambda_2\n",
    "\n",
    "        self.train_loss     = tf.keras.metrics.Mean(name='train_loss')\n",
    "        self.train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "        self.test_loss     = tf.keras.metrics.Mean(name='test_loss')\n",
    "        self.test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "  \n",
    "    @tf.function\n",
    "    def train_step(self, x , y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.model(x)\n",
    "            loss = self.loss(y, predictions)\n",
    "\n",
    "        #print(self.model.trainable_variables)\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
    "        self.train_loss(loss)\n",
    "        self.train_accuracy(y, predictions)\n",
    "        return loss\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, x , y):\n",
    "        predictions = self.model(x)\n",
    "        loss = self.loss(y, predictions)\n",
    "        self.test_loss(loss)\n",
    "        self.test_accuracy(y, predictions)\n",
    "\n",
    "    def train(self):\n",
    "        for mbX, mbY in self.train_ds:\n",
    "            self.train_step(mbX, mbY)\n",
    "\n",
    "    def test(self):\n",
    "        for mbX, mbY in self.test_ds:\n",
    "            self.test_step(mbX, mbY)    \n",
    "\n",
    "    def run(self, dataX, dataY, testX, testY, epochs, verbose=2):\n",
    "        historyTR = []\n",
    "        historyTS = []\n",
    "        template = '{} {}, {}: {}, {}: {}'\n",
    "        self.train_ds = tf.data.Dataset.from_tensor_slices((dataX, dataY)).shuffle(16000).batch(self.mb)\n",
    "        self.test_ds  = tf.data.Dataset.from_tensor_slices((testX,testY)).batch(self.mb)\n",
    "        for i in range(epochs):\n",
    "\n",
    "            self.train ()\n",
    "            #   print(lossTR)\n",
    "            self.test  ()\n",
    "            if verbose > 0:\n",
    "                print(template.format(\"epoch: \", i+1,\n",
    "                              #\" TRAIN LOSS: \", self.train_loss.result(),\n",
    "                              #\" TEST LOSS: \" , self.test_loss.result()))\n",
    "                               \" TRAIN ACC: \" , self.train_accuracy.result()*100,\n",
    "                               \" TEST ACC: \"  , self.test_accuracy.result()*100) )\n",
    "\n",
    "            temp = '{}'\n",
    "            historyTR.append(float(temp.format(self.train_loss.result())))\n",
    "            historyTS.append(float(temp.format(self.test_loss.result() )))\n",
    "\n",
    "            self.train_loss.reset_states()\n",
    "            self.train_accuracy.reset_states()\n",
    "            self.test_loss.reset_states()\n",
    "            self.test_accuracy.reset_states()\n",
    "        return historyTR, historyTS\n",
    "\n",
    "    #Defining the loss function utilities\n",
    "    def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        diff_norm_2 = np.sum(np.square(np.subtract(final_predictions, real_sequance)), axis=2)\n",
    "        return np.sum(diff_norm_2, axis=1) \n",
    "\n",
    "    def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        predictions_tomporal_diffs = np.diff(predicted_sequance_batch, axis=1)\n",
    "        real_tomporal_diffs = np.diff(target_sequance_batch, axis=1)\n",
    "        prediction_motion_flow_diff_norm_2 = np.sum(np.square(np.subtract(final_predictions, real_sequance)), axis=2)\n",
    "        return np.sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
    "\n",
    "    def total_loss(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
    "        motion_flow_loss = self.motion_flow_loss(predicted_sequance_batch, target_sequance_batch)\n",
    "        return joints_loss*self.lambda_1 + motion_flow_loss*self.lambda_2\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Glocal_Net_Reimplementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
