{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5bGejq7QySe"
   },
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6oWUCJCnPmT",
    "outputId": "7c5b49ce-0b4d-4f9a-eac8-f05ee626a96d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Model as Model_\n",
    "from tensorflow.keras.layers import Input, ReLU, LSTM, GRU, SimpleRNN, Dense, TimeDistributed, Bidirectional, GaussianNoise \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_remediation.min_diff.losses.mmd_loss as MMD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import os\n",
    "import re\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tUdJibeQWdF"
   },
   "source": [
    "## Dataset Reading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing all the movable joints in the human skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VYTyMttRQVHV"
   },
   "outputs": [],
   "source": [
    "#Was done in the preprocessing in [1]\n",
    "# Joints in H3.6M -- data has 32 joints, but only 17 that move; these are the indices.\n",
    "H36M_NAMES = ['']*32\n",
    "H36M_NAMES[0]  = 'Hip'\n",
    "H36M_NAMES[1]  = 'RHip'\n",
    "H36M_NAMES[2]  = 'RKnee'\n",
    "H36M_NAMES[3]  = 'RFoot'\n",
    "H36M_NAMES[6]  = 'LHip'\n",
    "H36M_NAMES[7]  = 'LKnee'\n",
    "H36M_NAMES[8]  = 'LFoot'\n",
    "H36M_NAMES[12] = 'Spine'\n",
    "H36M_NAMES[13] = 'Thorax'\n",
    "H36M_NAMES[14] = 'Neck/Nose'\n",
    "H36M_NAMES[15] = 'Head'\n",
    "H36M_NAMES[17] = 'LShoulder'\n",
    "H36M_NAMES[18] = 'LElbow'\n",
    "H36M_NAMES[19] = 'LWrist'\n",
    "H36M_NAMES[25] = 'RShoulder'\n",
    "H36M_NAMES[26] = 'RElbow'\n",
    "H36M_NAMES[27] = 'RWrist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A class to Read and Combine all the Dataset files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ikRIhsrrnhlH"
   },
   "outputs": [],
   "source": [
    "class Dataset_loading:\n",
    "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, \n",
    "                 total_classes = 17, datatype = 'float32', include_movable_joints = False, batch_size = 20, \n",
    "                 include_action_labels = True, return_action_labels = False):\n",
    "        \n",
    "        #Dataset Directory path\n",
    "        self.dir_path = dir_path\n",
    "        \n",
    "        #Which Dimension file to include, possible values: 2 and 3\n",
    "        self.include_dimension = include_dimension\n",
    "        \n",
    "        #Total frames in one Sample\n",
    "        self.sample_size = sample_size\n",
    "        \n",
    "        #Default Datatype for all the samples\n",
    "        self.datatype = datatype\n",
    "        \n",
    "        #Batch Size of the dataset for experimentation \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #Boolean value to indicate whether to include action class in each frames\n",
    "        self.include_action_labels = include_action_labels\n",
    "        \n",
    "        #Whether to return action labels with data\n",
    "        self.return_action_labels = return_action_labels\n",
    "        \n",
    "        #Activity classes to include\n",
    "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
    "        \n",
    "        #Total activity classes\n",
    "        self.total_classes = len(self.classes)\n",
    "        \n",
    "        #Subject Folders names in the Dataset\n",
    "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
    "\n",
    "        #Boolean value indicating whether to include all joints or only the movable joints.\n",
    "        self.include_movable_joints = include_movable_joints\n",
    "        \n",
    "        self.movable_joints = [0, 1, 2, 3, 6, 7, 8, 12, 13, 14, 15, 17, 18, 19, 25, 26, 27]\n",
    "    \n",
    "    def read_dataset(self):\n",
    "        try:\n",
    "            #Contains all the different activity vectors\n",
    "            activity_vector = {}\n",
    "            \n",
    "            #Contains the overall dataset\n",
    "            sampled_data = None\n",
    "            sampled_labels = None\n",
    "            \n",
    "            #Based on dimensions, which folder to use for extracting the dataset files\n",
    "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
    "            \n",
    "            #Checking if the dataset path is valid\n",
    "            if not os.path.exists(self.dir_path):\n",
    "                print('The Data Directory Does not Exist!')\n",
    "                return None\n",
    "\n",
    "            #Iterating over all the subject folders\n",
    "            for fld in self.internal_folders:\n",
    "                #Iterating for each file in the specified folder\n",
    "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
    "                    #Extracting the activity from the filename\n",
    "                    activity = self.__extract_activity(file)\n",
    "                    \n",
    "                    if activity not in self.classes:\n",
    "                        continue\n",
    "                    \n",
    "                    #Reading the CSV file using Pandas\n",
    "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
    "\n",
    "                    #Formulating the activity vector using one hot encoding\n",
    "                    if activity not in activity_vector:\n",
    "                        total_keys = len(activity_vector.keys())\n",
    "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
    "                        activity_vector[activity][total_keys] = 1\n",
    "                    vector = activity_vector[activity]\n",
    "                    \n",
    "                    #Sampling the dataset\n",
    "                    grouped_sample, grouped_activity = self.__group_samples(data, self.sample_size, vector)\n",
    "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
    "                    sampled_labels = grouped_activity if sampled_labels is None else np.append(sampled_labels, grouped_activity, axis=0)\n",
    "            \n",
    "            #Changing the Datatype\n",
    "            sampled_data = sampled_data.astype(self.datatype)\n",
    "            \n",
    "            #To make the data divisible for batch size\n",
    "            total_batches = sampled_data.shape[0]\n",
    "            sampled_data = sampled_data[:total_batches - (total_batches % self.batch_size)]\n",
    "            sampled_labels = sampled_labels[:total_batches - (total_batches % self.batch_size)]\n",
    "            \n",
    "            if self.return_action_labels:\n",
    "                return sampled_data, sampled_labels\n",
    "            \n",
    "            return sampled_data\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __extract_activity(self, filename):\n",
    "        try:\n",
    "            #Extracting the filename and excluding the extension\n",
    "            name = os.path.splitext(filename)[0]\n",
    "            \n",
    "            #Substituting the empty string with characters other than english alphabets\n",
    "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
    "            return activity\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    \n",
    "    def __group_samples(self, dataset, sample_size, activity):\n",
    "        try:\n",
    "            #Checking if the dataset is a Pandas Dataframe\n",
    "            if not isinstance(dataset, pd.DataFrame):\n",
    "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
    "                return None\n",
    "            \n",
    "            if self.include_movable_joints:\n",
    "                joints = list(chain.from_iterable((jt*2, (jt*2)+1) for jt in self.movable_joints))\n",
    "                dataset = dataset.iloc[: , joints].copy()\n",
    "\n",
    "            #Appending activity class to each row in the dataset\n",
    "            if self.include_action_labels:\n",
    "                dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
    "            \n",
    "            #Reshaping the dataset into sample batches\n",
    "            total_samples = dataset.shape[0]//sample_size\n",
    "            total_features = dataset.shape[1]\n",
    "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
    "            \n",
    "            grouped_activity = np.tile(activity, (dataset.shape[0]//self.sample_size, 1))\n",
    "            grouped_activity = grouped_activity[:total_samples*self.sample_size].reshape((-1, len(activity)))\n",
    "            \n",
    "            return grouped_rows, grouped_activity\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to split Dataset into Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8nABIG0rnxl_"
   },
   "outputs": [],
   "source": [
    "def split_to_features_labels(dataset, input_sequance_size=10, total_features=64):\n",
    "    \"\"\"\n",
    "    Function for splitting the data into features(with sequance size=iput_sequance_size)\n",
    "    and labels which should be the remainder of the sample length \n",
    "    \"\"\"\n",
    "    assert input_sequance_size < dataset.shape[1], f\"input sequence should be smaller than the total sample size\"\n",
    "    \n",
    "    #Dividing the dataset into features and labels by splitting the Time Frame Dimension\n",
    "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
    "    labels = dataset[:,np.s_[input_sequance_size:], :total_features]\n",
    "    \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5HkmNJJSKg3"
   },
   "source": [
    "### A function for downsampling the dataset on number of frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wNJmfjiGSDY2"
   },
   "outputs": [],
   "source": [
    "def downsampling(sampled_data, downsample_technique = 'skip'):\n",
    "    \"\"\"\n",
    "    The function used to down-sample the data using two different techniques. In Skip, one frame is skipped consecutively and\n",
    "    in the mean technique, two frames are averaged consecutively.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert downsample_technique in ['skip', 'mean'], f\"Only Skip and Mean techniques are available\"\n",
    "    \n",
    "    #Creating an empty variable to store Downsampled data when the technique is Mean\n",
    "    samples_per_batch = int(sampled_data.shape[1] / 2)\n",
    "    total_features = sampled_data.shape[2]\n",
    "    downsampled_data = np.empty(shape=(0, samples_per_batch, total_features))\n",
    "    \n",
    "    #In Skip technique, we skip 2 frames consecutively.\n",
    "    if downsample_technique == 'skip':\n",
    "        downsampled_data = sampled_data[:,::2,:]\n",
    "    else:\n",
    "        #Iterating over batches\n",
    "        for batch in sampled_data:\n",
    "    \n",
    "            averaged_batch = np.empty(shape=(0, total_features))\n",
    "    \n",
    "            #In each iteration, averaging 2 Frames and appending it to the variable\n",
    "            for i in range(0, batch.shape[0], 2):\n",
    "                averaged_batch = np.append(averaged_batch, np.mean(batch[i:i+2, :], axis = 0).reshape((1, total_features)), axis = 0)\n",
    "            \n",
    "            #Appending the whole batched averaged downsampled data to the new variable created before\n",
    "            downsampled_data = np.append(downsampled_data, averaged_batch.reshape((1, samples_per_batch, total_features)), axis = 0)\n",
    "    \n",
    "    return downsampled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLpG7OlQSUjW"
   },
   "source": [
    "### Adding more preprocessing steps (Normalization and gussian noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VzjeqMm1KNjQ"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(sampled_dataX, sampled_dataY, normalize=True, add_noise=True\n",
    "                    , stddev=0.05) :\n",
    "    \"\"\"\n",
    "    Function to preprocess data by normalizing input features and adding guassian\n",
    "    noise to increase model robustness\n",
    "    \"\"\"  \n",
    "    if normalize :\n",
    "        sampled_dataX =  tf.keras.utils.normalize(sampled_dataX, axis=2)\n",
    "    \n",
    "    if add_noise :\n",
    "        guassian_noise_layer = tf.keras.layers.GaussianNoise(stddev=stddev)\n",
    "        sampled_dataX = guassian_noise_layer(sampled_dataX)\n",
    "    \n",
    "    return sampled_dataX, sampled_dataY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwCrXubfJka4"
   },
   "source": [
    "## Defining different components of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ccVrCxin9KA"
   },
   "source": [
    "### DLinear Model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "7PIJXvLaaEvY"
   },
   "outputs": [],
   "source": [
    "class SeriesDecomp():\n",
    "    \"\"\"\n",
    "    Series decomposition into seasonal and tren\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "    def forward(self, x): \n",
    "        x_tf = tf.transpose(x, [0, 2, 1])\n",
    "        \n",
    "        avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=self.kernel_size, strides=1, padding='same')\n",
    "        trend = tf.transpose(avg_pool_1d(x_tf), [0, 2, 1])\n",
    "        return trend, tf.subtract(x, tf.cast(trend, x.dtype))\n",
    "    \n",
    "class Dlinear(Model_):\n",
    "\n",
    "    def __init__(self,  output_len=200):\n",
    "        super(Dlinear, self).__init__()\n",
    "       \n",
    "        self.output_len = output_len\n",
    "        # Decompsition Kernel Size\n",
    "        kernel_size = 25\n",
    "        \n",
    "        self.decomp = SeriesDecomp(kernel_size)\n",
    "        # season layer\n",
    "        self.Linear_Seasonal = tf.keras.models.Sequential([tf.keras.layers.Dense(self.output_len)])\n",
    "        # trend layer\n",
    "        self.Linear_Trend = tf.keras.models.Sequential([tf.keras.layers.Dense(self.output_len)])\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        '''\n",
    "        :param inputs: \n",
    "        :return: prediction output\n",
    "        '''\n",
    "        seasonal_init, trend_init = self.decomp.forward(inputs)\n",
    "        seasonal_init = tf.transpose(seasonal_init, [0, 2, 1])\n",
    "        trend_init = tf.transpose(trend_init, [0, 2, 1])\n",
    "        \n",
    "        # model linear layers for seasonal and trend components\n",
    "        seasonal_output = self.Linear_Seasonal(seasonal_init)\n",
    "        trend_output = self.Linear_Trend(trend_init)\n",
    "        \n",
    "        # adding both components prediction\n",
    "        final_output = seasonal_output + trend_output\n",
    "        return tf.transpose(final_output, [0, 2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining different Types of Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bVUFJsv3fVYh"
   },
   "outputs": [],
   "source": [
    "class Loss() :\n",
    "    \"\"\"\n",
    "    Joint loss class with two weight attributes for two different losses\n",
    "    first one is the loss joint and the second is the loss_motion_flow\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, lambda1=0.5, lambda2=0.5) :\n",
    "        self.lambda1 = lambda1\n",
    "        self.lambda2 = lambda2\n",
    "\n",
    "    def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        \"\"\"\n",
    "        Loss between the joint positions and its corresponding counterparts in the groundtruth\n",
    "        \"\"\"\n",
    "        diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
    "        return tf.reduce_sum(diff_norm_2, axis=1) \n",
    "\n",
    "    def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
    "        \"\"\"\n",
    "        Loss between the motion flow of predicted sequance and the ground truth\n",
    "        where the motion flow is the euclidean distance between each two consecutive frames\n",
    "        \"\"\"\n",
    "        predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
    "        real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
    "        prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
    "        return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
    "\n",
    "    def total_loss(self, target_sequance_batch, predicted_sequance_batch) :\n",
    "        \"\"\"\n",
    "        calculating the total loss through a combination of the joint_loss and motion_flow_loss\n",
    "        \"\"\"\n",
    "        joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
    "        motion_flow_loss = self.loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
    "        return self.lambda1*joints_loss + self.lambda2*motion_flow_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining different types of Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics():\n",
    "    \"\"\"\n",
    "    A class containing different types of Evaluation Metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mmd_kernel='gaussian') :\n",
    "        self.mmd_kernel = mmd_kernel\n",
    "    \n",
    "    def custom_sequence_MMD_loss(self, target_sequance_batch, predicted_sequance_batch):\n",
    "        \"\"\"\n",
    "        Calculating the Sequence MMD Loss between prediction and the ground Truth.\n",
    "        Additionally combining the last two dimensions \n",
    "        \"\"\"\n",
    "        mmd_loss = MMD.MMDLoss(kernel=self.mmd_kernel)\n",
    "        \n",
    "        total_batches = predicted_sequance_batch.shape[0]\n",
    "        frames_per_batch = predicted_sequance_batch.shape[1] * predicted_sequance_batch.shape[2]\n",
    "        \n",
    "        return mmd_loss(tf.reshape(predicted_sequance_batch, [total_batches, frames_per_batch]),\n",
    "                        tf.reshape(target_sequance_batch, [total_batches, frames_per_batch]))\n",
    "        \n",
    "    def MPJPE2(self, y_true, y_pred, number_of_joints = 32):\n",
    "        \"\"\"\n",
    "        Calculating the Mean Per Joint Position Error (MPJPE) between prediction and the ground Truth.\n",
    "        \"\"\"\n",
    "        yt= y_true.reshape((-1,number_of_joints,2))\n",
    "        yp= y_pred.reshape((-1,number_of_joints,2))\n",
    "        dist= np.zeros(yt.shape[0])\n",
    "        \n",
    "        for i in range(yt.shape[0]):\n",
    "            dist[i] = np.linalg.norm(yt[i] - yp[i])\n",
    "        \n",
    "        return np.mean(dist)\n",
    "    \n",
    "    def NPSS(self, euler_gt_sequences, euler_pred_sequences):\n",
    "        \"\"\"\n",
    "        A function to compute the Normalized Power Spectrum Similarity (NPSS) metric between predictions and the ground Truth [2] and [3].\n",
    "        \"\"\"        \n",
    "        # computing 1) fourier coeffs 2)power of fft 3) normalizing power of fft dim-wise 4) cumsum over freq. 5) EMD \n",
    "        gt_fourier_coeffs = np.zeros(euler_gt_sequences.shape, dtype = 'complex_')\n",
    "        pred_fourier_coeffs = np.zeros(euler_pred_sequences.shape, dtype = 'complex_')\n",
    "\n",
    "        # power vars\n",
    "        gt_power = np.zeros((gt_fourier_coeffs.shape))\n",
    "        pred_power = np.zeros((gt_fourier_coeffs.shape))\n",
    "\n",
    "        # normalizing power vars\n",
    "        gt_norm_power = np.zeros(gt_fourier_coeffs.shape)\n",
    "        pred_norm_power = np.zeros(gt_fourier_coeffs.shape)\n",
    "\n",
    "        cdf_gt_power = np.zeros(gt_norm_power.shape)\n",
    "        cdf_pred_power = np.zeros(pred_norm_power.shape)\n",
    "\n",
    "        emd = np.zeros(cdf_pred_power.shape[0:3:2])\n",
    "\n",
    "        # used to store powers of feature_dims and sequences used for avg later\n",
    "        seq_feature_power = np.zeros(euler_gt_sequences.shape[0:3:2])\n",
    "        power_weighted_emd = 0\n",
    "\n",
    "        for s in range(euler_gt_sequences.shape[0]):\n",
    "\n",
    "            for d in range(euler_gt_sequences.shape[2]):\n",
    "                gt_fourier_coeffs[s,:,d] = np.fft.fft(euler_gt_sequences[s,:,d]) # slice is 1D array\n",
    "                pred_fourier_coeffs[s,:,d] = np.fft.fft(euler_pred_sequences[s,:,d])\n",
    "\n",
    "                # computing power of fft per sequence per dim\n",
    "                gt_power[s,:,d] = np.square(np.absolute(gt_fourier_coeffs[s,:,d]))\n",
    "                pred_power[s,:,d] = np.square(np.absolute(pred_fourier_coeffs[s,:,d]))\n",
    "\n",
    "                # matching power of gt and pred sequences\n",
    "                gt_total_power = np.sum(gt_power[s,:,d])\n",
    "                pred_total_power = np.sum(pred_power[s,:,d])\n",
    "                #power_diff = gt_total_power - pred_total_power\n",
    "\n",
    "                # adding power diff to zero freq of pred seq\n",
    "                #pred_power[s,0,d] = pred_power[s,0,d] + power_diff\n",
    "\n",
    "                # computing seq_power and feature_dims power \n",
    "                seq_feature_power[s,d] = gt_total_power\n",
    "\n",
    "                # normalizing power per sequence per dim\n",
    "                if gt_total_power != 0:\n",
    "                    gt_norm_power[s,:,d] = gt_power[s,:,d] / gt_total_power \n",
    "\n",
    "                if pred_total_power !=0:\n",
    "                    pred_norm_power[s,:,d] = pred_power[s,:,d] / pred_total_power\n",
    "\n",
    "                # computing cumsum over freq\n",
    "                cdf_gt_power[s,:,d] = np.cumsum(gt_norm_power[s,:,d]) # slice is 1D\n",
    "                cdf_pred_power[s,:,d] = np.cumsum(pred_norm_power[s,:,d])\n",
    "\n",
    "                # computing EMD \n",
    "                emd[s,d] = np.linalg.norm((cdf_pred_power[s,:,d] - cdf_gt_power[s,:,d]), ord=1)\n",
    "\n",
    "        # computing weighted emd (by sequence and feature powers)\n",
    "        power_weighted_emd = np.average(emd, weights=seq_feature_power) \n",
    "\n",
    "        return power_weighted_emd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function to start the experiment of training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "labA65KEakkZ"
   },
   "outputs": [],
   "source": [
    "def run_experiment_Dliner(sampled_dataX, sampled_dataY, learning_rate=0.002, lambda1=0.5,\n",
    "                   lambda2=0.5, use_mse=False, use_MMD=False, metrics=None, output_diminsion=64,\n",
    "                   batch_size=100, epochs=50, validation_split=0.2) :\n",
    "    \"\"\"\n",
    "    Method takes all hyperparameters as input paramters and returns the model and history as\n",
    "    a result\n",
    "    \"\"\"\n",
    "\n",
    "    dliner_model = Dlinear(output_diminsion)\n",
    "\n",
    "    if use_mse :\n",
    "        loss_function = tf.keras.losses.mean_squared_error\n",
    "    elif use_MMD :\n",
    "        loss_function = Loss().custom_sequence_MMD_loss\n",
    "    else :\n",
    "        loss_function = Loss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
    "\n",
    "    dliner_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), loss=loss_function,metrics=metrics, run_eagerly=False)\n",
    "    with tf.device('/gpu:0'):\n",
    "        history = dliner_model.fit(sampled_dataX, sampled_dataY, batch_size=batch_size, \n",
    "                               epochs=epochs, validation_split=validation_split)\n",
    "    return history, dliner_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to visualize certain frames from the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gif(lenght = 10, path_to_save = ''):\n",
    "    \"\"\"\n",
    "    Function to create and save a GIF from different number of frames\n",
    "    \"\"\"\n",
    "    list=[]\n",
    "    for l in range(lenght):\n",
    "        list.append(f'{path_to_save}_frame{l}.png')\n",
    "\n",
    "    with imageio.get_writer(f'{path_to_save}.gif', mode='I', duration=0.1) as writer:\n",
    "        for filename in list:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "CIHR8VGhPfHf"
   },
   "outputs": [],
   "source": [
    "def visualize_frames(sample, dynamic_joints_only=False, num_frames_to_visualize=10, \n",
    "                    path_to_save=\"\", save_gif=False,\n",
    "                    joints_to_ignore=[4,5,9,10,11,16,20,21,22,23,24,28,29,30,31]) :\n",
    "    \"\"\"\n",
    "    Visualization function to draw a certain number of frames in a given sample\n",
    "    ignoring the joints mentioned in joints_to_ignore array\n",
    "    \"\"\"\n",
    "    assert num_frames_to_visualize <= sample.shape[0], f\"number of frames should be less than or equal to the total frames in the sample\"\n",
    "    \n",
    "    fig, axs = plt.subplots(ncols=num_frames_to_visualize, figsize=(40, 10))\n",
    "    fig.tight_layout(pad=1.0)\n",
    "    \n",
    "    for t in range(num_frames_to_visualize) :\n",
    "        #Removing unnecessary joints for visualization\n",
    "        if(dynamic_joints_only) :\n",
    "            #Check if no joints needs to be removed\n",
    "            truncated_frame = sample[t]\n",
    "        else :\n",
    "            #Removing the joints based on joints_to_ignore\n",
    "            joints_to_ignore_2d = [element * 2 for element in joints_to_ignore]\n",
    "            for i in range(len(joints_to_ignore_2d)) :\n",
    "                joints_to_ignore_2d.append(joints_to_ignore_2d[i]+1)\n",
    "            truncated_frame = np.delete(sample[t], joints_to_ignore_2d)   \n",
    "\n",
    "        #In case of including only moving joints for Human3.6M(17 joints)      \n",
    "        x_axis_array = truncated_frame[0:34:2]\n",
    "        y_axis_array = truncated_frame[1:35:2]\n",
    "        #Scattering all the 17 joints\n",
    "        axs[t].scatter(x_axis_array, y_axis_array)\n",
    "        #Plotting right leg\n",
    "        axs[t].plot(x_axis_array[:4], y_axis_array[:4], \"tab:blue\")\n",
    "        #plotting left leg\n",
    "        axs[t].plot(x_axis_array[[0, 4, 5, 6]], y_axis_array[[0, 4, 5, 6]])\n",
    "        #plotting from hip to head\n",
    "        axs[t].plot(x_axis_array[[0, 7, 8, 9, 10]], y_axis_array[[0, 7, 8, 9, 10]])\n",
    "        #plotting from neck to left shoulder\n",
    "        axs[t].plot(x_axis_array[[9, 11, 12, 13]], y_axis_array[[9, 11, 12, 13]])\n",
    "        #plotting from neck to right shoulder\n",
    "        axs[t].plot(x_axis_array[[9, 14, 15, 16]], y_axis_array[[9, 14, 15, 16]])\n",
    "        axs[t].invert_yaxis()\n",
    "        axs[t].set_xticks([])\n",
    "        axs[t].set_yticks([])\n",
    "        \n",
    "        if(len(path_to_save) > 0) :\n",
    "            extent = axs[t].get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "            fig.savefig(f'{path_to_save}_frame{t}.png', bbox_inches=extent)\n",
    "    \n",
    "    if save_gif==True:\n",
    "        create_gif(lenght=num_frames_to_visualize,path_to_save=path_to_save)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the whole dataset for shortterm predictions\n",
    "sampled_data_short_term = Dataset_loading('data\\H3.6csv', sample_size=20, include_movable_joints=False).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading only the movable dataset points for shortterm predictions\n",
    "sampled_data_short_term_movable = Dataset_loading('data\\H3.6csv', sample_size=20, include_movable_joints=True).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "f8PgK1UVnkrw"
   },
   "outputs": [],
   "source": [
    "#For long term prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
    "sampled_data_all = Dataset_loading('data\\H3.6csv', sample_size=60, include_movable_joints=False).read_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For long term movable joints prediction, we need a sample size of 60(10 frames input sequance, 50 frames predicted sequance)\n",
    "sampled_data_movable = Dataset_loading('data\\H3.6csv', sample_size=60, include_movable_joints=True).read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Dataset for Action Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data_classifier, sampled_labels_classifier = Dataset_loading('data\\H3.6csv', sample_size=60, include_movable_joints=False, include_action_labels=False, return_action_labels=True).read_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "h6bSQTwdn2Fo"
   },
   "outputs": [],
   "source": [
    "sampled_dataX_all, sampled_dataY_all = split_to_features_labels(sampled_data_all, input_sequance_size=10, total_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_movable, sampled_dataY_movable = split_to_features_labels(sampled_data_movable, input_sequance_size=10, total_features=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_short_term, sampled_dataY_short_term = split_to_features_labels(sampled_data_short_term, input_sequance_size=10, total_features=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataX_short_term_movable, sampled_dataY_short_term_movable = split_to_features_labels(sampled_data_short_term_movable, input_sequance_size=10, total_features=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with all features(short term)\n",
      "Total Samples: 77140\n",
      "Total Frames: 10\n",
      "Total Features: 64\n"
     ]
    }
   ],
   "source": [
    "print('Dataset with all features(short term)')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_short_term.shape[0],\n",
    "                                                                       sampled_dataY_short_term.shape[1],\n",
    "                                                                       sampled_dataY_short_term.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with Movable features(short term)\n",
      "Total Samples: 77140\n",
      "Total Frames: 10\n",
      "Total Features: 34\n"
     ]
    }
   ],
   "source": [
    "print('Dataset with Movable features(short term)')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_short_term_movable.shape[0],\n",
    "                                                                       sampled_dataY_short_term_movable.shape[1],\n",
    "                                                                       sampled_dataY_short_term_movable.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CVXQiscVn4O2",
    "outputId": "dffaa998-a452-4358-d205-9754186aba0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with all features\n",
      "Total Samples: 25520\n",
      "Total Frames: 50\n",
      "Total Features: 64\n"
     ]
    }
   ],
   "source": [
    "print('Dataset with all features')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_all.shape[0],\n",
    "                                                                       sampled_dataY_all.shape[1],\n",
    "                                                                       sampled_dataY_all.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with only movable features\n",
      "Total Samples: 25520\n",
      "Total Frames: 50\n",
      "Total Features: 34\n"
     ]
    }
   ],
   "source": [
    "print('Dataset with only movable features')\n",
    "print('Total Samples: {}\\nTotal Frames: {}\\nTotal Features: {}'.format(sampled_dataY_movable.shape[0],\n",
    "                                                                       sampled_dataY_movable.shape[1],\n",
    "                                                                       sampled_dataY_movable.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snGEPtNOKLvg"
   },
   "source": [
    "### Adding Noise and Downsampling to improve model performance and robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed and Downsampled Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Z9Bkt02CSh5P"
   },
   "outputs": [],
   "source": [
    "downsampled_data_all = downsampling(sampled_data_all, 'skip')\n",
    "downsampled_data_movable = downsampling(sampled_data_movable, 'skip')\n",
    "downsampled_data_shortterm_movable = downsampling(sampled_data_short_term_movable, 'skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "bsR__kGeSmNG"
   },
   "outputs": [],
   "source": [
    "downsampled_dataX_all, downsampled_dataY_all = split_to_features_labels(downsampled_data_all, input_sequance_size=10, total_features=64)\n",
    "preprocessed_downsampled_dataX_all, preprocessed_downsampled_dataY_all = preprocess_data(downsampled_dataX_all, downsampled_dataY_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataX_movable, downsampled_dataY_movable = split_to_features_labels(downsampled_data_movable, input_sequance_size=10, total_features=34)\n",
    "preprocessed_downsampled_dataX_movable, preprocessed_downsampled_dataY_movable = preprocess_data(downsampled_dataX_movable, downsampled_dataY_movable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dataX_shortterm_movable, downsampled_dataX_shortterm_movable = split_to_features_labels(downsampled_data_shortterm_movable, input_sequance_size=5, total_features=34)\n",
    "preprocessed_downsampled_dataX_shortterm_movable, preprocessed_downsampled_dataY_shortterm_movable = preprocess_data(downsampled_dataX_shortterm_movable, downsampled_dataX_shortterm_movable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_downsampled_dataX_movable1, preprocessed_downsampled_dataY_movable1 = preprocess_data(downsampled_dataX_movable, downsampled_dataY_movable, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az25davkJq9Y"
   },
   "source": [
    "### Running experiments with different hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DLinear Base Implementation Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment on Base Paper with Downsampled movable datapoints and MSE loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2552/2552 [==============================] - 7s 3ms/step - loss: 4263.6948 - mean_absolute_percentage_error: 4.8933\n",
      "Epoch 2/50\n",
      "2552/2552 [==============================] - 7s 3ms/step - loss: 619.1418 - mean_absolute_percentage_error: 2.9817\n",
      "Epoch 3/50\n",
      "2552/2552 [==============================] - 7s 3ms/step - loss: 547.6927 - mean_absolute_percentage_error: 2.8138\n",
      "Epoch 4/50\n",
      "2552/2552 [==============================] - 6s 2ms/step - loss: 494.9692 - mean_absolute_percentage_error: 2.6363\n",
      "Epoch 5/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 457.0446 - mean_absolute_percentage_error: 2.5028\n",
      "Epoch 6/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 427.4623 - mean_absolute_percentage_error: 2.3870\n",
      "Epoch 7/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 408.5404 - mean_absolute_percentage_error: 2.3366\n",
      "Epoch 8/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 395.5079 - mean_absolute_percentage_error: 2.3115\n",
      "Epoch 9/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 386.6317 - mean_absolute_percentage_error: 2.3118\n",
      "Epoch 10/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 380.3552 - mean_absolute_percentage_error: 2.3033\n",
      "Epoch 11/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 374.4729 - mean_absolute_percentage_error: 2.2929\n",
      "Epoch 12/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 371.5479 - mean_absolute_percentage_error: 2.2960\n",
      "Epoch 13/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 367.7514 - mean_absolute_percentage_error: 2.2891\n",
      "Epoch 14/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 365.7180 - mean_absolute_percentage_error: 2.2906\n",
      "Epoch 15/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 364.0795 - mean_absolute_percentage_error: 2.2935\n",
      "Epoch 16/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 361.7592 - mean_absolute_percentage_error: 2.2844\n",
      "Epoch 17/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 360.0824 - mean_absolute_percentage_error: 2.2798\n",
      "Epoch 18/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 359.0601 - mean_absolute_percentage_error: 2.2790\n",
      "Epoch 19/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 358.9379 - mean_absolute_percentage_error: 2.2913\n",
      "Epoch 20/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 357.9139 - mean_absolute_percentage_error: 2.2875\n",
      "Epoch 21/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 355.7883 - mean_absolute_percentage_error: 2.2721\n",
      "Epoch 22/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 355.3568 - mean_absolute_percentage_error: 2.2797\n",
      "Epoch 23/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 353.7609 - mean_absolute_percentage_error: 2.2687\n",
      "Epoch 24/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 353.1696 - mean_absolute_percentage_error: 2.2681\n",
      "Epoch 25/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 352.0346 - mean_absolute_percentage_error: 2.2619\n",
      "Epoch 26/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 351.6336 - mean_absolute_percentage_error: 2.2666\n",
      "Epoch 27/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 351.4734 - mean_absolute_percentage_error: 2.2660\n",
      "Epoch 28/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 350.1508 - mean_absolute_percentage_error: 2.2627\n",
      "Epoch 29/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 349.2777 - mean_absolute_percentage_error: 2.2579\n",
      "Epoch 30/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 348.3636 - mean_absolute_percentage_error: 2.2489\n",
      "Epoch 31/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 347.6223 - mean_absolute_percentage_error: 2.2487\n",
      "Epoch 32/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 347.4578 - mean_absolute_percentage_error: 2.2522\n",
      "Epoch 33/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 347.3586 - mean_absolute_percentage_error: 2.2535\n",
      "Epoch 34/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 346.1552 - mean_absolute_percentage_error: 2.2469\n",
      "Epoch 35/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 345.6617 - mean_absolute_percentage_error: 2.2413\n",
      "Epoch 36/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 345.2133 - mean_absolute_percentage_error: 2.2412\n",
      "Epoch 37/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 344.1462 - mean_absolute_percentage_error: 2.2360\n",
      "Epoch 38/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 344.0602 - mean_absolute_percentage_error: 2.2381\n",
      "Epoch 39/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 343.5959 - mean_absolute_percentage_error: 2.2365\n",
      "Epoch 40/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 342.2164 - mean_absolute_percentage_error: 2.2290\n",
      "Epoch 41/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 341.5679 - mean_absolute_percentage_error: 2.2240\n",
      "Epoch 42/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 340.8010 - mean_absolute_percentage_error: 2.2208\n",
      "Epoch 43/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 341.6783 - mean_absolute_percentage_error: 2.2313\n",
      "Epoch 44/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 340.5146 - mean_absolute_percentage_error: 2.2225\n",
      "Epoch 45/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 339.5876 - mean_absolute_percentage_error: 2.2140\n",
      "Epoch 46/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 339.2652 - mean_absolute_percentage_error: 2.2101\n",
      "Epoch 47/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 339.3853 - mean_absolute_percentage_error: 2.2200\n",
      "Epoch 48/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 338.8959 - mean_absolute_percentage_error: 2.2169\n",
      "Epoch 49/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 339.2634 - mean_absolute_percentage_error: 2.2208\n",
      "Epoch 50/50\n",
      "2552/2552 [==============================] - 5s 2ms/step - loss: 339.2697 - mean_absolute_percentage_error: 2.2267\n"
     ]
    }
   ],
   "source": [
    "history_model14, dLinear_pred = run_experiment_Dliner(preprocessed_downsampled_dataX_movable1[:,:,:-10], preprocessed_downsampled_dataY_movable1 ,\n",
    "                                                        batch_size=10, epochs=50, use_mse=True, output_diminsion=20, validation_split=0.0, \n",
    "                                                        metrics=[tf.keras.losses.mean_absolute_percentage_error])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Experiment on Base Paper with Downsampled movable datapoints and Joint loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XaBTxF3Pbmo"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frames(dLinear_pred[4000], dynamic_joints_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_frames(preprocessed_downsampled_dataX_movable1[4000], dynamic_joints_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWZSPjKM9u-z"
   },
   "source": [
    "# References\n",
    "\n",
    "[1] https://github.com/una-dinosauria/3d-pose-baseline/blob/master/src/data_utils.py\n",
    "\n",
    "[2] Gopalakrishnan, Anand, et al. \"A neural temporal model for human motion prediction.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019\n",
    "\n",
    "[3] https://github.com/cr7anand/neural_temporal_models"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "KTHffB1KrfBy",
    "H-ekHZVlrq1o",
    "hQg05uuoRtEi",
    "nozip8IVhhkg"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
