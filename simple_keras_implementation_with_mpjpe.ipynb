{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IbramMedhat/Human-Motion-Synthesis/blob/main/simple_keras_implementation_with_mpjpe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfc1OMd0WeGG",
        "outputId": "28bc688c-af24-4375-da98-50fd6d370969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.8\n",
        "import numpy as np\n",
        "from tensorflow.keras import Model as Model_\n",
        "from tensorflow.keras.layers import Input, ReLU, LSTM, Dense, TimeDistributed\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "print(tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2VwrVR379Znr",
        "outputId": "d503fd30-837d-4e6e-b824-4333c0fc723e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.8.0%2Bzzzcolab20220506162203-cp37-cp37m-linux_x86_64.whl (668.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 668.3 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.0.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (4.1.1)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.8.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.2.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 31.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.27.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (57.4.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.49.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (22.9.24)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (14.0.6)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.1)\n",
            "Installing collected packages: tf-estimator-nightly, tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220929150707\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220929150707:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220929150707\n",
            "Successfully installed tensorflow-2.8.0+zzzcolab20220506162203 tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install tensorflow==2.8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kW9oi8rW67z2"
      },
      "outputs": [],
      "source": [
        "#!pip install tensorflow_model_remediation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09QZEu_wWkVa",
        "outputId": "ec8e5621-cc42-48cd-f038-840b98f9791c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# #Need only to be used with google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VJlcJTFQWsoI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "class Dataset_Preprocessing:\n",
        "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
        "        \n",
        "        #Dataset Directory path\n",
        "        self.dir_path = dir_path\n",
        "        \n",
        "        #Which Dimension file to include, possible values: 2 and 3\n",
        "        self.include_dimension = include_dimension\n",
        "        \n",
        "        #Total frames in one Sample\n",
        "        self.sample_size = sample_size\n",
        "        \n",
        "        #Activity classes to include\n",
        "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
        "        \n",
        "        #Total activity classes\n",
        "        self.total_classes = len(self.classes)\n",
        "        \n",
        "        #Subject Folders names in the Dataset\n",
        "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
        "    \n",
        "    def read_dataset(self):\n",
        "        try:\n",
        "            #Contains all the different activity vectors\n",
        "            activity_vector = {}\n",
        "            \n",
        "            #Contains the overall dataset\n",
        "            sampled_data = None\n",
        "            \n",
        "            #Based on dimensions, which folder to use for extracting the dataset files\n",
        "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
        "            \n",
        "            #Checking if the dataset path is valid\n",
        "            if not os.path.exists(self.dir_path):\n",
        "                print('The Data Directory Does not Exist!')\n",
        "                return None\n",
        "\n",
        "            #Iterating over all the subject folders\n",
        "            for fld in self.internal_folders:\n",
        "                #Iterating for each file in the specified folder\n",
        "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
        "                    #Extracting the activity from the filename\n",
        "                    activity = self.__extract_activity(file)\n",
        "                    \n",
        "                    if activity not in self.classes:\n",
        "                        continue\n",
        "                    \n",
        "                    #Reading the CSV file using Pandas\n",
        "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
        "\n",
        "                    #Formulating the activity vector using one hot encoding\n",
        "                    if activity not in activity_vector:\n",
        "                        total_keys = len(activity_vector.keys())\n",
        "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
        "                        activity_vector[activity][total_keys] = 1\n",
        "                    vector = activity_vector[activity]\n",
        "                    \n",
        "                    #Sampling the dataset\n",
        "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
        "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
        "                    \n",
        "            return sampled_data\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __extract_activity(self, filename):\n",
        "        try:\n",
        "            #Extracting the filename and excluding the extension\n",
        "            name = os.path.splitext(filename)[0]\n",
        "            \n",
        "            #Substituting the empty string with characters other than english alphabets\n",
        "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
        "            return activity\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __group_samples(self, dataset, sample_size, activity):\n",
        "        try:\n",
        "            #Checking if the dataset is a Pandas Dataframe\n",
        "            if not isinstance(dataset, pd.DataFrame):\n",
        "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
        "                return None\n",
        "            \n",
        "            #Appending activity class to each row in the dataset\n",
        "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
        "            \n",
        "            #Reshaping the dataset into sample batches\n",
        "            total_samples = dataset.shape[0]//sample_size\n",
        "            total_features = dataset.shape[1]\n",
        "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
        "            \n",
        "            return grouped_rows\n",
        "        except Exception as e:\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Eud56W84W80a"
      },
      "outputs": [],
      "source": [
        "#For short term prediction, we need a sample size of 20(10 frames input sequance, 10 frames predicted sequance)\n",
        "sampled_data = Dataset_Preprocessing('/content/drive/MyDrive/Colab Notebooks/H3.6csv', sample_size=20).read_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Eed-DrIHXCU4"
      },
      "outputs": [],
      "source": [
        "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
        "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
        "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
        "    labels = dataset[:,np.s_[input_sequance_size:], :64]\n",
        "    \n",
        "    return features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7TbKYYsfXEsn"
      },
      "outputs": [],
      "source": [
        "sampled_dataX, sampled_dataY = split_to_features_labels(sampled_data, input_sequance_size=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MbSHOacXHjf",
        "outputId": "c189d785-8610-43cc-b022-e37c74c54438"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Samples: 77144\n",
            "Total Frames: 10\n",
            "Total Features: 64\n"
          ]
        }
      ],
      "source": [
        "print('Total Samples: {}'.format(sampled_dataY.shape[0]))\n",
        "print('Total Frames: {}'.format(sampled_dataY.shape[1]))\n",
        "print('Total Features: {}'.format(sampled_dataY.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "s6YVjgYIJc9_"
      },
      "outputs": [],
      "source": [
        "class GloGen(Model_):\n",
        "    def __init__(self, enocder_hidden_state=200, decoder_hidden_state=200, output_diminsion=64, activation='relu'):\n",
        "        super(GloGen, self).__init__()\n",
        "        self.encoder = LSTM(enocder_hidden_state, return_state=True, return_sequences=True)\n",
        "        self.decoder = LSTM(decoder_hidden_state, return_sequences=True, return_state=True)\n",
        "        self.dense_layer = TimeDistributed(Dense(output_diminsion, activation=activation)) \n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoder_outputs, state_h, state_c = self.encoder(inputs)\n",
        "        encoder_states = [state_h, state_c]\n",
        "        output, _, _ = self.decoder(encoder_outputs, initial_state=encoder_states)\n",
        "        output = self.dense_layer(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LrOKkqY8J9eD"
      },
      "outputs": [],
      "source": [
        "# Define the model that will turn\n",
        "# `encoder_input_data` into `decoder_target_data`\n",
        "glogen_model = GloGen()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ocEBTNq1XTho"
      },
      "outputs": [],
      "source": [
        "class JointLoss() :\n",
        "  def __init__(self, lambda1=0.5, lambda2=0.5) :\n",
        "    self.lambda1 = lambda1\n",
        "    self.lambda2 = lambda2\n",
        "\n",
        "  def loss_joint(self, predicted_sequance_batch, target_sequance_batch) :\n",
        "      diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
        "      return tf.reduce_sum(diff_norm_2, axis=1) \n",
        "\n",
        "  def loss_motion_flow(self, predicted_sequance_batch, target_sequance_batch) :\n",
        "      predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
        "      real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
        "      prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
        "      return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
        "\n",
        "\n",
        "  def total_loss(self, target_sequance_batch, predicted_sequance_batch) :\n",
        "      joints_loss = self.loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
        "      motion_flow_loss = self.loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
        "      return self.lambda1*joints_loss + self.lambda2*motion_flow_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "IHiBCGcQ3vTw"
      },
      "outputs": [],
      "source": [
        "def run_experiment(learning_rate=0.002, lambda1=0.5, lambda2=0.5, metrics=None, batch_size=100, epochs=50, validation_split=0.2) :\n",
        "  glogen_model = GloGen()\n",
        "  loss_function = JointLoss(lambda1=lambda1, lambda2=lambda2).total_loss\n",
        "  glogen_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "                       loss=loss_function, metrics=metrics,run_eagerly=True)\n",
        "  history = glogen_model.fit(sampled_dataX, sampled_dataY,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=validation_split)\n",
        "  return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EwF6affW5tNn"
      },
      "outputs": [],
      "source": [
        "#history = run_experiment(epochs=10, lambda1=0.8, lambda2=0.2, metrics=['mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kw6vWAoBLRBQ"
      },
      "outputs": [],
      "source": [
        "def compute_kernel(predicted_sequance_batch, target_sequance_batch):\n",
        "    x_size = tf.shape(predicted_sequance_batch)[0]\n",
        "    y_size = tf.shape(target_sequance_batch)[0]\n",
        "    dim = tf.shape(predicted_sequance_batch)[1]\n",
        "    tiled_x = tf.tile(tf.reshape(predicted_sequance_batch, tf.stack([x_size, 1, dim])), tf.stack([1, y_size, 1]))\n",
        "    tiled_y = tf.tile(tf.reshape(target_sequance_batch, tf.stack([1, y_size, dim])), tf.stack([x_size, 1, 1]))\n",
        "    return tf.exp(-tf.reduce_mean(tf.square(tiled_x - tiled_y), axis=2) / tf.cast(dim, tf.float32))\n",
        "\n",
        "\n",
        "def MMD(predicted_sequance_batch, target_sequance_batch):\n",
        "    x_kernel = compute_kernel(predicted_sequance_batch, predicted_sequance_batch)\n",
        "    y_kernel = compute_kernel(target_sequance_batch, target_sequance_batch)\n",
        "    xy_kernel = compute_kernel(predicted_sequance_batch, target_sequance_batch)\n",
        "    return tf.reduce_mean(x_kernel) + tf.reduce_mean(y_kernel) - 2 * tf.reduce_mean(xy_kernel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "50Ify0yBJ-BX"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def MPJPE(y_true, y_pred):\n",
        "    return K.mean(K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dgirOiFdEQno"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "def MPJPE2(y_true, y_pred):\n",
        "    yt= y_true.numpy().reshape((-1,32,2))\n",
        "    yp= y_pred.numpy().reshape((-1,32,2))\n",
        "    dist= np.zeros(10)\n",
        "    for i in range(10):\n",
        "        dist[i] = np.linalg.norm(yt[i] - yp[i])\n",
        "    return np.mean(dist)\n",
        "\n",
        "def PJPE2(y_true, y_pred):\n",
        "    yt= y_true.numpy().reshape((10,64))\n",
        "    yp= y_pred.numpy().reshape((10,64))\n",
        "    dist= np.zeros(10)\n",
        "    for i in range(10):\n",
        "        dist[i] = np.linalg.norm(yt[i] - yp[i])\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6tEXjftGUpTf"
      },
      "outputs": [],
      "source": [
        "# for layer in glogen_model.layers:\n",
        "#     print(layer.output_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-UuKxWD_Dyq",
        "outputId": "d68dc403-f344-4731-b61e-155a111ea506"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "618/618 [==============================] - 31s 38ms/step - loss: 77599352.0000 - MPJPE2: 3014.2136 - val_loss: 43005436.0000 - val_MPJPE2: 2268.2800\n",
            "Epoch 2/10\n",
            "618/618 [==============================] - 23s 38ms/step - loss: 26647932.0000 - MPJPE2: 1703.2275 - val_loss: 15255072.0000 - val_MPJPE2: 1316.8422\n",
            "Epoch 3/10\n",
            "618/618 [==============================] - 24s 38ms/step - loss: 10848242.0000 - MPJPE2: 1097.5651 - val_loss: 8374320.5000 - val_MPJPE2: 973.1194\n",
            "Epoch 4/10\n",
            "618/618 [==============================] - 23s 37ms/step - loss: 7496513.0000 - MPJPE2: 904.0046 - val_loss: 7389505.0000 - val_MPJPE2: 917.6710\n",
            "Epoch 5/10\n",
            "618/618 [==============================] - 26s 41ms/step - loss: 7085911.5000 - MPJPE2: 867.8441 - val_loss: 7337661.5000 - val_MPJPE2: 914.9301\n",
            "Epoch 6/10\n",
            "618/618 [==============================] - 23s 37ms/step - loss: 7046922.0000 - MPJPE2: 895.0511 - val_loss: 7291641.5000 - val_MPJPE2: 912.6095\n",
            "Epoch 7/10\n",
            "618/618 [==============================] - 23s 37ms/step - loss: 7009970.5000 - MPJPE2: 880.2917 - val_loss: 7282020.0000 - val_MPJPE2: 912.3286\n",
            "Epoch 8/10\n",
            "618/618 [==============================] - 23s 37ms/step - loss: 6999975.0000 - MPJPE2: 883.6992 - val_loss: 7275798.5000 - val_MPJPE2: 912.0588\n",
            "Epoch 9/10\n",
            "618/618 [==============================] - 23s 37ms/step - loss: 6995717.0000 - MPJPE2: 890.1360 - val_loss: 7272069.5000 - val_MPJPE2: 911.8392\n",
            "Epoch 10/10\n",
            "618/618 [==============================] - 23s 37ms/step - loss: 6992987.0000 - MPJPE2: 879.4654 - val_loss: 7269227.5000 - val_MPJPE2: 911.6763\n"
          ]
        }
      ],
      "source": [
        "# from keras import backend as K\n",
        "# def compute_kernel(x, y):\n",
        "#     # x=tf.expand_dims(x, axis=-1)\n",
        "#     # y=tf.expand_dims(y, axis=-1)\n",
        "#     x_size = K.shape(x)[0] #77144\n",
        "#     y_size = K.shape(y)[0] #77144\n",
        "#     dim = K.shape(x)[1] #10\n",
        "#     #dim= 10,64\n",
        "#     tiled_x = K.tile(K.reshape(x, [x_size, 1, dim]), [1, y_size, 1])\n",
        "#     tiled_y = K.tile(K.reshape(y, [1, y_size, dim]), [x_size, 1, 1])\n",
        "#     return K.exp(-K.mean(K.square(tiled_x - tiled_y), axis=2) / K.cast(dim, 'float32'))\n",
        "\n",
        "# def compute_mmd(x, y):\n",
        "#     x_kernel = compute_kernel(x, x)\n",
        "#     y_kernel = compute_kernel(y, y)\n",
        "#     xy_kernel = compute_kernel(x, y)\n",
        "#     return K.mean(x_kernel) + K.mean(y_kernel) - 2 * K.mean(xy_kernel)\n",
        "# #Trying MMD loss instead\n",
        "history = run_experiment(epochs=10, lambda1=0.8, lambda2=0.2, metrics=[MPJPE2])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}