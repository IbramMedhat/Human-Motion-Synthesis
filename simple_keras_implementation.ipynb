{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfc1OMd0WeGG",
        "outputId": "ec96d007-7c4d-4029-d8b7-da7618a7bfdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Model as Model_\n",
        "from tensorflow.keras.layers import Input, PReLU, LeakyReLU, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, Conv2D, Flatten, Reshape, Conv1D, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Dense, BatchNormalization\n",
        "from tensorflow.keras.datasets import mnist, cifar10\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "print(tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_remediation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW9oi8rW67z2",
        "outputId": "b03dc439-a0eb-43e7-c785-4378a731e8fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_model_remediation\n",
            "  Downloading tensorflow_model_remediation-0.1.7.1-py3-none-any.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 27.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (1.3.5)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (0.3.5.1)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.7/dist-packages (from tensorflow_model_remediation) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (4.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.47.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (3.3.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (0.5.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (2.8.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (3.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.1.2)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.21.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (0.26.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (14.0.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (1.6.3)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0.0->tensorflow_model_remediation) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0.0->tensorflow_model_remediation) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0.0->tensorflow_model_remediation) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0.0->tensorflow_model_remediation) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_model_remediation) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->tensorflow_model_remediation) (2022.1)\n",
            "Installing collected packages: mock, tensorflow-model-remediation\n",
            "Successfully installed mock-4.0.3 tensorflow-model-remediation-0.1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "w09WoKXq1uih"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_remediation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09QZEu_wWkVa",
        "outputId": "46ec16db-6f16-4e22-ddbd-cbdef08a4769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Need only to be used with google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VJlcJTFQWsoI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "class Dataset_Preprocessing:\n",
        "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
        "        \n",
        "        #Dataset Directory path\n",
        "        self.dir_path = dir_path\n",
        "        \n",
        "        #Which Dimension file to include, possible values: 2 and 3\n",
        "        self.include_dimension = include_dimension\n",
        "        \n",
        "        #Total frames in one Sample\n",
        "        self.sample_size = sample_size\n",
        "        \n",
        "        #Activity classes to include\n",
        "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
        "        \n",
        "        #Total activity classes\n",
        "        self.total_classes = len(self.classes)\n",
        "        \n",
        "        #Subject Folders names in the Dataset\n",
        "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
        "    \n",
        "    def read_dataset(self):\n",
        "        try:\n",
        "            #Contains all the different activity vectors\n",
        "            activity_vector = {}\n",
        "            \n",
        "            #Contains the overall dataset\n",
        "            sampled_data = None\n",
        "            \n",
        "            #Based on dimensions, which folder to use for extracting the dataset files\n",
        "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
        "            \n",
        "            #Checking if the dataset path is valid\n",
        "            if not os.path.exists(self.dir_path):\n",
        "                print('The Data Directory Does not Exist!')\n",
        "                return None\n",
        "\n",
        "            #Iterating over all the subject folders\n",
        "            for fld in self.internal_folders:\n",
        "                #Iterating for each file in the specified folder\n",
        "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
        "                    #Extracting the activity from the filename\n",
        "                    activity = self.__extract_activity(file)\n",
        "                    \n",
        "                    if activity not in self.classes:\n",
        "                        continue\n",
        "                    \n",
        "                    #Reading the CSV file using Pandas\n",
        "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
        "\n",
        "                    #Formulating the activity vector using one hot encoding\n",
        "                    if activity not in activity_vector:\n",
        "                        total_keys = len(activity_vector.keys())\n",
        "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
        "                        activity_vector[activity][total_keys] = 1\n",
        "                    vector = activity_vector[activity]\n",
        "                    \n",
        "                    #Sampling the dataset\n",
        "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
        "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
        "                    \n",
        "            return sampled_data\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __extract_activity(self, filename):\n",
        "        try:\n",
        "            #Extracting the filename and excluding the extension\n",
        "            name = os.path.splitext(filename)[0]\n",
        "            \n",
        "            #Substituting the empty string with characters other than english alphabets\n",
        "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
        "            return activity\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __group_samples(self, dataset, sample_size, activity):\n",
        "        try:\n",
        "            #Checking if the dataset is a Pandas Dataframe\n",
        "            if not isinstance(dataset, pd.DataFrame):\n",
        "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
        "                return None\n",
        "            \n",
        "            #Appending activity class to each row in the dataset\n",
        "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
        "            \n",
        "            #Reshaping the dataset into sample batches\n",
        "            total_samples = dataset.shape[0]//sample_size\n",
        "            total_features = dataset.shape[1]\n",
        "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
        "            \n",
        "            return grouped_rows\n",
        "        except Exception as e:\n",
        "            print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Eud56W84W80a"
      },
      "outputs": [],
      "source": [
        "#For short term prediction, we need a sample size of 20(10 frames input sequance, 10 frames predicted sequance)\n",
        "sampled_data = Dataset_Preprocessing('/content/drive/MyDrive/Colab Notebooks/H3.6csv', sample_size=20).read_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmD7-mCDXAPI",
        "outputId": "850e7b52-ce0e-4b52-ae78-2c9f0258c1cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77144, 20, 74)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "sampled_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Eed-DrIHXCU4"
      },
      "outputs": [],
      "source": [
        "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
        "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
        "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
        "    labels = dataset[:,np.s_[input_sequance_size:], :64]\n",
        "    \n",
        "    return features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7TbKYYsfXEsn"
      },
      "outputs": [],
      "source": [
        "sampled_dataX, sampled_dataY = split_to_features_labels(sampled_data, input_sequance_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MbSHOacXHjf",
        "outputId": "6429dfec-8750-4873-e27c-631858f31b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Samples: 77144\n",
            "Total Frames: 10\n",
            "Total Features: 64\n"
          ]
        }
      ],
      "source": [
        "print('Total Samples: {}'.format(sampled_dataY.shape[0]))\n",
        "print('Total Frames: {}'.format(sampled_dataY.shape[1]))\n",
        "print('Total Features: {}'.format(sampled_dataY.shape[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FWYdj9HqXKIY"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, 74))\n",
        "encoder = LSTM(200, return_state=True, return_sequences=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, 200))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(200, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(64, activation='relu')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ocEBTNq1XTho"
      },
      "outputs": [],
      "source": [
        "def loss_joint(predicted_sequance_batch, target_sequance_batch) :\n",
        "    diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
        "    return tf.reduce_sum(diff_norm_2, axis=1) \n",
        "\n",
        "def loss_motion_flow(predicted_sequance_batch, target_sequance_batch) :\n",
        "    predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
        "    real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
        "    prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
        "    return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
        "\n",
        "\n",
        "def total_loss(target_sequance_batch, predicted_sequance_batch) :\n",
        "    joints_loss = loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
        "    motion_flow_loss = loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
        "    return 0.5*joints_loss + 0.5*motion_flow_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 536
        },
        "id": "TDDEHtklXXZ4",
        "outputId": "26772d38-71c7-4b4c-e5d2-4c5dc28a8c4f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "InternalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-661cbcba4712>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Whatever loss that accepts true values and predictions can be used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorflow_model_remediation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_diff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMMDLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"laplacian\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit([sampled_dataX, encoder(sampled_dataX)[0]], sampled_dataY,\n\u001b[0m\u001b[1;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           epochs=10)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Exception encountered when calling layer \"lstm\" (type LSTM).\n\nFailed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 74, 200, 1, 10, 77144, 200]  [Op:CudnnRNN]\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(77144, 10, 74), dtype=float32)\n  • mask=None\n  • training=None\n  • initial_state=None"
          ]
        }
      ],
      "source": [
        "#Whatever loss that accepts true values and predictions can be used\n",
        "model.compile(optimizer='adam', loss=total_loss, metrics=[tensorflow_model_remediation.min_diff.losses.MMDLoss(kernel=\"laplacian\")])\n",
        "history = model.fit([sampled_dataX, encoder(sampled_dataX)[0]], sampled_dataY,\n",
        "          batch_size=100,\n",
        "          epochs=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.predict([sampled_dataX, encoder(sampled_dataX)[0]]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0H7wS7_p_0CU",
        "outputId": "4f7dae1c-54ad-4115-ba42-de4264c54eb2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77144, 10, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dataY.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KCGjKcpAiWz",
        "outputId": "b892f619-0054-4488-cf49-76dae14e7a2e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77144, 10, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8rX4vJlbYLd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53b4e56d-6921-44d6-8713-138b651dfb61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': [0.024643927812576294,\n",
              "  0.0598541684448719,\n",
              "  0.0598541684448719,\n",
              "  0.0598541684448719,\n",
              "  0.0598541684448719,\n",
              "  0.0598541684448719,\n",
              "  0.0598541684448719,\n",
              "  0.061210401356220245,\n",
              "  0.08420481532812119,\n",
              "  0.0946350172162056],\n",
              " 'loss': [61417832.0,\n",
              "  37850928.0,\n",
              "  23338742.0,\n",
              "  14991470.0,\n",
              "  10690170.0,\n",
              "  8801425.0,\n",
              "  8113604.0,\n",
              "  7633512.5,\n",
              "  6673252.0,\n",
              "  5815005.0],\n",
              " 'val_accuracy': [0.02505023032426834,\n",
              "  0.02505023032426834,\n",
              "  0.02505023032426834,\n",
              "  0.02505023032426834,\n",
              "  0.02505023032426834,\n",
              "  0.02505023032426834,\n",
              "  0.02505023032426834,\n",
              "  0.026573335751891136,\n",
              "  0.04958195611834526,\n",
              "  0.0511309877038002],\n",
              " 'val_loss': [47519920.0,\n",
              "  29205120.0,\n",
              "  18277972.0,\n",
              "  12349246.0,\n",
              "  9546026.0,\n",
              "  8474323.0,\n",
              "  7995638.0,\n",
              "  7606455.5,\n",
              "  6257934.0,\n",
              "  5380754.0]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M-UuKxWD_Dyq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "simple_keras_implementation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}