{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simple_keras_implementation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gfc1OMd0WeGG",
        "outputId": "77e46263-33c0-4f0a-dd22-b209101820e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras import Model as Model_\n",
        "from tensorflow.keras.layers import Input, PReLU, LeakyReLU, MaxPooling2D, Dropout, concatenate, UpSampling2D, ReLU, Conv2D, Flatten, Reshape, Conv1D, LSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.models import Sequential\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2DTranspose, Dense, BatchNormalization\n",
        "from tensorflow.keras.datasets import mnist, cifar10\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
        "\n",
        "print(tf.keras.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Need only to be used with google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09QZEu_wWkVa",
        "outputId": "d2f63f0d-999a-4cdc-bd15-41b6b4ff965b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "class Dataset_Preprocessing:\n",
        "    def __init__(self, dir_path, include_dimension = 2, sample_size = 50, total_classes = 17):\n",
        "        \n",
        "        #Dataset Directory path\n",
        "        self.dir_path = dir_path\n",
        "        \n",
        "        #Which Dimension file to include, possible values: 2 and 3\n",
        "        self.include_dimension = include_dimension\n",
        "        \n",
        "        #Total frames in one Sample\n",
        "        self.sample_size = sample_size\n",
        "        \n",
        "        #Activity classes to include\n",
        "        self.classes = ['SittingDown', 'Walking', 'Directions', 'Discussion', 'Sitting', 'Phoning', 'Eating', 'Posing', 'Greeting', 'Smoking']\n",
        "        \n",
        "        #Total activity classes\n",
        "        self.total_classes = len(self.classes)\n",
        "        \n",
        "        #Subject Folders names in the Dataset\n",
        "        self.internal_folders = ['S1', 'S5','S6','S7','S8','S9','S11']\n",
        "    \n",
        "    def read_dataset(self):\n",
        "        try:\n",
        "            #Contains all the different activity vectors\n",
        "            activity_vector = {}\n",
        "            \n",
        "            #Contains the overall dataset\n",
        "            sampled_data = None\n",
        "            \n",
        "            #Based on dimensions, which folder to use for extracting the dataset files\n",
        "            data_folder = 'Poses_D2_Positions' if self.include_dimension == 2 else 'Poses_D3_Positions'\n",
        "            \n",
        "            #Checking if the dataset path is valid\n",
        "            if not os.path.exists(self.dir_path):\n",
        "                print('The Data Directory Does not Exist!')\n",
        "                return None\n",
        "\n",
        "            #Iterating over all the subject folders\n",
        "            for fld in self.internal_folders:\n",
        "                #Iterating for each file in the specified folder\n",
        "                for file in os.listdir(os.path.join(self.dir_path, fld, data_folder)):\n",
        "                    #Extracting the activity from the filename\n",
        "                    activity = self.__extract_activity(file)\n",
        "                    \n",
        "                    if activity not in self.classes:\n",
        "                        continue\n",
        "                    \n",
        "                    #Reading the CSV file using Pandas\n",
        "                    data = pd.read_csv(os.path.join(self.dir_path, fld, data_folder, file), header=None)\n",
        "\n",
        "                    #Formulating the activity vector using one hot encoding\n",
        "                    if activity not in activity_vector:\n",
        "                        total_keys = len(activity_vector.keys())\n",
        "                        activity_vector[activity] = np.zeros(self.total_classes)\n",
        "                        activity_vector[activity][total_keys] = 1\n",
        "                    vector = activity_vector[activity]\n",
        "                    \n",
        "                    #Sampling the dataset\n",
        "                    grouped_sample = self.__group_samples(data, self.sample_size, vector)\n",
        "                    sampled_data = grouped_sample if sampled_data is None else np.append(sampled_data, grouped_sample, axis=0)\n",
        "                    \n",
        "            return sampled_data\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __extract_activity(self, filename):\n",
        "        try:\n",
        "            #Extracting the filename and excluding the extension\n",
        "            name = os.path.splitext(filename)[0]\n",
        "            \n",
        "            #Substituting the empty string with characters other than english alphabets\n",
        "            activity = re.sub('[^A-Za-z]+' , '' , name)\n",
        "            return activity\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "    \n",
        "    def __group_samples(self, dataset, sample_size, activity):\n",
        "        try:\n",
        "            #Checking if the dataset is a Pandas Dataframe\n",
        "            if not isinstance(dataset, pd.DataFrame):\n",
        "                print('Expecting Pandas Dataframe, but got {}'.format(type(dataset)))\n",
        "                return None\n",
        "            \n",
        "            #Appending activity class to each row in the dataset\n",
        "            dataset = pd.concat([dataset, pd.DataFrame(np.tile(activity, (dataset.shape[0],1)))], axis=1)\n",
        "            \n",
        "            #Reshaping the dataset into sample batches\n",
        "            total_samples = dataset.shape[0]//sample_size\n",
        "            total_features = dataset.shape[1]\n",
        "            grouped_rows = dataset.to_numpy()[:total_samples*self.sample_size].reshape((-1,self.sample_size, total_features))\n",
        "            \n",
        "            return grouped_rows\n",
        "        except Exception as e:\n",
        "            print(e)"
      ],
      "metadata": {
        "id": "VJlcJTFQWsoI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For short term prediction, we need a sample size of 20(10 frames input sequance, 10 frames predicted sequance)\n",
        "sampled_data = Dataset_Preprocessing('/content/drive/MyDrive/Colab Notebooks/H3.6csv', sample_size=20).read_dataset()"
      ],
      "metadata": {
        "id": "Eud56W84W80a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmD7-mCDXAPI",
        "outputId": "70e4db75-1aaa-466b-e568-ab71b748c645"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77144, 20, 74)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_to_features_labels(dataset, input_sequance_size=10) :\n",
        "    assert input_sequance_size < dataset.shape[1], f\"input sequance should be smaller than the total sample size\"\n",
        "    features = dataset[:, np.s_[0:input_sequance_size], :]\n",
        "    labels = dataset[:,np.s_[input_sequance_size:], :64]\n",
        "    \n",
        "    return features, labels"
      ],
      "metadata": {
        "id": "Eed-DrIHXCU4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_dataX, sampled_dataY = split_to_features_labels(sampled_data, input_sequance_size=10)"
      ],
      "metadata": {
        "id": "7TbKYYsfXEsn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Total Samples: {}'.format(sampled_dataY.shape[0]))\n",
        "print('Total Frames: {}'.format(sampled_dataY.shape[1]))\n",
        "print('Total Features: {}'.format(sampled_dataY.shape[2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MbSHOacXHjf",
        "outputId": "3009d83d-1efd-444d-fc5a-2ac980a91c5f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Samples: 77144\n",
            "Total Frames: 10\n",
            "Total Features: 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense, Bidirectional\n",
        "\n",
        "# Define an input sequence and process it.\n",
        "encoder_inputs = Input(shape=(None, 74))\n",
        "encoder = LSTM(200, return_state=True, return_sequences=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = Input(shape=(None, 200))\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the \n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_lstm = LSTM(200, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
        "                                     initial_state=encoder_states)\n",
        "decoder_dense = Dense(64, activation='relu')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "FWYdj9HqXKIY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_joint(predicted_sequance_batch, target_sequance_batch) :\n",
        "    diff_norm_2 = tf.math.reduce_sum(tf.square(tf.subtract(predicted_sequance_batch, target_sequance_batch)), axis=2)\n",
        "    return tf.reduce_sum(diff_norm_2, axis=1) \n",
        "\n",
        "def loss_motion_flow(predicted_sequance_batch, target_sequance_batch) :\n",
        "    predictions_tomporal_diffs = tf.experimental.numpy.diff(predicted_sequance_batch, axis=1)\n",
        "    real_tomporal_diffs = tf.experimental.numpy.diff(target_sequance_batch, axis=1)\n",
        "    prediction_motion_flow_diff_norm_2 = tf.reduce_sum(tf.square(tf.subtract(predictions_tomporal_diffs, real_tomporal_diffs)), axis=2)\n",
        "    return tf.reduce_sum(prediction_motion_flow_diff_norm_2, axis=1)\n",
        "\n",
        "\n",
        "def total_loss(target_sequance_batch, predicted_sequance_batch) :\n",
        "    joints_loss = loss_joint(predicted_sequance_batch, target_sequance_batch)\n",
        "    motion_flow_loss = loss_motion_flow(predicted_sequance_batch, target_sequance_batch)\n",
        "    return 0.5*joints_loss + 0.5*motion_flow_loss"
      ],
      "metadata": {
        "id": "ocEBTNq1XTho"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Whatever loss that accepts true values and predictions can be used\n",
        "model.compile(optimizer='adam', loss=total_loss)\n",
        "model.fit([sampled_dataX, encoder(sampled_dataX)[0]], sampled_dataY,\n",
        "          batch_size=100,\n",
        "          epochs=50,\n",
        "          validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TDDEHtklXXZ4",
        "outputId": "9cdf3510-7358-4aa0-db2b-78b21f61dd2a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "618/618 [==============================] - 93s 144ms/step - loss: 61583732.0000 - val_loss: 47868304.0000\n",
            "Epoch 2/50\n",
            "618/618 [==============================] - 86s 139ms/step - loss: 38346088.0000 - val_loss: 29817322.0000\n",
            "Epoch 3/50\n",
            "618/618 [==============================] - 90s 145ms/step - loss: 24037766.0000 - val_loss: 19044090.0000\n",
            "Epoch 4/50\n",
            "618/618 [==============================] - 91s 147ms/step - loss: 15810402.0000 - val_loss: 13197959.0000\n",
            "Epoch 5/50\n",
            "618/618 [==============================] - 88s 142ms/step - loss: 11570831.0000 - val_loss: 10437044.0000\n",
            "Epoch 6/50\n",
            "618/618 [==============================] - 86s 140ms/step - loss: 9710052.0000 - val_loss: 9379813.0000\n",
            "Epoch 7/50\n",
            "618/618 [==============================] - 88s 142ms/step - loss: 9049651.0000 - val_loss: 9032657.0000\n",
            "Epoch 8/50\n",
            "618/618 [==============================] - 83s 134ms/step - loss: 8794798.0000 - val_loss: 8855046.0000\n",
            "Epoch 9/50\n",
            "618/618 [==============================] - 74s 121ms/step - loss: 8683697.0000 - val_loss: 8781850.0000\n",
            "Epoch 10/50\n",
            "618/618 [==============================] - 75s 121ms/step - loss: 7846606.5000 - val_loss: 7470406.5000\n",
            "Epoch 11/50\n",
            "618/618 [==============================] - 79s 128ms/step - loss: 7087134.0000 - val_loss: 7051416.5000\n",
            "Epoch 12/50\n",
            "618/618 [==============================] - 80s 130ms/step - loss: 6764002.5000 - val_loss: 6741524.0000\n",
            "Epoch 13/50\n",
            "618/618 [==============================] - 77s 125ms/step - loss: 6495554.5000 - val_loss: 6483065.0000\n",
            "Epoch 14/50\n",
            "618/618 [==============================] - 78s 126ms/step - loss: 6120058.0000 - val_loss: 5612611.5000\n",
            "Epoch 15/50\n",
            "618/618 [==============================] - 80s 130ms/step - loss: 5126653.0000 - val_loss: 4918247.0000\n",
            "Epoch 16/50\n",
            "618/618 [==============================] - 86s 140ms/step - loss: 4634062.5000 - val_loss: 4553426.0000\n",
            "Epoch 17/50\n",
            "618/618 [==============================] - 81s 130ms/step - loss: 4342154.5000 - val_loss: 4319907.0000\n",
            "Epoch 18/50\n",
            "618/618 [==============================] - 81s 132ms/step - loss: 4152547.5000 - val_loss: 4020707.0000\n",
            "Epoch 19/50\n",
            "618/618 [==============================] - 83s 135ms/step - loss: 3594653.7500 - val_loss: 3501346.7500\n",
            "Epoch 20/50\n",
            "618/618 [==============================] - 82s 132ms/step - loss: 3321261.5000 - val_loss: 3329004.7500\n",
            "Epoch 21/50\n",
            "618/618 [==============================] - 84s 135ms/step - loss: 3205515.0000 - val_loss: 3249671.0000\n",
            "Epoch 22/50\n",
            "618/618 [==============================] - 79s 128ms/step - loss: 3134853.0000 - val_loss: 3186236.0000\n",
            "Epoch 23/50\n",
            "618/618 [==============================] - 77s 125ms/step - loss: 3082770.0000 - val_loss: 3141287.2500\n",
            "Epoch 24/50\n",
            "618/618 [==============================] - 81s 130ms/step - loss: 3042227.5000 - val_loss: 3101182.7500\n",
            "Epoch 25/50\n",
            "618/618 [==============================] - 81s 130ms/step - loss: 3009413.7500 - val_loss: 3067599.5000\n",
            "Epoch 26/50\n",
            "618/618 [==============================] - 82s 132ms/step - loss: 2985399.2500 - val_loss: 3064730.5000\n",
            "Epoch 27/50\n",
            "618/618 [==============================] - 80s 129ms/step - loss: 2964811.2500 - val_loss: 3031468.0000\n",
            "Epoch 28/50\n",
            "618/618 [==============================] - 85s 137ms/step - loss: 2945192.2500 - val_loss: 3006725.2500\n",
            "Epoch 29/50\n",
            "618/618 [==============================] - 81s 131ms/step - loss: 2931141.7500 - val_loss: 2990081.5000\n",
            "Epoch 30/50\n",
            "618/618 [==============================] - 78s 126ms/step - loss: 2914735.7500 - val_loss: 2976521.0000\n",
            "Epoch 31/50\n",
            "618/618 [==============================] - 80s 130ms/step - loss: 2902512.0000 - val_loss: 2961530.5000\n",
            "Epoch 32/50\n",
            "  1/618 [..............................] - ETA: 1:12 - loss: 2680176.0000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-7a978fb17ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           validation_split=0.2)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8rX4vJlbYLd3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}